import Overview from "../partials/cmx/_overview.mdx"
import SupportedClusters from "../partials/cmx/_supported-clusters-overview.mdx"

# About Compatibility Matrix

This topic describes Replicated Compatibility Matrix, including use cases, billing, limitations, and more.

## Overview

<Overview/>

You can use Compatibility Matrix with the Replicated CLI or the Replicated Vendor Portal. For more information about how to use Compatibility Matrix, see [Using Compatibility Matrix](testing-how-to).

### Supported Clusters

<SupportedClusters/>

### Billing and Credits

Clusters created with Compatibility Matrix are billed by the minute. Per-minute billing begins when the cluster reaches a `running` status and ends when the cluster is deleted. Compatibility Matrix marks a cluster as `running` when a working kubeconfig for the cluster is accessible.

You are billed only for the time that the cluster is in a `running` status. You are _not_ billed for the time that it takes Compatibility Matrix to create and tear down clusters, including when the cluster is in an `assigned` status.

For more information about pricing, see [Compatibility Matrix Pricing](testing-pricing).

To create clusters with Compatibility Matrix, you must have credits in your Vendor Portal account.
If you have a contract, you can purchase credits by logging in to the Vendor Portal and going to [**Compatibility Matrix > Buy additional credits**](https://vendor.replicated.com/compatibility-matrix).
Otherwise, to request credits, log in to the Vendor Portal and go to [**Compatibility Matrix > Request more credits**](https://vendor.replicated.com/compatibility-matrix).

### Quotas and Capacity

By default, Compatibility Matrix sets quotas for the capacity that can be used concurrently by each vendor portal team. These quotas are designed to ensure that Replicated maintains a minimum amount of capacity for provisioning both VM and cloud-based clusters.

By default, the quota for cloud-based cluster distributions (AKS, GKE, EKS) is three clusters running concurrently.

VM-based cluster distributions (such as kind, OpenShift, and Replicated Embedded Cluster) have the following default quotas:
* 32 vCPUs
* 128 GiB memory
* 800 GiB disk size 

You can request increased quotas at any time with no additional cost. To view your team's current quota and capacity usage, or to request a quota increase, go to [**Compatibility Matrix > Settings**](https://vendor.replicated.com/compatibility-matrix/settings) in the vendor portal:

![Compatibility matrix settings page](/images/compatibility-matrix-settings.png)

[View a larger version of this image](/images/compatibility-matrix-settings.png)

### Cluster Status

Clusters created with Compatibility Matrix can have the following statuses:

* `assigned`: The cluster resources were requested and Compatibility Matrix is provisioning the cluster. You are not billed for the time that a cluster spends in the `assigned` status.

* `running`: A working kubeconfig for the cluster is accessible. Billing begins when the cluster reaches a `running` status.

   Additionally, clusters are verified prior to transitioning to a `running` status. Verification includes checking that the cluster is healthy and running with the correct number of nodes, as well as passing [sonobuoy](https://sonobuoy.io/) tests in `--quick` mode.

* `terminated`: The cluster is deleted. Billing ends when the cluster status is changed from `running` to `terminated`.

* `error`: An error occured when attempting to provision the cluster.

You can view the status of clusters using the `replicated cluster ls` command. For more information, see [cluster ls](/reference/replicated-cli-cluster-ls).

### Cluster Add-ons

The Replicated Compatibility Matrix enables you to extend your cluster with add-ons, to make use of by your application, such as an AWS S3 object store.
This allows you to more easily provision dependencies required by your application.

For more information about how to use the add-ons, see [Compatibility Matrix Cluster Add-ons](testing-cluster-addons).

## Limitations

Compatibility Matrix has the following limitations:

- Clusters cannot be resized. Create another cluster if you want to make changes, such as add another node.
- Clusters cannot be rebooted. Create another cluster if you need to reset/reboot the cluster. 
- On cloud clusters, node groups are not available for every distribution. For distribution-specific details, see [Supported Compatibility Matrix Cluster Types](/vendor/testing-supported-clusters).
- Multi-node support is not available for every distribution. For distribution-specific details, see [Supported Compatibility Matrix Cluster Types](/vendor/testing-supported-clusters).
- ARM instance types are only supported on Cloud Clusters. For distribution-specific details, see [Supported Compatibility Matrix Cluster Types](/vendor/testing-supported-clusters).
- GPU instance types are only supported on Cloud Clusters. For distribution-specific details, see [Supported Compatibility Matrix Cluster Types](/vendor/testing-supported-clusters).
- There is no support for IPv6 as a single stack. Dual stack support is available on kind clusters.
- There is no support for air gap testing. 
- The `cluster upgrade` feature is available only for kURL distributions. See [cluster upgrade](/reference/replicated-cli-cluster-upgrade).
- Cloud clusters do not allow for the configuration of CNI, CSI, CRI, Ingress, or other plugins, add-ons, services, and interfaces.
- The node operating systems for clusters created with Compatibility Matrix cannot be configured nor replaced with different operating systems.
- The Kubernetes scheduler for clusters created with Compatibility Matrix cannot be replaced with a different scheduler.
- Each team has a quota limit on the amount of resources that can be used simultaneously. This limit can be raised by messaging your account representative.
- Team actions with Compatibility Matrix (for example, creating and deleting clusters and requesting quota increases) are not logged and displayed in the [Vendor Team Audit Log](https://vendor.replicated.com/team/audit-log). 

For additional distribution-specific limitations, see [Supported Compatibility Matrix Cluster Types](testing-supported-clusters).


---


import TestRecs from "../partials/ci-cd/_test-recs.mdx"
import Prerequisites from "../partials/cmx/_prerequisites.mdx"

# Using Compatibility Matrix

This topic describes how to use Replicated Compatibility Matrix to create ephemeral clusters.

## Prerequisites

Before you can use Compatibility Matrix, you must complete the following prerequisites:

<Prerequisites/>

* Existing accounts must accept the TOS for the trial on the [**Compatibility Matrix**](https://vendor.replicated.com/compatibility-matrix) page in the Replicated Vendor Portal.

## Create and Manage Clusters

This section explains how to use Compatibility Matrix to create and manage clusters with the Replicated CLI or the Vendor Portal.

For information about creating and managing clusters with the Vendor API v3, see the [clusters](https://replicated-vendor-api.readme.io/reference/listclusterusage) section in the Vendor API v3 documentation.

### Create Clusters

You can create clusters with Compatibility Matrix using the Replicated CLI or the Vendor Portal.

#### Replicated CLI

To create a cluster using the Replicated CLI:

1. (Optional) View the available cluster distributions, including the supported Kubernetes versions, instance types, and maximum nodes for each distribution:

   ```bash
   replicated cluster versions
   ```
   For command usage, see [cluster versions](/reference/replicated-cli-cluster-versions).

1. Run the following command to create a cluster:

   ```
   replicated cluster create --name NAME --distribution K8S_DISTRO --version K8S_VERSION --disk DISK_SIZE --instance-type INSTANCE_TYPE [--license-id LICENSE_ID]
   ```
   Where:
   * `NAME` is any name for the cluster. If `--name` is excluded, a name is automatically generated for the cluster.
   * `K8S_DISTRO` is the Kubernetes distribution for the cluster.
   * `K8S_VERSION` is the Kubernetes version for the cluster if creating a standard Cloud or VM-based cluster. If creating an Embedded Cluster or kURL cluster type,`--version` is optional:
      * For Embedded Cluster types, `--verison` is the latest available release on the channel by default. Otherwise, to specify a different release, set `--version` to the `Channel release sequence` value for the release.  
      * For kURL cluster types, `--verison` is the `"latest"` kURL Installer ID by default.  Otherwise, to specify a different kURL Installer, set `--version` to the kURL Installer ID. 
   * `DISK_SIZE` is the disk size (GiB) to request per node.
   * `INSTANCE_TYPE` is the instance type to use for each node.
   * (Embedded Cluster Only) `LICENSE_ID` is a valid customer license. Required to create an Embedded Cluster.

   For command usage and additional optional flags, see [cluster create](/reference/replicated-cli-cluster-create).

   **Example:**

   The following example creates a kind cluster with Kubernetes version 1.27.0, a disk size of 100 GiB, and an instance type of `r1.small`. 

   ```bash
   replicated cluster create --name kind-example --distribution kind --version 1.27.0 --disk 100 --instance-type r1.small
   ```

1. Verify that the cluster was created:

   ```bash
   replicated cluster ls CLUSTER_NAME
   ```
   Where `CLUSTER_NAME` is the name of the cluster that you created.

   In the output of the command, you can see that the `STATUS` of the cluster is `assigned`. When the kubeconfig for the cluster is accessible, the cluster's status is changed to `running`. For more information about cluster statuses, see [Cluster Status](testing-about#cluster-status) in _About Compatibility Matrix._

#### Vendor Portal

To create a cluster using the Vendor Portal:

1. Go to [**Compatibility Matrix > Create cluster**](https://vendor.replicated.com/compatibility-matrix/create-cluster).

    <img alt="Create a cluster page" src="/images/create-a-cluster.png" width="650px"/>

    [View a larger version of this image](/images/create-a-cluster.png)

1. On the **Create a cluster** page, complete the following fields:

   <table>
     <tr>
       <th>Field</th>
       <th>Description</th>
     </tr>
     <tr>
       <td>Kubernetes distribution</td>
       <td>Select the Kubernetes distribution for the cluster.</td>
     </tr>
     <tr>
       <td>Version</td>
       <td>Select the Kubernetes version for the cluster. The options available are specific to the distribution selected.</td>
     </tr>
     <tr>
       <td>Name (optional)</td>
       <td>Enter an optional name for the cluster.</td>
     </tr>
     <tr>
       <td>Tags</td>
       <td>Add one or more tags to the cluster as key-value pairs.</td>
     </tr>
     <tr>
       <td>Set TTL</td>
       <td>Select the Time to Live (TTL) for the cluster. When the TTL expires, the cluster is automatically deleted. TTL can be adjusted after cluster creation with [cluster update ttl](/reference/replicated-cli-cluster-update-ttl).</td>
     </tr>
   </table>  

1. For **Nodes & Nodes Groups**, complete the following fields to configure nodes and node groups for the cluster:

   <table>
   <tr>
       <td>Instance type</td>
       <td>Select the instance type to use for the nodes in the node group. The options available are specific to the distribution selected.</td>
     </tr>   
     <tr>
       <td>Disk size</td>
       <td>Select the disk size in GiB to use per node.</td>
     </tr>
     <tr>
       <td>Nodes</td>
       <td>Select the number of nodes to provision in the node group. The options available are specific to the distribution selected.</td>
     </tr>  
   </table>

1. (Optional) Click **Add node group** to add additional node groups.

1. Click **Create cluster**.

   The cluster is displayed in the list of clusters on the **Compatibility Matrix** page with a status of Assigned. When the kubeconfig for the cluster is accessible, the cluster's status is changed to Running.

   :::note
   If the cluster is not automatically displayed, refresh your browser window.
   :::

   <img alt="Cluster configuration dialog" src="/images/cmx-assigned-cluster.png" width="700px"/>

   [View a larger version of this image](/images/cmx-assigned-cluster.png)

### Prepare Clusters

For applications distributed with the Replicated Vendor Portal, the [`cluster prepare`](/reference/replicated-cli-cluster-prepare) command reduces the number of steps required to provision a cluster and then deploy a release to the cluster for testing. This is useful in continuous integration (CI) workflows that run multiple times a day. For an example workflow that uses the `cluster prepare` command, see [Recommended CI/CD Workflows](/vendor/ci-workflows).

The `cluster prepare` command does the following:
* Creates a cluster
* Creates a release for your application based on either a Helm chart archive or a directory containing the application YAML files
* Creates a temporary customer of type `test`
  :::note
  Test customers created by the `cluster prepare` command are not saved in your Vendor Portal team.
  :::
* Installs the release in the cluster using either the Helm CLI or Replicated KOTS

The `cluster prepare` command requires either a Helm chart archive or a directory containing the application YAML files to be installed:

* **Install a Helm chart with the Helm CLI**:

  ```bash
  replicated cluster prepare \
    --distribution K8S_DISTRO \
    --version K8S_VERSION \
    --chart HELM_CHART_TGZ
  ```
  The following example creates a kind cluster and installs a Helm chart in the cluster using the `nginx-chart-0.0.14.tgz` chart archive:
  ```bash
  replicated cluster prepare \
    --distribution kind \
    --version 1.27.0 \
    --chart nginx-chart-0.0.14.tgz \
    --set key1=val1,key2=val2 \
    --set-string s1=val1,s2=val2 \
    --set-json j1='{"key1":"val1","key2":"val2"}' \
    --set-literal l1=val1,l2=val2 \
    --values values.yaml
  ```

* **Install with KOTS from a YAML directory**:

  ```bash
  replicated cluster prepare \
    --distribution K8S_DISTRO \
    --version K8S_VERSION \
    --yaml-dir PATH_TO_YAML_DIR
  ```
  The following example creates a k3s cluster and installs an application in the cluster using the manifest files in a local directory named `config-validation`: 
  ```bash
  replicated cluster prepare \
    --distribution k3s \
    --version 1.26 \
    --namespace config-validation \
    --shared-password password \
    --app-ready-timeout 10m \
    --yaml-dir config-validation \
    --config-values-file conifg-values.yaml \
    --entitlements "num_of_queues=5"
    ```

For command usage, including additional options, see [cluster prepare](/reference/replicated-cli-cluster-prepare).

### Access Clusters

Compatibility Matrix provides the kubeconfig for clusters so that you can access clusters with the kubectl command line tool. For more information, see [Command line tool (kubectl)](https://kubernetes.io/docs/reference/kubectl/) in the Kubernetes documentation.

To access a cluster from the command line:

1. Verify that the cluster is in a Running state:

   ```bash
   replicated cluster ls
   ```
   In the output of the command, verify that the `STATUS` for the target cluster is `running`. For command usage, see [cluster ls](/reference/replicated-cli-cluster-ls).

1. Run the following command to open a new shell session with the kubeconfig configured for the cluster:

   ```bash
   replicated cluster shell CLUSTER_ID
   ``` 
   Where `CLUSTER_ID` is the unique ID for the running cluster that you want to access.

   For command usage, see [cluster shell](/reference/replicated-cli-cluster-shell).

1. Verify that you can interact with the cluster through kubectl by running a command. For example:

   ```bash
   kubectl get ns
   ```

1. Press Ctrl-D or type `exit` when done to end the shell and the connection to the server.

### Upgrade Clusters (kURL Only)

For kURL clusters provisioned with Compatibility Matrix, you can use the the `cluster upgrade` command to upgrade the version of the kURL installer specification used to provision the cluster. A recommended use case for the `cluster upgrade` command is for testing your application's compatibility with Kubernetes API resource version migrations after upgrade.

The following example upgrades a kURL cluster from its previous version to version `9d5a44c`:

```bash
replicated cluster upgrade cabb74d5 --version 9d5a44c
```

For command usage, see [cluster upgrade](/reference/replicated-cli-cluster-upgrade).

### Delete Clusters

You can delete clusters using the Replicated CLI or the Vendor Portal.

#### Replicated CLI

To delete a cluster using the Replicated CLI:

1. Get the ID of the target cluster:

   ```
   replicated cluster ls
   ```
   In the output of the command, copy the ID for the cluster.

   **Example:**

   ```
   ID        NAME              DISTRIBUTION   VERSION   STATUS    CREATED                        EXPIRES 
   1234abc   My Test Cluster   eks            1.27      running   2023-10-09 17:08:01 +0000 UTC  - 
   ``` 

   For command usage, see [cluster ls](/reference/replicated-cli-cluster-ls).

1. Run the following command:

    ```
    replicated cluster rm CLUSTER_ID
    ```
    Where `CLUSTER_ID` is the ID of the target cluster. 
    For command usage, see [cluster rm](/reference/replicated-cli-cluster-rm).
1. Confirm that the cluster was deleted:
   ```
   replicated cluster ls CLUSTER_ID --show-terminated
   ```
   Where `CLUSTER_ID` is the ID of the target cluster.
   In the output of the command, you can see that the `STATUS` of the cluster is `terminated`. For command usage, see [cluster ls](/reference/replicated-cli-cluster-ls).
#### Vendor Portal

To delete a cluster using the Vendor Portal:

1. Go to **Compatibility Matrix**.

1. Under **Clusters**, in the vertical dots menu for the target cluster, click **Delete cluster**.

   <img alt="Delete cluster button" src="/images/cmx-delete-cluster.png" width="700px"/>

   [View a larger version of this image](/images/cmx-delete-cluster.png)

## About Using Compatibility Matrix with CI/CD

Replicated recommends that you integrate Compatibility Matrix into your existing CI/CD workflow to automate the process of creating clusters to install your application and run tests. For more information, including additional best practices and recommendations for CI/CD, see [About Integrating with CI/CD](/vendor/ci-overview).

### Replicated GitHub Actions

Replicated maintains a set of custom GitHub actions that are designed to replace repetitive tasks related to using Compatibility Matrix and distributing applications with Replicated.

If you use GitHub Actions as your CI/CD platform, you can include these custom actions in your workflows rather than using Replicated CLI commands. Integrating the Replicated GitHub actions into your CI/CD pipeline helps you quickly build workflows with the required inputs and outputs, without needing to manually create the required CLI commands for each step.

To view all the available GitHub actions that Replicated maintains, see the [replicatedhq/replicated-actions](https://github.com/replicatedhq/replicated-actions/) repository in GitHub.

For more information, see [Integrating Replicated GitHub Actions](/vendor/ci-workflows-github-actions).

### Recommended Workflows

Replicated recommends that you maintain unique CI/CD workflows for development (continuous integration) and for releasing your software (continuous delivery). For example development and release workflows that integrate Compatibility Matrix for testing, see [Recommended CI/CD Workflows](/vendor/ci-workflows).

### Test Script Recommendations

Incorporating code tests into your CI/CD workflows is important for ensuring that developers receive quick feedback and can make updates in small iterations. Replicated recommends that you create and run all of the following test types as part of your CI/CD workflows:

<TestRecs/>


---


import Pool from "../partials/cmx/\_openshift-pool.mdx"

# Supported Compatibility Matrix Cluster Types

This topic describes the supported Kubernetes distributions, Kubernetes versions, instance types, nodes, limitations, and common use cases for clusters created with Replicated Compatibility Matrix.

Compatibility Matrix provisions cloud-based or virtual machine (VM) clusters.

## VM Clusters

This section lists the supported VM cluster distributions for clusters created with Compatibility Matrix.

### kind

Compatibility Matrix supports creating [kind](https://kind.sigs.k8s.io/) clusters.

<table>
  <tr>
        <th width="35%">Type</th>
        <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported Kubernetes Versions</th>
    <td>{/* START_kind_VERSIONS */}1.26.15, 1.27.16, 1.28.15, 1.29.14, 1.30.10, 1.31.6, 1.32.3{/* END_kind_VERSIONS */}</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td>See <a href="#types">Replicated Instance Types</a></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports a single node.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4` or `dual`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td>See <a href="testing-about#limitations">Limitations</a></td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Smoke tests</td>
  </tr>
</table>

### k3s

Compatibility Matrix supports creating [k3s](https://k3s.io) clusters.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported k3s Versions</th>
    <td>The upstream k8s version that matches the Kubernetes version requested.</td>
  </tr>
  <tr>
    <th>Supported Kubernetes Versions</th>
    <td>{/* START_k3s_VERSIONS */}1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.24.6, 1.24.7, 1.24.8, 1.24.9, 1.24.10, 1.24.11, 1.24.12, 1.24.13, 1.24.14, 1.24.15, 1.24.16, 1.24.17, 1.25.0, 1.25.2, 1.25.3, 1.25.4, 1.25.5, 1.25.6, 1.25.7, 1.25.8, 1.25.9, 1.25.10, 1.25.11, 1.25.12, 1.25.13, 1.25.14, 1.25.15, 1.25.16, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 1.26.5, 1.26.6, 1.26.7, 1.26.8, 1.26.9, 1.26.10, 1.26.11, 1.26.12, 1.26.13, 1.26.14, 1.26.15, 1.27.1, 1.27.2, 1.27.3, 1.27.4, 1.27.5, 1.27.6, 1.27.7, 1.27.8, 1.27.9, 1.27.10, 1.27.11, 1.27.12, 1.27.13, 1.27.14, 1.27.15, 1.27.16, 1.28.1, 1.28.2, 1.28.3, 1.28.4, 1.28.5, 1.28.6, 1.28.7, 1.28.8, 1.28.9, 1.28.10, 1.28.11, 1.28.12, 1.28.13, 1.28.14, 1.28.15, 1.29.0, 1.29.1, 1.29.2, 1.29.3, 1.29.4, 1.29.5, 1.29.6, 1.29.7, 1.29.8, 1.29.9, 1.29.10, 1.29.11, 1.29.12, 1.29.13, 1.29.14, 1.29.15, 1.30.0, 1.30.1, 1.30.2, 1.30.3, 1.30.4, 1.30.5, 1.30.6, 1.30.7, 1.30.8, 1.30.9, 1.30.10, 1.30.11, 1.31.0, 1.31.1, 1.31.2, 1.31.3, 1.31.4, 1.31.5, 1.31.6, 1.31.7, 1.32.0, 1.32.1, 1.32.2, 1.32.3{/* END_k3s_VERSIONS */}</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td>See <a href="#types">Replicated Instance Types</a></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>  
  <tr>
    <th>Limitations</th>
    <td>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td><ul><li>Smoke tests</li><li>Customer release tests</li></ul></td>
  </tr>
</table>

### RKE2 (Beta)

Compatibility Matrix supports creating [RKE2](https://docs.rke2.io/) clusters.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported RKE2 Versions</th>
    <td>The upstream k8s version that matches the Kubernetes version requested.</td>
  </tr>
  <tr>
    <th>Supported Kubernetes Versions</th>
    <td>{/* START_rke2_VERSIONS */}1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.24.6, 1.24.7, 1.24.8, 1.24.9, 1.24.10, 1.24.11, 1.24.12, 1.24.13, 1.24.14, 1.24.15, 1.24.16, 1.24.17, 1.25.0, 1.25.2, 1.25.3, 1.25.4, 1.25.5, 1.25.6, 1.25.7, 1.25.8, 1.25.9, 1.25.10, 1.25.11, 1.25.12, 1.25.13, 1.25.14, 1.25.15, 1.25.16, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 1.26.5, 1.26.6, 1.26.7, 1.26.8, 1.26.9, 1.26.10, 1.26.11, 1.26.12, 1.26.13, 1.26.14, 1.26.15, 1.27.1, 1.27.2, 1.27.3, 1.27.4, 1.27.5, 1.27.6, 1.27.7, 1.27.8, 1.27.9, 1.27.10, 1.27.11, 1.27.12, 1.27.13, 1.27.14, 1.27.15, 1.27.16, 1.28.2, 1.28.3, 1.28.4, 1.28.5, 1.28.6, 1.28.7, 1.28.8, 1.28.9, 1.28.10, 1.28.11, 1.28.12, 1.28.13, 1.28.14, 1.28.15, 1.29.0, 1.29.1, 1.29.2, 1.29.3, 1.29.4, 1.29.5, 1.29.6, 1.29.7, 1.29.8, 1.29.9, 1.29.10, 1.29.11, 1.29.12, 1.29.13, 1.29.14, 1.29.15, 1.30.0, 1.30.1, 1.30.2, 1.30.3, 1.30.4, 1.30.5, 1.30.6, 1.30.7, 1.30.8, 1.30.9, 1.30.10, 1.30.11, 1.31.0, 1.31.1, 1.31.2, 1.31.3, 1.31.4, 1.31.5, 1.31.6, 1.31.7, 1.32.0, 1.32.1, 1.32.2, 1.32.3{/* END_rke2_VERSIONS */}</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td>See <a href="#types">Replicated Instance Types</a></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td><ul><li>Smoke tests</li><li>Customer release tests</li></ul></td>
  </tr>
</table>

### OpenShift OKD

Compatibility Matrix supports creating [Red Hat OpenShift OKD](https://www.okd.io/) clusters, which is the community distribution of OpenShift, using CodeReady Containers (CRC).

OpenShift clusters are provisioned with two users:

- (Default) A `kubeadmin` user with `cluster-admin` priviledges. Use the `kubeadmin` user only for administrative tasks such as creating new users or setting roles.
- A `developer` user with namespace-scoped priviledges. The `developer` user can be used to better simulate access in end-customer environments.

By default, kubeconfig context is set to the `kubeadmin` user. To switch to the `developer` user, run the command `oc login --username developer`.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported OpenShift Versions</th>
    <td>{/* START_openshift_VERSIONS */}4.10.0-okd, 4.11.0-okd, 4.12.0-okd, 4.13.0-okd, 4.14.0-okd, 4.15.0-okd, 4.16.0-okd, 4.17.0-okd{/* END_openshift_VERSIONS */}</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td>See <a href="#types">Replicated Instance Types</a></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes for versions 4.13.0-okd and later.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td>
      <ul>
        <li>OpenShift does not support r1.small instance types.</li>
        <li>OpenShift versions earlier than 4.13-okd do not have a registry mirror and so may be subject to rate limiting from Docker Hub. For information about Docker Hub rate limiting, see <a href="https://docs.docker.com/docker-hub/download-rate-limit/">Docker Hub rate limit</a>. To increase limits, Replicated recommends that you configure an image pull secret to pull public Docker Hub images as an authenticated user. For more information about how to configure image pull secrets, see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">Pull an Image from a Private Registry</a> in the Kubernetes documentation.</li>
        <li>
          <p>OpenShift builds take approximately 17 minutes.</p>
          <p><Pool/></p>
        </li>
      </ul>
      <p>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</p>
    </td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Customer release tests</td>
  </tr>
</table>

### Embedded Cluster

Compatibility Matrix supports creating clusters with Replicated Embedded Cluster. For more information, see [Embedded Cluster Overview](/vendor/embedded-overview).

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported Embedded Cluster Versions</th>
    <td>
      Any valid release sequence that has previously been promoted to the channel where the customer license is assigned.
      Version is optional and defaults to the latest available release on the channel.
    </td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td>See <a href="#types">Replicated Instance Types</a></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes (alpha).</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td>
      <ul>
        <li>The Admin Console UI is not exposed publicly and must be exposed via `kubectl -n kotsadm port-forward svc/kurl-proxy-kotsadm 38800:8800`. The password for the Admin Console is `password`.</li>
        <li><strong>A valid customer license is required to create an Embedded Cluster.</strong></li>
        <li>The [cluster prepare](/vendor/testing-how-to#prepare-clusters) command is not supported.</li>
      </ul>
      <p>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</p>
    </td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Customer release tests</td>
  </tr>
</table>

### kURL

Compatibility Matrix supports creating [kURL](https://kurl.sh) clusters.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported kURL Versions</th>
    <td>Any promoted kURL installer. Version is optional. For an installer version other than "latest", you can find the specific Installer ID for a previously promoted installer under the relevant **Install Command** (ID after kurl.sh/) on the **Channels > kURL Installer History** page in the Vendor Portal. For more information about viewing the history of kURL installers promoted to a channel, see [Installer History](/vendor/installer-history).</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td>See <a href="#types">Replicated Instance Types</a></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td><p>Does not work with the <a href="https://kurl.sh/docs/add-ons/longhorn">Longhorn add-on</a>.</p><p>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</p></td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Customer release tests</td>
  </tr>
</table>

## Cloud Clusters

This section lists the supported cloud clusters for compatibility testing.

### EKS

Compatibility Matrix supports creating [AWS EKS](https://aws.amazon.com/eks/?nc2=type_a) clusters.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported Kubernetes Versions</th>
    <td><p>{/* START_eks_VERSIONS */}1.25, 1.26, 1.27, 1.28, 1.29, 1.30, 1.31, 1.32{/* END_eks_VERSIONS */}</p><p>Extended Support Versions: 1.25, 1.26, 1.27, 1.28, 1.29</p></td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td><p>m6i.large, m6i.xlarge, m6i.2xlarge, m6i.4xlarge, m6i.8xlarge, m7i.large, m7i.xlarge, m7i.2xlarge, m7i.4xlarge, m7i.8xlarge, m5.large, m5.xlarge, m5.2xlarge,
			m5.4xlarge, m5.8xlarge, m7g.large (arm), m7g.xlarge (arm), m7g.2xlarge (arm), m7g.4xlarge (arm), m7g.8xlarge (arm), c5.large, c5.xlarge, c5.2xlarge, c5.4xlarge,
			c5.9xlarge, g4dn.xlarge (gpu), g4dn.2xlarge (gpu), g4dn.4xlarge (gpu), g4dn.8xlarge (gpu), g4dn.12xlarge (gpu), g4dn.16xlarge (gpu)</p><p>g4dn instance types depend on available capacity. After a g4dn cluster is running, you also need to install your version of the NVIDIA device plugin for Kubernetes. See [Amazon EKS optimized accelerated Amazon Linux AMIs](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami) in the AWS documentation.</p></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>Yes. Cost will be based on the max number of nodes.</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td><p>You can only choose a minor version, not a patch version. The EKS installer chooses the latest patch for that minor version.</p><p>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</p></td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Customer release tests</td>
  </tr>
</table>

### GKE

Compatibility Matrix supports creating [Google GKE](https://cloud.google.com/kubernetes-engine) clusters.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported Kubernetes Versions</th>
    <td>{/* START_gke_VERSIONS */}1.30, 1.31, 1.32.2{/* END_gke_VERSIONS */}</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td><p>n2-standard-2, n2-standard-4, n2-standard-8, n2-standard-16, n2-standard-32, t2a-standard-2 (arm), t2a-standard-4 (arm), t2a-standard-8 (arm), t2a-standard-16 (arm), t2a-standard-32 (arm), t2a-standard-48 (arm), e2-standard-2, e2-standard-4, e2-standard-8, e2-standard-16, e2-standard-32, n1-standard-1+nvidia-tesla-t4+1 (gpu), n1-standard-1+nvidia-tesla-t4+2 (gpu), n1-standard-1+nvidia-tesla-t4+4 (gpu), n1-standard-2+nvidia-tesla-t4+1 (gpu), n1-standard-2+nvidia-tesla-t4+2 (gpu), n1-standard-2+nvidia-tesla-t4+4 (gpu), n1-standard-4+nvidia-tesla-t4+1 (gpu), n1-standard-4+nvidia-tesla-t4+2 (gpu), n1-standard-4+nvidia-tesla-t4+4 (gpu), n1-standard-8+nvidia-tesla-t4+1 (gpu), n1-standard-8+nvidia-tesla-t4+2 (gpu), n1-standard-8+nvidia-tesla-t4+4 (gpu), n1-standard-16+nvidia-tesla-t4+1 (gpu), n1-standard-16+nvidia-tesla-t4+2 (gpu), n1-standard-16+nvidia-tesla-t4+4 (gpu), n1-standard-32+nvidia-tesla-t4+1 (gpu), n1-standard-32+nvidia-tesla-t4+2 (gpu), n1-standard-32+nvidia-tesla-t4+4 (gpu), n1-standard-64+nvidia-tesla-t4+1 (gpu), n1-standard-64+nvidia-tesla-t4+2 (gpu), n1-standard-64+nvidia-tesla-t4+4 (gpu), n1-standard-96+nvidia-tesla-t4+1 (gpu), n1-standard-96+nvidia-tesla-t4+2 (gpu), n1-standard-96+nvidia-tesla-t4+4 (gpu)</p><p>You can specify more than one node.</p></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>Yes. Cost will be based on the max number of nodes.</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td><p>You can choose only a minor version, not a patch version. The GKE installer chooses the latest patch for that minor version.</p><p>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</p></td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Customer release tests</td>
  </tr>
</table>

### AKS

Compatibility Matrix supports creating [Azure AKS](https://azure.microsoft.com/en-us/products/kubernetes-service) clusters.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported Kubernetes Versions</th>
    <td>{/* START_aks_VERSIONS */}1.29, 1.30, 1.31{/* END_aks_VERSIONS */}</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td><p>Standard_B2ms, Standard_B4ms, Standard_B8ms, Standard_B16ms, Standard_DS2_v2, Standard_DS3_v2, Standard_DS4_v2, Standard_DS5_v2, Standard_DS2_v5, Standard_DS3_v5, Standard_DS4_v5, Standard_DS5_v5, Standard_D2ps_v5 (arm), Standard_D4ps_v5 (arm), Standard_D8ps_v5 (arm), Standard_D16ps_v5 (arm), Standard_D32ps_v5 (arm), Standard_D48ps_v5 (arm), Standard_NC4as_T4_v3 (gpu), Standard_NC8as_T4_v3 (gpu), Standard_NC16as_T4_v3 (gpu), Standard_NC64as_T4_v3 (gpu)</p><p>GPU instance types depend on available capacity. After a GPU cluster is running, you also need to install your version of the NVIDIA device plugin for Kubernetes. See [NVIDIA GPU Operator with Azure Kubernetes Service](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/microsoft-aks.html) in the NVIDIA documentation.</p></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>Yes. Cost will be based on the max number of nodes.</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td><p>You can choose only a minor version, not a patch version. The AKS installer chooses the latest patch for that minor version.</p><p>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</p></td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Customer release tests</td>
  </tr>
</table>

### OKE (Beta)

Compatibility Matrix supports creating [Oracle Container Engine for Kubernetes (OKE)](https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengoverview.htm) clusters.

<table>
  <tr>
    <th width="35%">Type</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
    <th>Supported Kubernetes Versions</th>
    <td>{/* START_oke_VERSIONS */}1.29.1, 1.29.10, 1.30.1, 1.31.1, 1.32.1{/* END_oke_VERSIONS */}</td>
  </tr>
  <tr>
    <th>Supported Instance Types</th>
    <td><p>VM.Standard2.1, VM.Standard2.2, VM.Standard2.4, VM.Standard2.8, VM.Standard2.16, VM.Standard3.Flex.1, VM.Standard3.Flex.2, VM.Standard3.Flex.4, VM.Standard3.Flex.8, VM.Standard3.Flex.16, VM.Standard.A1.Flex.1 (arm), VM.Standard.A1.Flex.2 (arm), VM.Standard.A1.Flex.4 (arm), VM.Standard.A1.Flex.8 (arm), VM.Standard.A1.Flex.16 (arm)</p></td>
  </tr>
  <tr>
    <th>Node Groups</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Node Auto Scaling</th>
    <td>No.</td>
  </tr>
  <tr>
    <th>Nodes</th>
    <td>Supports multiple nodes.</td>
  </tr>
  <tr>
    <th>IP Family</th>
    <td>Supports `ipv4`.</td>
  </tr>
  <tr>
    <th>Limitations</th>
    <td><p>Provising an OKE cluster does take between 8 to 10 minutes. If needed, some timeouts in your CI pipelines might have to be adjusted.</p><p>For additional limitations that apply to all distributions, see <a href="testing-about#limitations">Limitations</a>.</p></td>
  </tr>
  <tr>
    <th>Common Use Cases</th>
    <td>Customer release tests</td>
  </tr>
</table>

## Replicated Instance Types {#types}

When creating a VM-based cluster with Compatibility Matrix, you must specify a Replicated instance type.

<table>
  <tr>
    <th width="30%">Type</th>
    <th width="35%">Memory (GiB)</th>
    <th width="35%">VCPU Count</th>
  </tr>
  <tr>
    <th>r1.small</th>
    <td>8 GB</td>
    <td>2 VCPUs</td>
  </tr>
  <tr>
    <th>r1.medium</th>
    <td>16 GB</td>
    <td>4 VCPUs</td>
  </tr>
  <tr>
    <th>r1.large</th>
    <td>32 GB</td>
    <td>8 VCPUs</td>
  </tr>
  <tr>
    <th>r1.xlarge</th>
    <td>64 GB</td>
    <td>16 VCPUs</td>
  </tr>
  <tr>
    <th>r1.2xlarge</th>
    <td>128 GB</td>
    <td>32 VCPUs</td>
  </tr>
</table>

## Kubernetes Version Support Policy

We do not maintain forks or patches of the supported distributions. When a Kubernetes version in Compatibility Matrix is out of support (EOL), Replicated will attempt to continue to support this version for six months for compatibility testing to support customers who are running out-of-date versions of Kubernetes. In the event that a critical security issue or bug is found and unresolved, we might discontinue support for EOL versions of Kubernetes prior to 6 months post EOL.


---


import HaArchitecture from "../partials/embedded-cluster/_multi-node-ha-arch.mdx"

# Managing Multi-Node Clusters with Embedded Cluster

This topic describes managing nodes in clusters created with Replicated Embedded Cluster, including how to add nodes and enable high-availability for multi-node clusters.

## Limitations

Multi-node clusters with Embedded Cluster have the following limitations:

* Support for multi-node clusters with Embedded Cluster is Beta. Only single-node embedded clusters are Generally Available (GA).

* High availability for Embedded Cluster in an Alpha feature. This feature is subject to change, including breaking changes. For more information about this feature, reach out to Alex Parker at [alexp@replicated.com](mailto:alexp@replicated.com).

* The same Embedded Cluster data directory used at installation is used for all nodes joined to the cluster. This is either the default `/var/lib/embedded-cluster` directory or the directory set with the [`--data-dir`](/reference/embedded-cluster-install#flags) flag. You cannot choose a different data directory for Embedded Cluster when joining nodes.

* More than one controller node should not be joined at the same time. When joining a controller node, a warning is printed that explains that the user should not attempt to join another node until the controller node joins successfully.

## Add Nodes to a Cluster (Beta) {#add-nodes}

You can add nodes to create a multi-node cluster in online (internet-connected) and air-gapped (limited or no outbound internet access) environments. The Admin Console provides the join command that you use to join nodes to the cluster.

:::note
Multi-node clusters are not highly available by default. For information about enabling high availability, see [Enable High Availability for Multi-Node Clusters (Alpha)](#ha) below.
:::

To add nodes to a cluster:

1. (Optional) In the Embedded Cluster Config, configure the `roles` key to customize node roles. For more information, see [roles](/reference/embedded-config#roles) in _Embedded Cluster Config_. When you are done, create and promote a new release with the updated Config.

1. Do one of the following to get the join command from the Admin Console:

   1. To add nodes during the application installation process, follow the steps in [Online Installation with Embedded Cluster](/enterprise/installing-embedded) or [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap) to install. A **Nodes** screen is displayed as part of the installation flow in the Admin Console that allows you to choose a node role and copy the relevant join command.

   1. Otherwise, if you have already installed the application:

         1. Log in to the Admin Console.
         
         1. If you promoted a new release that configures the `roles` key in the Embedded Cluster Config, update the instance to the new version. See [Performing Updates in Embedded Clusters](/enterprise/updating-embedded).
         
         1. Go to **Cluster Management > Add node** at the top of the page.

            <img alt="Add node page in the Admin Console" src="/images/admin-console-add-node.png" width="600px"/>

            [View a larger version of this image](/images/admin-console-add-node.png)
   
1. Either on the Admin Console **Nodes** screen that is displayed during installation or in the **Add a Node** dialog, select one or more roles for the new node that you will join. Copy the join command.

     Note the following:

     * If the Embedded Cluster Config [roles](/reference/embedded-config#roles) key is not configured, all new nodes joined to the cluster are assigned the `controller` role by default. The `controller` role designates nodes that run the Kubernetes control plane. Controller nodes can also run other workloads, such as application or Replicated KOTS workloads.

     * Roles are not updated or changed after a node is added. If you need to change a node’s role, reset the node and add it again with the new role.

     * For multi-node clusters with high availability (HA), at least three `controller` nodes are required. You can assign both the `controller` role and one or more `custom` roles to the same node. For more information about creating HA clusters with Embedded Cluster, see [Enable High Availability for Multi-Node Clusters (Alpha)](#ha) below.

     * To add non-controller or _worker_ nodes that do not run the Kubernetes control plane, select one or more `custom` roles for the node and deselect the `controller` role.

1. Do one of the following to make the Embedded Cluster installation assets available on the machine that you will join to the cluster:

    * **For online (internet-connected) installations**: SSH onto the machine that you will join. Then, use the same commands that you ran during installation to download and untar the Embedded Cluster installation assets on the machine. See [Online Installation with Embedded Cluster](/enterprise/installing-embedded).

    * **For air gap installations with limited or no outbound internet access**: On a machine that has internet access, download the Embedded Cluster installation assets (including the air gap bundle) using the same command that you ran during installation. See [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap). Then, move the downloaded assets to the air-gapped machine that you will join, and untar. 

    :::important
    The Embedded Cluster installation assets on each node must all be the same version. If you use a different version than what is installed elsewhere in the cluster, the cluster will not be stable. To download a specific version of the Embedded Cluster assets, select a version in the **Embedded cluster install instructions** dialog.
    :::

1. On the machine that you will join to the cluster, run the join command that you copied from the Admin Console.

    **Example:**

    ```bash
    sudo ./APP_SLUG join 10.128.0.32:30000 TxXboDstBAamXaPdleSK7Lid
    ```
    **Air Gap Example:**

    ```bash
    sudo ./APP_SLUG join --airgap-bundle APP_SLUG.airgap 10.128.0.32:30000 TxXboDstBAamXaPdleSK7Lid
    ```

1. In the Admin Console, either on the installation **Nodes** screen or on the **Cluster Management** page, verify that the node appears. Wait for the node's status to change to Ready.

1. Repeat these steps for each node you want to add.    

## Enable High Availability for Multi-Node Clusters (Alpha) {#ha}

Multi-node clusters are not highly available by default. The first node of the cluster is special and holds important data for Kubernetes and KOTS, such that the loss of this node would be catastrophic for the cluster. Enabling high availability (HA) requires that at least three controller nodes are present in the cluster. Users can enable HA when joining the third node.

:::important
High availability for Embedded Cluster in an Alpha feature. This feature is subject to change, including breaking changes. For more information about this feature, reach out to Alex Parker at [alexp@replicated.com](mailto:alexp@replicated.com).
:::

### HA Architecture

<HaArchitecture/>

For more information about the Embedded Cluster built-in extensions, see [Built-In Extensions](/vendor/embedded-overview#built-in-extensions) in _Embedded Cluster Overview_.

### Requirements

Enabling high availability has the following requirements:

* High availability is supported with Embedded Cluster 1.4.1 or later.

* High availability is supported only for clusters where at least three nodes with the `controller` role are present.

### Limitations

Enabling high availability has the following limitations:

* High availability for Embedded Cluster in an Alpha feature. This feature is subject to change, including breaking changes. For more information about this feature, reach out to Alex Parker at [alexp@replicated.com](mailto:alexp@replicated.com).

* The `--enable-ha` flag serves as a feature flag during the Alpha phase. In the future, the prompt about migrating to high availability will display automatically if the cluster is not yet HA and you are adding the third or more controller node. 

* HA multi-node clusters use rqlite to store support bundles up to 100 MB in size. Bundles over 100 MB can cause rqlite to crash and restart.

### Best Practices for High Availability

Consider the following best practices and recommendations for creating HA clusters:

* At least three _controller_ nodes that run the Kubernetes control plane are required for HA. This is because clusters use a quorum system, in which more than half the nodes must be up and reachable. In clusters with three controller nodes, the Kubernetes control plane can continue to operate if one node fails because a quorum can still be reached by the remaining two nodes. By default, with Embedded Cluster, all new nodes added to a cluster are controller nodes. For information about customizing the `controller` node role, see [roles](/reference/embedded-config#roles) in _Embedded Cluster Config_.

* Always use an odd number of controller nodes in HA clusters. Using an odd number of controller nodes ensures that the cluster can make decisions efficiently with quorum calculations. Clusters with an odd number of controller nodes also avoid split-brain scenarios where the cluster runs as two, independent groups of nodes, resulting in inconsistencies and conflicts.

* You can have any number of _worker_ nodes in HA clusters. Worker nodes do not run the Kubernetes control plane, but can run workloads such as application or Replicated KOTS workloads.

### Create a Multi-Node HA Cluster

To create a multi-node HA cluster:

1. Set up a cluster with at least two controller nodes. You can do an online (internet-connected) or air gap installation. For more information, see [Online Installation with Embedded Cluster](/enterprise/installing-embedded) or [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap).

1. SSH onto a third node that you want to join to the cluster as a controller.

1. Run the join command provided in the Admin Console **Cluster Management** tab and pass the `--enable-ha` flag. For example:

     ```bash
     sudo ./APP_SLUG join --enable-ha 10.128.0.80:30000 tI13KUWITdIerfdMcWTA4Hpf
     ```

1. After the third node joins the cluster, type `y` in response to the prompt asking if you want to enable high availability.

    ![high availability command line prompt](/images/embedded-cluster-ha-prompt.png)
    [View a larger version of this image](/images/embedded-cluster-ha-prompt.png)

1. Wait for the migration to complete.


---


import UpdateAirGapAdm from "../partials/embedded-cluster/_update-air-gap-admin-console.mdx"
import UpdateAirGapCli from "../partials/embedded-cluster/_update-air-gap-cli.mdx"
import UpdateAirGapOverview from "../partials/embedded-cluster/_update-air-gap-overview.mdx"
import DoNotDowngrade from "../partials/embedded-cluster/_warning-do-not-downgrade.mdx"
import Prerequisites from "../partials/install/_ec-prereqs.mdx"

# Air Gap Installation with Embedded Cluster

This topic describes how to install applications with Embedded Cluster on a virtual machine (VM) or bare metal server with no outbound internet access.

## Overview

When an air gap bundle is built for a release containing an Embedded Cluster Config, both an application air gap bundle and an Embedded Cluster air gap bundle are built. The application air gap bundle can be used for air gap installations with Replicated kURL or with Replicated KOTS in an existing cluster. The Embedded Cluster air gap bundle is used for air gap installations with Embedded Cluster.

The Embedded Cluster air gap bundle not only contains the assets normally contained in an application air gap bundle (`airgap.yaml`, `app.tar.gz`, and an images directory), but it also contains an `embedded-cluster` directory with the assets needed to install the infrastructure (Embedded Cluster/k0s and [extensions](/reference/embedded-config#extensions).

During installation with Embedded Cluster in air gap environments, a Docker registry is deployed to the cluster to store application images. Infrastructure images (for Embedded Cluster and Helm extensions) and the Helm charts are preloaded on each node at installation time.

### Requirement

Air gap installations are supported with Embedded Cluster version 1.3.0 or later.

### Limitations and Known Issues

Embedded Cluster installations in air gap environments have the following limitations and known issues:

* If you pass `?airgap=true` to the `replicated.app` endpoint but an air gap bundle is not built for the latest release, the API will not return a 404. Instead it will return the tarball without the air gap bundle (as in, with the installer and the license in it, like for online installations).

* Images used by Helm extensions must not refer to a multi-architecture image by digest. Only x64 images are included in air gap bundles, and the digest for the x64 image will be different from the digest for the multi-architecture image, preventing the image from being discovered in the bundle. An example of a chart that does this is ingress-nginx/ingress-nginx chart. For an example of how the digests should be set to empty string to pull by tag only, see [extensions](/reference/embedded-config#extensions) in _Embedded Cluster Config_.

* Images for Helm extensions are loaded directly into containerd so that they are available without internet access. But if an image used by a Helm extension has **Always** set as the image pull policy, Kubernetes will try to pull the image from the internet. If necessary, use the Helm values to set `IfNotPresent` as the image pull policy to ensure the extension works in air gap environments.

* On the channel release history page, the links for **Download air gap bundle**, **Copy download URL**, and **View bundle contents** pertain to the application air gap bundle only, not the Embedded Cluster bundle.

## Prerequisites

Before you install, complete the following prerequisites:

<Prerequisites/>

## Install

To install with Embedded Cluster in an air gap environment:

1. In the [Vendor Portal](https://vendor.replicated.com), go the channel where the target release was promoted to build the air gap bundle. Do one of the following:
     * If the **Automatically create airgap builds for newly promoted releases in this channel** setting is enabled on the channel, watch for the build status to complete.
     * If automatic air gap builds are not enabled, go to the **Release history** page for the channel and build the air gap bundle manually.
     
    :::note
    Errors in building either the application air gap bundle or the Embedded Cluster infrastructure will be shown if present.
    :::

1. Go to **Customers** and click on the target customer. 

1. On the **Manage customer** tab, under **License options**, enable the **Airgap Download Enabled** license field.

1. At the top of the page, click **Install instructions > Embedded Cluster**.

     ![Customer install instructions drop down button](/images/customer-install-instructions-dropdown.png)

     [View a larger version of this image](/images/customer-install-instructions-dropdown.png)
    
1. In the **Embedded Cluster install instructions** dialog, verify that the **Install in an air gap environment** checkbox is enabled.

    <img alt="Embedded cluster install instruction dialog" src="/images/embedded-cluster-install-dialog-airgap.png" width="500px"/>

    [View a larger version of this image](/images/embedded-cluster-install-dialog-airgap.png)

1. (Optional) For **Select a version**, select a specific application version to install. By default, the latest version is selected.

1. SSH onto the machine where you will install.

1. On a machine with internet access, run the curl command to download the air gap installation assets as a `.tgz`.

1. Move the downloaded `.tgz` to the air-gapped machine where you will install. 

1. On your air-gapped machine, untar the `.tgz` following the instructions provided in the **Embedded Cluster installation instructions** dialog. This will produce three files:
    * The installer
    * The license
    * The air gap bundle (`APP_SLUG.airgap`)

1. Install the application with the installation command copied from the **Embedded Cluster installation instructions** dialog:

    ```bash
    sudo ./APP_SLUG install --license license.yaml --airgap-bundle APP_SLUG.airgap
    ```
    Where `APP_SLUG` is the unique application slug.
    
    :::note
    Embedded Cluster supports installation options such as installing behind a proxy and changing the data directory used by Embedded Cluster. For the list of flags supported with the Embedded Cluster `install` command, see [Embedded Cluster Install Command Options](/reference/embedded-cluster-install).
    :::

1. When prompted, enter a password for accessing the KOTS Admin Console.

     The installation command takes a few minutes to complete. During installation, Embedded Cluster completes tasks to prepare the cluster and install KOTS in the cluster. Embedded Cluster also automatically runs a default set of [_host preflight checks_](/vendor/embedded-using#about-host-preflight-checks) which verify that the environment meets the requirements for the installer.

      **Example output:**

      ```bash
      ? Enter an Admin Console password: ********
      ? Confirm password: ********
      ✔  Host files materialized!
      ✔  Running host preflights
      ✔  Node installation finished!
      ✔  Storage is ready!
      ✔  Embedded Cluster Operator is ready!
      ✔  Admin Console is ready!
      ✔  Additional components are ready!
      Visit the Admin Console to configure and install gitea-kite: http://104.155.145.60:30000
      ```

      At this point, the cluster is provisioned and the Admin Console is deployed, but the application is not yet installed.

1. Go to the URL provided in the output to access to the Admin Console.

1. On the Admin Console landing page, click **Start**.

1. On the **Secure the Admin Console** screen, review the instructions and click **Continue**. In your browser, follow the instructions that were provided on the **Secure the Admin Console** screen to bypass the warning.

1. On the **Certificate type** screen, either select **Self-signed** to continue using the self-signed Admin Console certificate or click **Upload your own** to upload your own private key and certificacte.

    By default, a self-signed TLS certificate is used to secure communication between your browser and the Admin Console. You will see a warning in your browser every time you access the Admin Console unless you upload your own certificate.

1. On the login page, enter the Admin Console password that you created during installation and click **Log in**.

1. On the **Nodes** page, you can view details about the machine where you installed, including its node role, status, CPU, and memory. 

     Optionally, add nodes to the cluster before deploying the application. For more information about joining nodes, see [Managing Multi-Node Clusters with Embedded Cluster](/enterprise/embedded-manage-nodes). Click **Continue**.

1. On the **Configure [App Name]** screen, complete the fields for the application configuration options. Click **Continue**.

1. On the **Validate the environment & deploy [App Name]** screen, address any warnings or failures identified by the preflight checks and then click **Deploy**.

    Preflight checks are conformance tests that run against the target namespace and cluster to ensure that the environment meets the minimum requirements to support the application.

The Admin Console dashboard opens.

On the Admin Console dashboard, the application status changes from Missing to Unavailable while the application is being installed. When the installation is complete, the status changes to Ready. For example:

![Admin console dashboard showing ready status](/images/gitea-ec-ready.png)

[View a larger version of this image](/images/gitea-ec-ready.png)

---


import ConfigValuesExample from "../partials/configValues/_configValuesExample.mdx"
import ConfigValuesProcedure from "../partials/configValues/_config-values-procedure.mdx"

# Automating Installation with Embedded Cluster

This topic describes how to install an application with Replicated Embedded Cluster from the command line, without needing to access the Replicated KOTS Admin Console.

## Overview

A common use case for installing with Embedded Cluster from the command line is to automate installation, such as performing headless installations as part of CI/CD pipelines.

With headless installation, you provide all the necessary installation assets, such as the license file and the application config values, with the installation command rather than through the Admin Console UI. Any preflight checks defined for the application run automatically during headless installations from the command line rather than being displayed in the Admin Console.

## Prerequisite

Create a ConfigValues YAML file to define the configuration values for the application release. The ConfigValues file allows you to pass the configuration values for an application from the command line with the install command, rather than through the Admin Console UI. For air-gapped environments, ensure that the ConfigValues file can be accessed from the installation environment. 

The KOTS ConfigValues file includes the fields that are defined in the KOTS Config custom resource for an application release, along with the user-supplied and default values for each field, as shown in the example below:

<ConfigValuesExample/>

<ConfigValuesProcedure/>

## Online (Internet-Connected) Installation

To install with Embedded Cluster in an online environment:

1. Follow the steps provided in the Vendor Portal to download and untar the Embedded Cluster installation assets. For more information, see [Online Installation with Embedded Cluster](/enterprise/installing-embedded).

1. Run the following command to install:

    ```bash
    sudo ./APP_SLUG install --license-file PATH_TO_LICENSE \
      --config-values PATH_TO_CONFIGVALUES \
      --admin-console-password ADMIN_CONSOLE_PASSWORD
    ```

    Replace:
    * `APP_SLUG` with the unique slug for the application.
    * `LICENSE_FILE` with the customer license.
    * `ADMIN_CONSOLE_PASSWORD` with a password for accessing the Admin Console.
    * `PATH_TO_CONFIGVALUES` with the path to the ConfigValues file.

## Air Gap Installation

To install with Embedded Cluster in an air-gapped environment:

1. Follow the steps provided in the Vendor Portal to download and untar the Embedded Cluster air gap installation assets. For more information, see [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap).

1. Ensure that the Embedded Cluster installation assets are available on the air-gapped machine, then run the following command to install:

    ```bash
    sudo ./APP_SLUG install --license-file PATH_TO_LICENSE \
      --config-values PATH_TO_CONFIGVALUES \
      --admin-console-password ADMIN_CONSOLE_PASSWORD \
      --airgap-bundle PATH_TO_AIRGAP_BUNDLE
    ```

    Replace:
    * `APP_SLUG` with the unique slug for the application.
    * `LICENSE_FILE` with the customer license.
    * `PATH_TO_CONFIGVALUES` with the path to the ConfigValues file.
    * `ADMIN_CONSOLE_PASSWORD` with a password for accessing the Admin Console.
    * `PATH_TO_AIRGAP_BUNDLE` with the path to the Embedded Cluster `.airgap` bundle for the release.

---


import EmbeddedClusterRequirements from "../partials/embedded-cluster/_requirements.mdx"
import EmbeddedClusterPortRequirements from "../partials/embedded-cluster/_port-reqs.mdx"
import FirewallOpeningsIntro from "../partials/install/_firewall-openings-intro.mdx"

# Embedded Cluster Installation Requirements

This topic lists the installation requirements for Replicated Embedded Cluster. Ensure that the installation environment meets these requirements before attempting to install.

## System Requirements

<EmbeddedClusterRequirements/>

## Port Requirements

<EmbeddedClusterPortRequirements/>

## Firewall Openings for Online Installations with Embedded Cluster {#firewall}

<FirewallOpeningsIntro/>

<table>
  <tr>
      <th width="50%">Domain</th>
      <th>Description</th>
  </tr>
  <tr>
      <td>`proxy.replicated.com`</td>
      <td><p>Private Docker images are proxied through `proxy.replicated.com`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `proxy.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L52-L57) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`replicated.app`</td>
      <td><p>Upstream application YAML and metadata is pulled from `replicated.app`. The current running version of the application (if any), as well as a license ID and application ID to authenticate, are all sent to `replicated.app`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `replicated.app`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L60-L65) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`registry.replicated.com` &#42;</td>
      <td><p>Some applications host private images in the Replicated registry at this domain. The on-prem docker client uses a license ID to authenticate to `registry.replicated.com`. This domain is owned by Replicated, Inc which is headquartered in Los Angeles, CA.</p><p> For the range of IP addresses for `registry.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L20-L25) in GitHub.</p></td>
  </tr>
</table>

&#42; Required only if the application uses the [Replicated private registry](/vendor/private-images-replicated).

## About Firewalld Configuration

When Firewalld is enabled in the installation environment, Embedded Cluster modifies the Firewalld config to allow traffic over the pod and service networks and to open the required ports on the host. No additional configuration is required.

The following rule is added to Firewalld:

```xml
<?xml version="1.0" encoding="utf-8"?>
<zone target="ACCEPT">
  <interface name="cali+"/>
  <interface name="tunl+"/>
  <interface name="vxlan-v6.calico"/>
  <interface name="vxlan.calico"/>
  <interface name="wg-v6.cali"/>
  <interface name="wireguard.cali"/>
  <source address="[pod-network-cidr]"/>
  <source address="[service-network-cidr]"/>
</zone>
```

The following ports are opened in the default zone:

<table>
<tr>
  <th>Port</th>
  <th>Protocol</th>
</tr>
<tr>
  <td>6443</td>
  <td>TCP</td>
</tr>
<tr>
  <td>10250</td>
  <td>TCP</td>
</tr>
<tr>
  <td>9443</td>
  <td>TCP</td>
</tr>
<tr>
  <td>2380</td>
  <td>TCP</td>
</tr>
<tr>
  <td>4789</td>
  <td>UDP</td>
</tr>
</table>

---


import Prerequisites from "../partials/install/_ec-prereqs.mdx"

# Online Installation with Embedded Cluster

This topic describes how to install an application in an online (internet-connected) environment with the Replicated Embedded Cluster installer. For information about air gap installations with Embedded Cluster, see [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap).

## Prerequisites

Before you install, complete the following prerequisites:

<Prerequisites/>

* Ensure that the required domains are accessible from servers performing the installation. See [Firewall Openings for Online Installations](/enterprise/installing-embedded-requirements#firewall).

## Install

To install an application with Embedded Cluster:

1. In the [Vendor Portal](https://vendor.replicated.com), go to **Customers** and click on the target customer. Click **Install instructions > Embedded Cluster**.

     ![Customer install instructions drop down button](/images/customer-install-instructions-dropdown.png)

     [View a larger version of this image](/images/customer-install-instructions-dropdown.png)

     The **Embedded Cluster install instructions** dialog is displayed.

     <img alt="Embedded cluster install instruction dialog" src="/images/embedded-cluster-install-dialog.png" width="500px"/>

     [View a larger version of this image](/images/embedded-cluster-install-dialog.png)

1. (Optional) In the **Embedded Cluster install instructions** dialog, under **Select a version**, select a specific application version to install. By default, the latest version is selected.

1. SSH onto the machine where you will install.

1. Run the first command in the **Embedded Cluster install instructions** dialog to download the installation assets as a `.tgz`.

1. Run the second command to extract the `.tgz`. The will produce the following files:

    * The installer
    * The license

1. Run the third command to install the release:

    ```bash
    sudo ./APP_SLUG install --license LICENSE_FILE
    ```
    Where:
    * `APP_SLUG` is the unique slug for the application.
    * `LICENSE_FILE` is the customer license.
    <br/>
    :::note
    Embedded Cluster supports installation options such as installing behind a proxy and changing the data directory used by Embedded Cluster. For the list of flags supported with the Embedded Cluster `install` command, see [Embedded Cluster Install Command Options](/reference/embedded-cluster-install).
    :::

1. When prompted, enter a password for accessing the KOTS Admin Console.

     The installation command takes a few minutes to complete. During installation, Embedded Cluster completes tasks to prepare the cluster and install KOTS in the cluster. Embedded Cluster also automatically runs a default set of [_host preflight checks_](/vendor/embedded-using#about-host-preflight-checks) which verify that the environment meets the requirements for the installer.

      **Example output:**

      ```bash
      ? Enter an Admin Console password: ********
      ? Confirm password: ********
      ✔  Host files materialized!
      ✔  Running host preflights
      ✔  Node installation finished!
      ✔  Storage is ready!
      ✔  Embedded Cluster Operator is ready!
      ✔  Admin Console is ready!
      ✔  Additional components are ready!
      Visit the Admin Console to configure and install gitea-kite: http://104.155.145.60:30000
      ```

      At this point, the cluster is provisioned and the Admin Console is deployed, but the application is not yet installed.

1. Go to the URL provided in the output to access to the Admin Console.

1. On the Admin Console landing page, click **Start**.

1. On the **Secure the Admin Console** screen, review the instructions and click **Continue**. In your browser, follow the instructions that were provided on the **Secure the Admin Console** screen to bypass the warning.

1. On the **Certificate type** screen, either select **Self-signed** to continue using the self-signed Admin Console certificate or click **Upload your own** to upload your own private key and certificacte.

    By default, a self-signed TLS certificate is used to secure communication between your browser and the Admin Console. You will see a warning in your browser every time you access the Admin Console unless you upload your own certificate.

1. On the login page, enter the Admin Console password that you created during installation and click **Log in**.

1. On the **Nodes** page, you can view details about the machine where you installed, including its node role, status, CPU, and memory. 

     Optionally, add nodes to the cluster before deploying the application. For more information about joining nodes, see [Managing Multi-Node Clusters with Embedded Cluster](/enterprise/embedded-manage-nodes). Click **Continue**.

1. On the **Configure [App Name]** screen, complete the fields for the application configuration options. Click **Continue**.

1. On the **Validate the environment & deploy [App Name]** screen, address any warnings or failures identified by the preflight checks and then click **Deploy**.

    Preflight checks are conformance tests that run against the target namespace and cluster to ensure that the environment meets the minimum requirements to support the application.

The Admin Console dashboard opens.

On the Admin Console dashboard, the application status changes from Missing to Unavailable while the application is being installed. When the installation is complete, the status changes to Ready. For example:

![Admin console dashboard showing ready status](/images/gitea-ec-ready.png)

[View a larger version of this image](/images/gitea-ec-ready.png)

---


import ProxyLimitations from "../partials/embedded-cluster/_proxy-install-limitations.mdx"
import ProxyRequirements from "../partials/embedded-cluster/_proxy-install-reqs.mdx"


# Embedded Cluster Install Command Options

This topic describes the options available with the Embedded Cluster install command. For more information about how to install with Embedded Cluster, see [Online Installation with Embedded Cluster](/enterprise/installing-embedded) or [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded).

## Usage

```bash
sudo ./APP_SLUG install --license LICENSE_FILE [flags]
```
* `APP_SLUG` is the unique application slug
* `LICENSE_FILE` is the customer's license

## Flags

<table>
  <tr>
    <th width="35%">Flag</th>
    <th width="65%">Description</th>
  </tr>
  <tr>
     <td>`--admin-console-password`</td>
     <td>
        <p>Set the password for the Admin Console. The password must be at least six characters in length. If not set, the user is prompted to provide an Admin Console password.</p>
      </td>
  </tr>
  <tr>
     <td>`--admin-console-port`</td>
     <td>
        <p>Port on which to run the KOTS Admin Console. **Default**: By default, the Admin Console runs on port 30000.</p>
        <p>**Limitation:** It is not possible to change the port for the Admin Console during a restore with Embedded Cluster. For more information, see [Disaster Recovery for Embedded Cluster (Alpha)](/vendor/embedded-disaster-recovery).</p>
      </td>
  </tr>
  <tr>
     <td>`--airgap-bundle`</td>
     <td>The Embedded Cluster air gap bundle used for installations in air-gapped environments with no outbound internet access. For information about how to install in an air-gapped environment, see [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap).</td>
  </tr>
  <tr>
     <td>`--cidr`</td>
     <td>
        <p>The range of IP addresses that can be assigned to Pods and Services, in CIDR notation. **Default:** By default, the CIDR block is `10.244.0.0/16`.</p>
        <p>**Requirement**: Embedded Cluster 1.16.0 or later.</p>
     </td>
  </tr>
  <tr>
     <td>`--config-values`</td>
     <td>
        <p>Path to the ConfigValues file for the application. The ConfigValues file can be used to pass the application configuration values from the command line during installation, such as when performing automated installations as part of CI/CD pipelines. For more information, see [Automating Installation with Embedded Cluster](/enterprise/installing-embedded-automation).</p>
        <p><strong>Requirement:</strong> Embedded Cluster 1.18.0 and later.</p>
     </td>
  </tr>
  <tr>
     <td>`--data-dir`</td>
     <td>
        <p>The data directory used by Embedded Cluster. **Default**: `/var/lib/embedded-cluster`</p>
        <p>**Requirement**: Embedded Cluster 1.16.0 or later.</p>
        <p>**Limitations:**</p>
        <ul>
            <li>The data directory for Embedded Cluster cannot be changed after the cluster is installed.</li>
            <li>For multi-node installations, the same data directory that is supplied at installation is used for all nodes joined to the cluster. You cannot choose a different data directory when joining nodes with the Embedded Cluster `join` command. For more information about joining nodes, see [Add Nodes to a Cluster](/enterprise/embedded-manage-nodes#add-nodes) in _Managing Multi-Node Clusters with Embedded Cluster_.</li>
            <li>If you use the `--data-dir` flag to change the data directory during installation, then you must use the same location when restoring in a disaster recovery scenario. For more information about disaster recovery with Embedded Cluster, see [Disaster Recovery for Embedded Cluster](/vendor/embedded-disaster-recovery).</li>
            <li>Replicated does not support using symlinks for the Embedded Cluster data directory. Use the `--data-dir` flag instead of symlinking `/var/lib/embedded-cluster`.</li>
        </ul>
     </td>
  </tr>
  <tr>
     <td>`--http-proxy`</td>
     <td>
        <p>Proxy server to use for HTTP.</p>
        <ProxyRequirements/>
        <ProxyLimitations/>   
     </td>
  </tr>
  <tr>
     <td>`--https-proxy`</td>
     <td>
        <p>Proxy server to use for HTTPS.</p>
        <ProxyRequirements/>
        <ProxyLimitations/>
     </td>
  </tr>
  <tr>
     <td>`--ignore-host-preflights`</td>
     <td>
        <p>When `--ignore-host-preflights` is passed, the host preflight checks are still run, but the user is prompted and can choose to continue with the installation if preflight failures occur. If there are no failed preflights, no user prompt is displayed. Additionally, the Admin Console still runs any application-specific preflight checks before the application is deployed. For more information about the Embedded Cluster host preflight checks, see [About Host Preflight Checks](/vendor/embedded-using#about-host-preflight-checks) in _Using Embedded Cluster_</p>
        <p>Ignoring host preflight checks is _not_ recommended for production installations.</p>
     </td>
  </tr>
  <tr>
     <td>`-l, --license`</td>
     <td>
        <p>Path to the license file</p>
     </td>
  </tr>
  <tr>
     <td>`--local-artifact-mirror-port`</td>
     <td>
        <p>Port on which to run the Local Artifact Mirror (LAM). **Default**: By default, the LAM runs on port 50000.</p>
     </td>
  </tr>
  <tr>
     <td>`--network-interface`</td>
     <td>
        <p>The name of the network interface to bind to for the Kubernetes API. A common use case of `--network-interface` is for multi-node clusters where node communication should happen on a particular network. **Default**: If a network interface is not provided, the first valid, non-local network interface is used.</p>
     </td>
  </tr>
  <tr>
     <td>`--no-proxy`</td>
     <td>
       <p>Comma-separated list of hosts for which not to use a proxy.</p>
       <p>For single-node installations, pass the IP address of the node where you are installing. For multi-node installations, when deploying the first node, pass the list of IP addresses for all nodes in the cluster (typically in CIDR notation). The network interface's subnet will automatically be added to the no-proxy list if the node's IP address is not already included.</p>
       <p>The following are never proxied:</p>
       <ul>
         <li>Internal cluster communication (`localhost`, `127.0.0.1`, `.cluster.local`, `.svc`)</li>
         <li>The CIDR block used for assigning IPs to Kubernetes Pods and Services. By default, the CIDR block is `10.244.0.0/16`. For information about how to change this default, see [Set IP Address Range for Pods and Services](#set-ip-address-range-for-pods-and-services).</li>
       </ul>
       <p>To ensure your application's internal cluster communication is not proxied, use fully qualified domain names like `my-service.my-namespace.svc` or `my-service.my-namespace.svc.cluster.local`.</p>
       <ProxyRequirements/>
       <ProxyLimitations/>
     </td>
  </tr>
  <tr>
     <td>`--private-ca`</td>
     <td>
        <p>The path to trusted certificate authority (CA) certificates. Using the `--private-ca` flag ensures that the CA is trusted by the installation. KOTS writes the CA certificates provided with the `--private-ca` flag to a ConfigMap in the cluster.</p>
        <p>The KOTS [PrivateCACert](/reference/template-functions-static-context#privatecacert) template function returns the ConfigMap containing the private CA certificates supplied with the `--private-ca` flag. You can use this template function to mount the ConfigMap so your containers trust the CA too.</p>
     </td>
  </tr>
  <tr>
     <td>`-y, --yes`</td>
     <td>
        <p>In Embedded Cluster 1.21.0 and later, pass the `--yes` flag to provide an affirmative response to any user prompts for the command. For example, you can pass `--yes` with the `--ignore-host-preflights` flag to ignore host preflight checks during automated installations.</p>
        <p>**Requirement:** Embedded Cluster 1.21.0 and later</p>
     </td>
  </tr>
</table>

## Examples

### Air Gap Install

```bash
sudo ./my-app install --license license.yaml --airgap-bundle myapp.airgap
```

### Change the Admin Console and LAM Ports

```bash
sudo ./my-app install --license license.yaml --admin-console-port=20000 --local-artifact-mirror-port=40000
```

### Change the Data Directory

```bash
sudo ./my-app install --license license.yaml --data-dir /data/embedded-cluster
```

### Headless (Automated) Install

```bash
sudo ./my-app install --license license.yaml \
  --config-values configvalues.yaml \
  --admin-console-password password
```

### Install Behind a Proxy

```bash
sudo ./APP_SLUG install --license license.yaml \
  --http-proxy=HOST:PORT \
  --https-proxy=HOST:PORT \
  --no-proxy=LIST_OF_HOSTS
```
Where:

* `HOST:PORT` is the host and port of the proxy server
* `LIST_OF_HOSTS` is the list of hosts to not proxy. For example, the IP address of the node where you are installing. Or, for multi-node clusters, the list of IP addresses for all nodes in the cluster, typically in CIDR notation.

### Install Behind an MITM Proxy

```bash
sudo ./my-app install --license license.yaml --private-ca /path/to/private-ca-bundle \
  --http-proxy=http://10.128.0.0:3300 \
  --https-proxy=http://10.128.0.0:3300 \
  --no-proxy=123.89.46.4,10.96.0.0/16,*.example.com
```

### Set Admin Console Password

```bash
sudo ./my-app install --license license.yaml --admin-console-password password
```

### Set IP Address Range for Pods and Services

```bash
sudo ./my-app install --license license.yaml --cidr 172.16.136.0/16
```

### Use a Specific Network Interface

```bash
sudo ./my-app install --license license.yaml --network-interface eno167777
```


---


import EmbeddedCluster from "../partials/embedded-cluster/_definition.mdx"
import Requirements from "../partials/embedded-cluster/_requirements.mdx"
import EmbeddedClusterPortRequirements from "../partials/embedded-cluster/_port-reqs.mdx"
import HaArchitecture from "../partials/embedded-cluster/_multi-node-ha-arch.mdx"

# Embedded Cluster Overview

This topic provides an introduction to Replicated Embedded Cluster, including a description of the built-in extensions installed by Embedded Cluster, an overview of the Embedded Cluster single-node and multi-node architecture, and requirements and limitations.

:::note
If you are instead looking for information about creating Kubernetes Installers with Replicated kURL, see the [Replicated kURL](/vendor/packaging-embedded-kubernetes) section.
:::

## Overview

<EmbeddedCluster/>

## Architecture

This section describes the Embedded Cluster architecture, including the built-in extensions deployed by Embedded Cluster.

### Single-Node Architecture

The following diagram shows the architecture of a single-node Embedded Cluster installation for an application named Gitea:

![Embedded Cluster single-node architecture](/images/embedded-architecture-single-node.png)

[View a larger version of this image](/images/embedded-architecture-single-node.png)

As shown in the diagram above, the user downloads the Embedded Cluster installation assets as a `.tgz` in their installation environment. These installation assets include the Embedded Cluster binary, the user's license file, and (for air gap installations) an air gap bundle containing the images needed to install and run the release in an environment with limited or no outbound internet access. 

When the user runs the Embedded Cluster install command, the Embedded Cluster binary first installs the k0s cluster as a systemd service.

After all the Kubernetes components for the cluster are available, the Embedded Cluster binary then installs the Embedded Cluster built-in extensions. For more information about these extensions, see [Built-In Extensions](#built-in-extensions) below.

Any Helm extensions that were included in the [`extensions`](/reference/embedded-config#extensions) field of the Embedded Cluster Config are also installed. The namespace or namespaces where Helm extensions are installed is defined by the vendor in the Embedded Cluster Config.

Finally, Embedded Cluster also installs Local Artifact Mirror (LAM). In air gap installations, LAM is used to store and update images.

### Multi-Node Architecture

The following diagram shows the architecture of a multi-node Embedded Cluster installation:

![Embedded Cluster multi-node architecture](/images/embedded-architecture-multi-node.png)

[View a larger version of this image](/images/embedded-architecture-multi-node.png)

As shown in the diagram above, in multi-node installations, the Embedded Cluster Operator, KOTS, and the image registry for air gap installations are all installed on one controller node.

For installations that include disaster recovery with Velero, the Velero Node Agent runs on each node in the cluster. The Node Agent is a Kubernetes DaemonSet that performs backup and restore tasks such as creating snapshots and transferring data during restores.

Additionally, any Helm [`extensions`](/reference/embedded-config#extensions) that you include in the Embedded Cluster Config are installed in the cluster depending on the given chart and how it is configured to be deployed.

### Multi-Node Architecture with High Availability

:::note
High availability (HA) for multi-node installations with Embedded Cluster is Alpha and is not enabled by default. For more informaiton about enabling HA, see [Enable High Availability for Multi-Node Clusters (Alpha)](/enterprise/embedded-manage-nodes#ha).
:::

<HaArchitecture/>

## Built-In Extensions {#built-in-extensions}

Embedded Cluster includes several built-in extensions. The built-in extensions provide capabilities such as application management and storage. Each built-in extension is installed in its own namespace.

The built-in extensions installed by Embedded Cluster include:

* **Embedded Cluster Operator**: The Operator is used for reporting purposes as well as some clean up operations.

* **KOTS:** Embedded Cluster installs the KOTS Admin Console in the kotsadm namespace. End customers use the Admin Console to configure and install the application. Rqlite is also installed in the kotsadm namespace alongside KOTS. Rqlite is a distributed relational database that uses SQLite as its storage engine. KOTS uses rqlite to store information such as support bundles, version history, application metadata, and other small amounts of data needed to manage the application. For more information about rqlite, see the [rqlite](https://rqlite.io/) website.

* **OpenEBS:** Embedded Cluster uses OpenEBS to provide local PersistentVolume (PV) storage, including the PV storage for rqlite used by KOTS. For more information, see the [OpenEBS](https://openebs.io/docs/) documentation.

* **(Disaster Recovery Only) Velero:** If the installation uses the Embedded Cluster disaster recovery feature, Embedded Cluster installs Velero, which is an open-source tool that provides backup and restore functionality. For more information about Velero, see the [Velero](https://velero.io/docs/latest/) documentation. For more information about the disaster recovery feature, see [Disaster Recovery for Embedded Cluster (Alpha)](/vendor/embedded-disaster-recovery).

* **(Air Gap Only) Image registry:** For air gap installations in environments with limited or no outbound internet access, Embedded Cluster installs an image registry where the images required to install and run the application are pushed. For more information about installing in air-gapped environments, see [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap).

## Comparison to kURL

Embedded Cluster is a successor to Replicated kURL. Compared to kURL, Embedded Cluster offers several improvements such as:
* Significantly faster installation, updates, and node joins
* A redesigned Admin Console UI for managing the cluster
* Improved support for multi-node clusters
* One-click updates of both the application and the cluster at the same time

Additionally, Embedded Cluster automatically deploys several built-in extensions like KOTS and OpenEBS to provide capabilities such as application management and storage. This represents an improvement over kURL because vendors distributing their application with Embedded Cluster no longer need choose and define various add-ons in the installer spec. For additional functionality that is not included in the built-in extensions, such as an ingress controller, vendors can provide their own [`extensions`](/reference/embedded-config#extensions) that will be deployed alongside the application.

## Requirements

### System Requirements

<Requirements/>

### Port Requirements

<EmbeddedClusterPortRequirements/>

## Limitations

Embedded Cluster has the following limitations:

* **Reach out about migrating from kURL**: We are helping several customers migrate from kURL to Embedded Cluster. Reach out to Alex Parker at alexp@replicated.com for more information.

* **Multi-node support is in beta**: Support for multi-node embedded clusters is in beta, and enabling high availability for multi-node clusters is in alpha. Only single-node embedded clusters are generally available. For more information, see [Managing Multi-Node Clusters with Embedded Cluster](/enterprise/embedded-manage-nodes).

* **Disaster recovery is in alpha**: Disaster Recovery for Embedded Cluster installations is in alpha. For more information, see [Disaster Recovery for Embedded Cluster (Alpha)](/vendor/embedded-disaster-recovery).

* **Partial rollback support**: In Embedded Cluster 1.17.0 and later, rollbacks are supported only when rolling back to a version where there is no change to the [Embedded Cluster Config](/reference/embedded-config) compared to the currently-installed version. For example, users can roll back to release version 1.0.0 after upgrading to 1.1.0 only if both 1.0.0 and 1.1.0 use the same Embedded Cluster Config. For more information about how to enable rollbacks for your application in the KOTS Application custom resource, see [allowRollback](/reference/custom-resource-application#allowrollback) in _Application_.

* **Changing node hostnames is not supported**: After a host is added to a Kubernetes cluster, Kubernetes assumes that the hostname and IP address of the host will not change. If you need to change the hostname or IP address of a node, you must first remove the node from the cluster. For more information about the requirements for naming nodes, see [Node name uniqueness](https://kubernetes.io/docs/concepts/architecture/nodes/#node-name-uniqueness) in the Kubernetes documentation.

* **Automatic updates not supported**: Configuring automatic updates from the Admin Console so that new versions are automatically deployed is not supported for Embedded Cluster installations. For more information, see [Configuring Automatic Updates](/enterprise/updating-apps).

* **`minKotsVersion` and `targetKotsVersion` not supported**: The [`minKotsVersion`](/reference/custom-resource-application#minkotsversion-beta) and [`targetKotsVersion`](/reference/custom-resource-application#targetkotsversion) fields in the KOTS Application custom resource are not supported for Embedded Cluster installations. This is because each version of Embedded Cluster includes a particular version of KOTS. Setting `targetKotsVersion` or `minKotsVersion` to a version of KOTS that does not coincide with the version that is included in the specified version of Embedded Cluster will cause Embedded Cluster installations to fail with an error message like: `Error: This version of App Name requires a different version of KOTS from what you currently have installed`. To avoid installation failures, do not use targetKotsVersion or minKotsVersion in releases that support installation with Embedded Cluster.

* **Support bundles over 100MB in the Admin Console**: Support bundles are stored in rqlite. Bundles over 100MB could cause rqlite to crash, causing errors in the installation. You can still generate a support bundle from the command line. For more information, see [Generating Support Bundles for Embedded Cluster](/vendor/support-bundle-embedded).

* **Kubernetes version template functions not supported**: The KOTS [KubernetesVersion](/reference/template-functions-static-context#kubernetesversion), [KubernetesMajorVersion](/reference/template-functions-static-context#kubernetesmajorversion), and [KubernetesMinorVersion](/reference/template-functions-static-context#kubernetesminorversion) template functions do not provide accurate Kubernetes version information for Embedded Cluster installations. This is because these template functions are rendered before the Kubernetes cluster has been updated to the intended version. However, `KubernetesVersion` is not necessary for Embedded Cluster because vendors specify the Embedded Cluster version, which includes a known Kubernetes version.

* **KOTS Auto-GitOps workflow not supported**: Embedded Cluster does not support the KOTS Auto-GitOps workflow. If an end-user is interested in GitOps, consider the Helm install method instead. For more information, see [Installing with Helm](/vendor/install-with-helm).

* **Downgrading Kubernetes not supported**: Embedded Cluster does not support downgrading Kubernetes. The admin console will not prevent end-users from attempting to downgrade Kubernetes if a more recent version of your application specifies a previous Embedded Cluster version. You must ensure that you do not promote new versions with previous Embedded Cluster versions.

* **Templating not supported in Embedded Cluster Config**: The [Embedded Cluster Config](/reference/embedded-config) resource does not support the use of Go template functions, including [KOTS template functions](/reference/template-functions-about). This only applies to the Embedded Cluster Config. You can still use template functions in the rest of your release as usual.

* **Policy enforcement on Embedded Cluster workloads is not supported**: The Embedded Cluster runs workloads that require higher levels of privilege. If your application installs a policy enforcement engine such as Gatekeeper or Kyverno, ensure that its policies are not enforced in the namespaces used by Embedded Cluster.

* **Installing on STIG- and CIS-hardened OS images is not supported**: Embedded Cluster isn't tested on these images, and issues have arisen when trying to install on them.


---


import Prerequisites from "../partials/helm/_helm-install-prereqs.mdx"

# Installing and Updating with Helm in Air Gap Environments

This topic describes how to use Helm to install releases that contain one or more Helm charts in air-gapped environments.

## Overview

Replicated supports installing and updating Helm charts in air-gapped environments with no outbound internet access. In air gap Helm installations, customers are guided through the process with instructions provided in the [Replicated Download Portal](/vendor/releases-share-download-portal).

When air gap Helm installations are enabled, an **Existing cluster with Helm** option is displayed in the Download Portal on the left nav. When selected, **Existing cluster with Helm** displays three tabs (**Install**, **Manual Update**, **Automate Updates**), as shown in the screenshot below:

![download helm option](/images/download-helm.png)

[View a larger version of this image](/images/download-helm.png)

Each tab provides instructions for how to install, perform a manual update, or configure automatic updates, respectively.

These installing and updating instructions assume that your customer is accessing the Download Portal from a workstation that can access the internet and their internal private registry. Direct access to the target cluster is not required.

Each method assumes that your customer is familiar with `curl`, `docker`, `helm`, `kubernetes`, and a bit of `bash`, particularly for automating updates.

## Prerequisites

Before you install, complete the following prerequisites:

* Reach out to your account rep to enable the Helm air gap installation feature.

<Prerequisites/>

## Install

The installation instructions provided in the Download Portal are designed to walk your customer through the first installation of your chart in an air gap environment.

To install with Helm in an air gap environment:

1. In the [Vendor Portal](https://vendor.replicated.com), go to **Customers > [Customer Name] > Reporting**.

1. In the **Download portal** section, click **Visit download portal** to log in to the Download Portal for the customer.

1. In the Download Portal left nav, click **Existing cluster with Helm**. 

     ![download helm option](/images/download-helm.png)

     [View a larger version of this image](/images/download-helm.png)

1. On the **Install** tab, in the **App version** dropdown, select the target application version to install.

1. Run the first command to authenticate into the Replicated proxy registry with the customer's credentials (the `license_id`).

1. Under **Get the list of images**, run the command provided to generate the list of images needed to install.

1. For **(Optional) Specify registry URI**, provide the URI for an internal image registry where you want to push images. If a registry URI is provided, Replicatd automatically updates the commands for tagging and pushing images with the URI.    

1. For **Pull, tag, and push each image to your private registry**, copy and paste the docker commands provided to pull, tag, and push each image to your internal registry.

    :::note
    If you did not provide a URI in the previous step, ensure that you manually replace the image names in the `tag` and `push` commands with the target registry URI.
    :::

1. Run the command to authenticate into the OCI registry that contains your Helm chart.

1. Run the command to install the `preflight` plugin. This allows you to run preflight checks before installing to ensure that the installation environment meets the requirements for the application.

1. For **Download a copy of the values.yaml file** and **Edit the values.yaml file**, run the `helm show values` command provided to download the values file for the Helm chart. Then, edit the values file as needed to customize the configuration of the given chart.

    If you are installing a release that contains multiple Helm charts, repeat these steps to download and edit each values file.

    :::note
    For installations with mutliple charts where two or more of the top-level charts in the release use the same name, ensure that each values file has a unique name to avoid installation error. For more information, see [Installation Fails for Release With Multiple Helm Charts](helm-install-troubleshooting#air-gap-values-file-conflict) in _Troubleshooting Helm Installations_.
    :::

1. For **Determine install method**, select one of the options depending on your ability to access the internet and the cluster from your workstation.

1. Use the commands provided and the values file or files that you edited to run preflight checks and then install the release.

## Perform Updates

This section describes the processes of performing manual and automatic updates with Helm in air gap environments using the instructions provided in the Download Portal.

### Manual Updates

The manual update instructions provided in the Download Portal are similar to the installation instructions.

However, the first step prompts the customer to select their current version an the target version to install. This step takes [required releases](/vendor/releases-about#properties) into consideration, thereby guiding the customer to the versions that are upgradable from their current version. 

The additional steps are consistent with installation process until the `preflight` and `install` commands where customers provide the existing values from the cluster with the `helm get values` command. Your customer will then need to edit the `values.yaml` to reference the new image tags.

If the new version introduces new images or other values, Replicated recommends that you explain this at the top of your release notes so that customers know they will need to make additional edits to the `values.yaml` before installing. 

### Automate Updates

The instructions in the Download Portal for automating updates use API endpoints that your customers can automate against. 

The instructions in the Download Portal provide customers with example commands that can be put into a script that they run periodically (nightly, weekly) using GitHub Actions, Jenkins, or other platforms.

This method assumes that the customer has already done a successful manual installation, including the configuration of the appropriate `values`.

After logging into the registry, the customer exports their current version and uses that to query an endpoint that provides the latest installable version number (either the next required release, or the latest release) and export it as the target version. With the target version, they can now query an API for the list of images. 

With the list of images the provided `bash` script will automate the process of pulling updated images from the repository, tagging them with a name for an internal registry, and then pushing the newly tagged images to their internal registry. 

Unless the customer has set up the `values` to preserve the updated tag (for example, by using the `latest` tag), they need to edit the `values.yaml` to reference the new image tags. After doing so, they can log in to the OCI registry and perform the commands to install the updated chart.

## Use a Harbor or Artifactory Registry Proxy

You can integrate the Replicated proxy registry with an existing Harbor or jFrog Artifactory instance to proxy and cache images on demand. For more information, see [Using a Registry Proxy for Helm Air Gap Installations](using-third-party-registry-proxy).


---


import Helm from "../partials/helm/_helm-definition.mdx"

# About Helm Installations with Replicated

This topic provides an introduction to Helm installations for applications distributed with Replicated.

## Overview

<Helm/>

Replicated strongly recommends that all applications are packaged using Helm because many enterprise users expect to be able to install an application with the Helm CLI. 

Existing releases in the Replicated Platform that already support installation with Replicated KOTS and Replicated Embedded Cluster (and that include one or more Helm charts) can also be installed with the Helm CLI; it is not necessary to create and manage separate releases or channels for each installation method.

For information about how to install with Helm, see:
* [Installing with Helm](/vendor/install-with-helm)
* [Installing and Updating with Helm in Air Gap Environments](helm-install-airgap)

The following diagram shows how Helm charts distributed with Replicated are installed with Helm in online (internet-connected) customer environments:

<img src="/images/helm-install-diagram.png" alt="diagram of a helm chart in a custom environment" width="700px"/> 

[View a larger version of this image](/images/helm-install-diagram.png)

As shown in the diagram above, when a release containing one or more Helm charts is promoted to a channel, the Replicated Vendor Portal automatically extracts any Helm charts included in the release. These charts are pushed as OCI objects to the Replicated registry. The Replicated registry is a private OCI registry hosted by Replicated at `registry.replicated.com`. For information about security for the Replicated registry, see [Replicated Registry Security](packaging-private-registry-security).

For example, if your application in the Vendor Portal is named My App and you promote a release containing a Helm chart with `name: my-chart` to a channel with the slug `beta`, then the Vendor Portal pushes the chart to the following location: `oci://registry.replicated.com/my-app/beta/my-chart`.

Customers can install your Helm chart by first logging in to the Replicated registry with their unique license ID. This step ensures that any customer who installs your chart from the registry has a valid, unexpired license. After the customer logs in to the Replicated registry, they can run `helm install` to install the chart from the registry.

During installation, the Replicated registry injects values into the `global.replicated` key of the parent Helm chart's values file. For more information about the values schema, see [Helm global.replicated Values Schema](helm-install-values-schema).

## Limitations

Helm installations have the following limitations:

* Helm CLI installations do not provide access to any of the features of the Replicated KOTS installer, such as:
    * The KOTS Admin Console
    * Strict preflight checks that block installation
    * Backup and restore with snapshots
    * Required releases with the **Prevent this release from being skipped during upgrades** option


---


import DependencyYaml from "../partials/replicated-sdk/_dependency-yaml.mdx"
import RegistryLogout from "../partials/replicated-sdk/_registry-logout.mdx"
import HelmPackage from "../partials/helm/_helm-package.mdx"

# Packaging a Helm Chart for a Release

This topic describes how to package a Helm chart and the Replicated SDK into a chart archive that can be added to a release.

## Overview

To add a Helm chart to a release, you first add the Replicated SDK as a dependency of the Helm chart and then package the chart and its dependencies as a `.tgz` chart archive.

The Replicated SDK is a Helm chart can be installed as a small service alongside your application. The SDK provides access to key Replicated features, such as support for collecting custom metrics on application instances. For more information, see [About the Replicated SDK](replicated-sdk-overview). 

## Requirements and Recommendations

This section includes requirements and recommendations for Helm charts.

### Chart Version Requirement

The chart version in your Helm chart must comply with image tag format requirements. A valid tag can contain only lowercase and uppercase letters, digits, underscores, periods, and dashes.

The chart version must also comply with the Semantic Versioning (SemVer) specification. When you run the `helm install` command without the `--version` flag, Helm retrieves the list of all available image tags for the chart from the registry and compares them using the SemVer comparison rules described in the SemVer specification. The version that is installed is the version with the largest tag value. For more information about the SemVer specification, see the [Semantic Versioning](https://semver.org) documentation.

### Chart Naming

For releases that contain more than one Helm chart, Replicated recommends that you use unique names for each top-level Helm chart in the release. This aligns with Helm best practices and also avoids potential conflicts in filenames during installation that could cause the installation to fail. For more information, see [Installation Fails for Release With Multiple Helm Charts](helm-install-troubleshooting#air-gap-values-file-conflict) in _Troubleshooting Helm Installations_.

### Helm Best Practices

Replicated recommends that you review the [Best Practices](https://helm.sh/docs/chart_best_practices/) guide in the Helm documentation to ensure that your Helm chart or charts follows the required and recommended conventions.

## Package a Helm Chart {#release}

This procedure shows how to create a Helm chart archive to add to a release. For more information about the Helm CLI commands in this procedure, see the [Helm Commands](https://helm.sh/docs/helm/helm/) section in the Helm documentation.

To package a Helm chart so that it can be added to a release:

1. In your application Helm chart `Chart.yaml` file, add the YAML below to declare the SDK as a dependency. If your application is installed as multiple charts, declare the SDK as a dependency of the chart that customers install first. Do not declare the SDK in more than one chart.

    <DependencyYaml/>
    
    For additional guidelines related to adding the SDK as a dependency, see [Install the SDK as a Subchart](replicated-sdk-installing#install-the-sdk-as-a-subchart) in _Installing the Replicated SDK_. 

1. Update dependencies and package the chart as a `.tgz` file:

    <HelmPackage/>

    :::note
    <RegistryLogout/>
    :::

1. Add the `.tgz` file to a release. For more information, see [Managing Releases with the Vendor Portal](releases-creating-releases) or [Managing Releases with the CLI](releases-creating-cli).

    After the release is promoted, your Helm chart is automatically pushed to the Replicated registry. For information about how to install a release with the Helm CLI, see [Installing with Helm](install-with-helm). For information about how to install Helm charts with KOTS, see [About Distributing Helm Charts with KOTS](/vendor/helm-native-about).


---


import Prerequisites from "../partials/helm/_helm-install-prereqs.mdx"
import FirewallOpeningsIntro from "../partials/install/_firewall-openings-intro.mdx"

# Installing with Helm

This topic describes how to use Helm to install releases that contain one or more Helm charts. For more information about the `helm install` command, including how to override values in a chart during installation, see [Helm Install](https://helm.sh/docs/helm/helm_install/) in the Helm documentation.

## Prerequisites

Before you install, complete the following prerequisites:

<Prerequisites/>

## Firewall Openings for Online Installations with Helm {#firewall}

<FirewallOpeningsIntro/>

<table>
  <tr>
      <th width="50%">Domain</th>
      <th>Description</th>
  </tr>
  <tr>
      <td>`replicated.app` &#42;</td>
      <td><p>Upstream application YAML and metadata is pulled from `replicated.app`. The current running version of the application (if any), as well as a license ID and application ID to authenticate, are all sent to `replicated.app`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `replicated.app`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L60-L65) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`registry.replicated.com`</td>
      <td><p>Some applications host private images in the Replicated registry at this domain. The on-prem docker client uses a license ID to authenticate to `registry.replicated.com`. This domain is owned by Replicated, Inc which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `registry.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L20-L25) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`proxy.replicated.com`</td>
      <td><p>Private Docker images are proxied through `proxy.replicated.com`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `proxy.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L52-L57) in GitHub.</p></td>
  </tr>
</table>

&#42; Required only if the [Replicated SDK](/vendor/replicated-sdk-overview) is included as a dependency of the application Helm chart.

## Install

To install a Helm chart:

1. In the Vendor Portal, go to **Customers** and click on the target customer.

1. Click **Helm install instructions**.

     <img alt="Helm install button" src="/images/helm-install-button.png" width="700px"/>

     [View a larger image](/images/helm-install-button.png)

1. In the **Helm install instructions** dialog, run the first command to log in to the Replicated registry:

     ```bash
     helm registry login registry.replicated.com --username EMAIL_ADDRESS --password LICENSE_ID
     ```
     Where:
     * `EMAIL_ADDRESS` is the customer's email address
     * `LICENSE_ID` is the ID of the customer's license

     :::note
     You can safely ignore the following warning message: `WARNING: Using --password via the CLI is insecure.` This message is displayed because using the `--password` flag stores the password in bash history. This login method is not insecure.

     Alternatively, to avoid the warning message, you can click **(show advanced)** in the **Helm install instructions** dialog to display a login command that excludes the `--password` flag. With the advanced login command, you are prompted for the password after running the command.  
     :::

1. (Optional) Run the second and third commands to install the preflight plugin and run preflight checks. If no preflight checks are defined, these commands are not displayed. For more information about defining and running preflight checks, see [About Preflight Checks and Support Bundles](preflight-support-bundle-about).

1. Run the fourth command to install using Helm:

     ```bash
     helm install RELEASE_NAME oci://registry.replicated.com/APP_SLUG/CHANNEL/CHART_NAME
     ```
     Where:
     * `RELEASE_NAME` is the name of the Helm release.
     * `APP_SLUG` is the slug for the application. For information about how to find the application slug, see [Get the Application Slug](/vendor/vendor-portal-manage-app#slug).
     * `CHANNEL` is the lowercased name of the channel where the release was promoted, such as `beta` or `unstable`. Channel is not required for releases promoted to the Stable channel.
     * `CHART_NAME` is the name of the Helm chart.

     :::note
     To install the SDK with custom RBAC permissions, include the `--set` flag with the `helm install` command to override the value of the `replicated.serviceAccountName` field with a custom service account. For more information, see [Customizing RBAC for the SDK](/vendor/replicated-sdk-customizing#customize-rbac-for-the-sdk).
     :::

1. (Optional) In the Vendor Portal, click **Customers**. You can see that the customer you used to install is marked as **Active** and the details about the application instance are listed under the customer name. 

     **Example**:

     ![example customer in the Vendor Portal with an active instance](/images/sdk-customer-active-example.png)
     [View a larger version of this image](/images/sdk-customer-active-example.png)

---


import SdkValues from "../partials/replicated-sdk/_sdk-values.mdx"

# Helm global.replicated Values Schema

This topic describes the `global.replicated` values that are injected in the values file of an application's parent Helm chart during Helm installations with Replicated.

## Overview

When a user installs a Helm application with the Helm CLI, the Replicated registry injects a set of customer-specific values into the `global.replicated` key of the parent Helm chart's values file.

The values in the `global.replicated` field include the following:

* The fields in the customer's license, such as the field names, descriptions, signatures, values, and any custom license fields that you define. Vendors can use this license information to check entitlements before the application is installed. For more information, see [Checking Entitlements in Helm Charts Before Deployment](/vendor/licenses-reference-helm).

* A base64 encoded Docker configuration file. To proxy images from an external private registry with the Replicated proxy registry, you can use the `global.replicated.dockerconfigjson` field to create an image pull secret for the proxy registry. For more information, see [Proxying Images for Helm Installations](/vendor/helm-image-registry).

The following is an example of a Helm values file containing the `global.replicated` values:

```yaml
# Helm values.yaml
global:
  replicated:
    channelName: Stable
    customerEmail: username@example.com
    customerName: Example Customer
    dockerconfigjson: eyJhdXRocyI6eyJd1dIRk5NbEZFVGsxd2JGUmFhWGxYWm5scloyNVRSV1pPT2pKT2NGaHhUVEpSUkU1...
    licenseFields:
      expires_at:
        description: License Expiration
        name: expires_at
        signature:
          v1: iZBpESXx7fpdtnbMKingYHiJH42rP8fPs0x8izy1mODckGBwVoA... 
        title: Expiration
        value: "2023-05-30T00:00:00Z"
        valueType: String
    licenseID: YiIXRTjiB7R...
    licenseType: dev
```

## `global.replicated` Values Schema

The `global.replicated` values schema contains the following fields:

| Field | Type | Description |
| --- | --- | --- |
| `channelName` | String | The name of the release channel |
| `customerEmail` | String | The email address of the customer |
| `customerName` | String | The name of the customer |
| `dockerconfigjson` | String | Base64 encoded docker config json for pulling images |
| `licenseFields`| | A list containing each license field in the customer's license. Each element under `licenseFields` has the following properties: `description`, `signature`, `title`, `value`, `valueType`. `expires_at` is the default `licenseField` that all licenses include. Other elements under `licenseField` include the custom license fields added by vendors in the Vendor Portal. For more information, see [Managing Customer License Fields](/vendor/licenses-adding-custom-fields). |
| `licenseFields.[FIELD_NAME].description` | String | Description of the license field |
| `licenseFields.[FIELD_NAME].signature.v1` | Object | Signature of the license field |
| `licenseFields.[FIELD_NAME].title` | String | Title of the license field |
| `licenseFields.[FIELD_NAME].value` | String | Value of the license field |
| `licenseFields.[FIELD_NAME].valueType` | String | Type of the license field value |
| `licenseID` | String | The unique identifier for the license |
| `licenseType` | String | The type of license, such as "dev" or "prod". For more information, see [Customer Types](/vendor/licenses-about#customer-types) in _About Customers and Licensing_. |

## Replicated SDK Helm Values

<SdkValues/>

---


---
pagination_prev: null
---

import ApiAbout from "/docs/partials/vendor-api/_api-about.mdx"
import Replicated from "/docs/partials/getting-started/_replicated-definition.mdx"
import Helm from "/docs/partials/helm/_helm-definition.mdx"
import Kots from "/docs/partials/kots/_kots-definition.mdx"
import KotsEntitlement from "/docs/partials/kots/_kots-entitlement-note.mdx"
import SDKOverview from "/docs/partials/replicated-sdk/_overview.mdx"
import CSDL from "/docs/partials/getting-started/_csdl-overview.mdx"
import PreflightSbAbout from "/docs/partials/preflights/_preflights-sb-about.mdx"

# Introduction to Replicated

This topic provides an introduction to the Replicated Platform, including a platform overview and a list of key features. It also describes the Commercial Software Distribution Lifecycle and how Replicated features can be used in each phase of the lifecycle.

## About the Replicated Platform

<Replicated/>

The Replicated Platform features are designed to support ISVs during each phase of the Commercial Software Distribution Lifecycle. For more information, see [Commercial Software Distribution Lifecycle](#csdl) below.

The following diagram demonstrates the process of using the Replicated Platform to distribute an application, install the application in a customer environment, and support the application after installation:

![replicated platform features workflow](/images/replicated-platform.png)

[View a larger version of this image](/images/replicated-platform.png)

The diagram above shows an application that is packaged with the [**Replicated SDK**](/vendor/replicated-sdk-overview). The application is tested in clusters provisioned with the [**Replicated Compatibility Matrix**](/vendor/testing-about), then added to a new release in the [**Vendor Portal**](/vendor/releases-about) using an automated CI/CD pipeline.

The application is then installed by a customer ("Big Bank") on a VM. To install, the customer downloads their license, which grants proxy access to the application images through the [**Replicated proxy registry**](/vendor/private-images-about). They also download the installation assets for the [**Replicated Embedded Cluster**](/vendor/embedded-overview) installer.

Embedded Cluster runs [**preflight checks**](/vendor/preflight-support-bundle-about) to verify that the environment meets the installation requirements, provisions a cluster on the VM, and installs [**Replicated KOTS**](intro-kots) in the cluster. KOTS provides an [**Admin Console**](intro-kots#kots-admin-console) where the customer enters application-specific configurations, runs application preflight checks, optionally joins nodes to the cluster, and then deploys the application. After installation, customers can manage both the application and the cluster from the Admin Console.

Finally, the diagram shows how [**instance data**](/vendor/instance-insights-event-data) is automatically sent from the customer environment to the Vendor Portal by the Replicated SDK API and the KOTS Admin Console. Additionally, tooling from the open source [**Troubleshoot**](https://troubleshoot.sh/docs/collect/) project is used to generate and send [**support bundles**](/vendor/preflight-support-bundle-about), which include logs and other important diagnostic data.

## Replicated Platform Features

The following describes the key features of the Replicated Platform.

### Compatibility Matrix

Replicated Compatibility Matrix can be used to get kubectl access to running clusters within minutes or less. Compatibility Matrix supports various Kubernetes distributions and versions and can be interacted with through the Vendor Portal or the Replicated CLI.

For more information, see [About Compatibility Matrix](/vendor/testing-about).

### Embedded Cluster

Replicated Embedded Cluster is a Kubernetes installer based on the open source Kubernetes distribution k0s. With Embedded Cluster, users install and manage both the cluster and the application together as a single appliance on a VM or bare metal server. In this way, Kubernetes is _embedded_ with the application.

Additionally, each version of Embedded Cluster includes a specific version of [Replicated KOTS](#kots) that is installed in the cluster during installation. KOTS is used by Embedded Cluster to deploy the application and also provides the Admin Console UI where users can manage both the application and the cluster.

For more information, see [Embedded Cluster Overview](/vendor/embedded-overview).

### KOTS (Admin Console) {#kots}

KOTS is a kubectl plugin and in-cluster Admin Console that installs Kubernetes applications in customer-controlled environments.

KOTS is used by [Replicated Embedded Cluster](#embedded-cluster) to deploy applications and also to provide the Admin Console UI where users can manage both the application and the cluster. KOTS can also be used to install applications in existing Kubernetes clusters in customer-controlled environments, including clusters in air-gapped environments with limited or no outbound internet access.

For more information, see [Introduction to KOTS](intro-kots).

### Preflight Checks and Support Bundles

<PreflightSbAbout/>

For more information, see [About Preflight Checks and Support Bundles](/vendor/preflight-support-bundle-about).

### Proxy Registry

The Replicated proxy registry grants proxy access to an application's images using the customer's unique license. This means that customers can get access to application images during installation without the vendor needing to provide registry credentials.

For more information, see [About the Replicated Proxy Registry](/vendor/private-images-about).

### Replicated SDK

The Replicated SDK is a Helm chart that can be installed as a small service alongside your application. It provides an in-cluster API that can be used to communicate with the Vendor Portal. For example, the SDK API can return details about the customer's license or report telemetry on the application instance back to the Vendor Portal.

For more information, see [About the Replicated SDK](/vendor/replicated-sdk-overview).

### Vendor Portal

The Replicated Vendor Portal is the web-based user interface that you can use to configure and manage all of the Replicated features for distributing and managing application releases, supporting your release, viewing customer insights and reporting, and managing teams.

The Vendor Portal can also be interacted with programmatically using the following developer tools:

* **Replicated CLI**: The Replicated CLI can be used to complete tasks programmatically, including all tasks for packaging and managing applications, and managing artifacts such as teams, license files, and so on. For more information, see [Installing the Replicated CLI](/reference/replicated-cli-installing).

* **Vendor API v3**: The Vendor API can be used to complete tasks programmatically, including all tasks for packaging and managing applications, and managing artifacts such as teams and license files. For more information, see [Using the Vendor API v3](/reference/vendor-api-using).

## Commercial Software Distribution Lifecycle {#csdl}

Replicated Platform features are designed to support ISVs in each phase of the Commercial Software Distribution Lifecycle shown below:

![software distribution lifecycle wheel](/images/software-dev-lifecycle.png)

[View a larger version of this image](/images/software-dev-lifecycle.png)

<CSDL/>

For more information about to download a copy of The Commercial Software Distribution Handbook, see [The Commercial Software Distribution Handbook](https://www.replicated.com/the-commercial-software-distribution-handbook).

The following describes the phases of the software distribution lifecycle:

* **[Develop](#develop)**: Application design and architecture decisions align with customer needs, and development teams can quickly iterate on new features.
* **[Test](#test)**: Run automated tests in several customer-representative environments as part of continuous integration and continuous delivery (CI/CD) workflows.
* **[Release](#release)**: Use channels to share releases with external and internal users, publish release artifacts securely, and use consistent versioning.
* **[License](#license)**: Licenses are customized to each customer and are easy to issue, manage, and update.
* **[Install](#install)**: Provide unique installation options depending on customers' preferences and experience levels.
* **[Report](#report)**: Make more informed prioritization decisions by collecting usage and performance metadata for application instances running in customer environments.
* **[Support](#support)**: Diagnose and resolve support issues quickly.

For more information about the Replicated features that support each of these phases, see the sections below.

### Develop

The Replicated SDK exposes an in-cluster API that can be developed against to quickly integrate and test core functionality with an application. For example, when the SDK is installed alongside an application in a customer environment, the in-cluster API can be used to send custom metrics from the instance to the Replicated vendor platform. 

For more information about using the Replicated SDK, see [About the Replicated SDK](/vendor/replicated-sdk-overview).

### Test

The Replicated Compatibility Matrix rapidly provisions ephemeral Kubernetes clusters, including multi-node and OpenShift clusters. When integrated into existing CI/CD pipelines for an application, the Compatibility Matrix can be used to automatically create a variety of customer-representative environments for testing code changes.

For more information, see [About Compatibility Matrix](/vendor/testing-about).

### Release

Release channels in the Replicated Vendor Portal allow ISVs to make different application versions available to different customers, without needing to maintain separate code bases. For example, a "Beta" channel can be used to share beta releases of an application with only a certain subset of customers. 

For more information about working with channels, see [About Channels and Releases](/vendor/releases-about).

Additionally, the Replicated proxy registry grants proxy access to private application images using the customers' license. This ensures that customers have the right access to images based on the channel they are assigned. For more information about using the proxy registry, see [About the Replicated Proxy Registry](/vendor/private-images-about).

### License

Create customers in the Replicated Vendor Portal to handle licensing for your application in both online and air gap environments. For example:
* License free trials and different tiers of product plans
* Create and manage custom license entitlements
* Verify license entitlements both before installation and during runtime
* Measure and report usage

For more information about working with customers and custom license fields, see [About Customers](/vendor/licenses-about).

### Install

Applications distributed with the Replicated Platform can support multiple different installation methods from the same application release, helping you to meet your customers where they are. For example:

* Customers who are not experienced with Kubernetes or who prefer to deploy to a dedicated cluster in their environment can install on a VM or bare metal server with the Replicated Embedded Cluster installer. For more information, see [Embedded Cluster Overview](/vendor/embedded-overview).
* Customers familiar with Kubernetes and Helm can install in their own existing cluster using Helm. For more information, see [Installing with Helm](/vendor/install-with-helm).
* Customers installing into environments with limited or no outbound internet access (often referred to as air-gapped environments) can securely access and push images to their own internal registry, then install using Helm or a Replicated installer. For more information, see [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap) and [Installing and Updating with Helm in Air Gap Environments (Alpha)](/vendor/helm-install-airgap).

### Report

When installed alongside an application, the Replicated SDK and Replicated KOTS automatically send instance data from the customer environment to the Replicated Vendor Portal. This instance data includes health and status indicators, adoption metrics, and performance metrics. For more information, see [About Instance and Event Data](/vendor/instance-insights-event-data).

ISVs can also set up email and Slack notifications to get alerted of important instance issues or performance trends. For more information, see [Configuring Instance Notifications](/vendor/instance-notifications-config).

### Support

Support teams can use Replicated features to more quickly diagnose and resolve application issues. For example:

- Customize and generate support bundles, which collect and analyze redacted information from the customer's cluster, environment, and application instance. See [About Preflights Checks and Support Bundles](/vendor/preflight-support-bundle-about).
- Provision customer-representative environments with Compatibility Matrix to recreate and diagnose issues. See [About Compatibility Matrix](/vendor/testing-about).
- Get insights into an instance's status by accessing telemetry data, which covers the health of the application, the current application version, and details about the infrastructure and cluster where the application is running. For more information, see [Customer Reporting](/vendor/customer-reporting). For more information, see [Customer Reporting](/vendor/customer-reporting).


---


import SDKOverview from "../partials/replicated-sdk/_overview.mdx"
import EmbeddedKubernetes from "../partials/kots/_embedded-kubernetes-definition.mdx"
import Helm from "../partials/helm/_helm-definition.mdx"
import KurlAvailability from "../partials/kurl/_kurl-availability.mdx"

# Replicated FAQs

This topic lists frequently-asked questions (FAQs) for different components of the Replicated Platform.

## Getting Started FAQs

### What are the supported application packaging options?

Replicated strongly recommends that all applications are packaged using Helm.

<Helm/>

Many enterprise customers expect to be able to install an application with Helm in their own cluster. Packaging with Helm allows you to support installation with the Helm CLI and with the Replicated installers (Replicated Emebdded Cluster and Replicated KOTS) from a single release in the Replicated Platform.

For vendors that do not want to use Helm, applications distributed with Replicated can be packaged as Kubernetes manifest files.

### How do I get started with Replicated?

Replicated recommends that new users start by completing one or more labs or tutorials to get familiar with the processes of creating, installing, and iterating on releases for an application with the Replicated Platform.

Then, when you are ready to begin onboarding your own application to the Replicated Platform, see [Replicated Onboarding](replicated-onboarding) for a list of Replicated features to begin integrating.

#### Labs

The following labs in Instruqt provide a hands-on introduction to working with Replicated features, without needing your own sample application or development environment:

* [Distributing Your Application with Replicated](https://play.instruqt.com/embed/replicated/tracks/distributing-with-replicated?token=em_VHOEfNnBgU3auAnN): Learn how to quickly get value from the Replicated Platform for your application.
* [Delivering Your Application as a Kubernetes Appliance](https://play.instruqt.com/embed/replicated/tracks/delivering-as-an-appliance?token=em_lUZdcv0LrF6alIa3): Use Embedded Cluster to distribute Kubernetes and an application together as a single appliance.
* [Avoiding Installation Pitfalls](https://play.instruqt.com/embed/replicated/tracks/avoiding-installation-pitfalls?token=em_gJjtIzzTTtdd5RFG): Learn how to use preflight checks to avoid common installation issues and assure your customer is installing into a supported environment.
* [Closing the Support Information Gap](https://play.instruqt.com/embed/replicated/tracks/closing-information-gap?token=em_MO2XXCz3bAgwtEca): Learn how to use support bundles to close the information gap between your customers and your support team.
* [Protecting Your Assets](https://play.instruqt.com/embed/replicated/tracks/protecting-your-assets?token=em_7QjY34G_UHKoREBd): Assure your customers have the right access to your application artifacts and features using Replicated licensing.

#### Tutorials

The following getting started tutorials demonstrate how to integrate key Replicated features with a sample Helm chart application:  
* [Install a Helm Chart on a VM with Embedded Cluster](/vendor/tutorial-embedded-cluster-setup): Create a release that can be installed on a VM with the Embedded Cluster installer.
* [Install a Helm Chart with KOTS and the Helm CLI](/vendor/tutorial-kots-helm-setup): Create a release that can be installed with both the KOTS installer and the Helm CLI.
* [Set Helm Chart Values with KOTS](/vendor/tutorial-config-setup): Configure the Admin Console Config screen to collect user-supplied values. 
* [Add Preflight Checks to a Helm Chart](/vendor/tutorial-preflight-helm-setup): Create preflight checks for your application by addin a spec for preflight checks to a Secret in the Helm templates.

### What are air gap installations?

_Air gap_ refers to a computer or network that does not have outbound internet access. Air-gapped environments are common for enterprises that require high security, such as government agencies or financial institutions.

Traditionally, air-gapped systems are physically isolated from the network. For example, an air-gapped server might be stored in a separate location away from network-connected servers. Physical access to air-gapped servers is often restricted as well.

It is also possible to use _virtual_ or _logical_ air gaps, in which security controls such as firewalls, role-based access control (RBAC), and encryption are used to logically isolate a device from a network. In this way, network access is still restricted, but there is not a phyiscal air gap that disconnects the device from the network.

Replicated supports installations into air-gapped environments. In an air gap installation, users first download the images and other assets required for installation on an internet-connected device. These installation assets are usually provided in an _air gap bundle_ that ISVs can build in the Replicated Vendor Portal. Then, users transfer the installation assets to their air-gapped machine where they can push the images to an internal private registry and install.

For more information, see:
* [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap)
* [Installing and Updating with Helm in Air Gap Environments](/vendor/helm-install-airgap)

### What is the Commercial Sotware Distribution Lifecycle?

Commercial software distribution is the business process that independent software vendors (ISVs) use to enable enterprise customers to self-host a fully private instance of the vendor's application in an environment controlled by the customer.

Replicated has developed the Commercial Software Distribution Lifecycle to represent the stages that are essential for every company that wants to deliver their software securely and reliably to customer-controlled environments.

This lifecycle was inspired by the DevOps lifecycle and the Software Development Lifecycle (SDLC), but it focuses on the unique things requirements for successfully distributing commercial software to tens, hundreds, or thousands of enterprise customers.

The phases are: 
* Develop
* Test
* Release
* License
* Install
* Report
* Support

For more information about the Replicated features that enhance each phase of the lifecycle, see [Introduction to Replicated](../intro-replicated).

## Compatibility Matrix FAQs

### What types of clusters can I create with Compatibility Matrix?

You can use Compatibility Matrix to get kubectl access to running clusters within minutes or less. Compatibility Matrix supports a variety of VM and cloud distributions, including Red Hat OpenShift, Replicated Embedded Cluster, and Oracle Container Engine for Kubernetes (OKE). For a complete list, see [Supported Compatibility Matrix Cluster Types](/vendor/testing-supported-clusters).

### How does billing work?

Clusters created with Compatibility Matrix are billed by the minute. Per-minute billing begins when the cluster reaches a running status and ends when the cluster is deleted. For more information, see [Billing and Credits](/vendor/testing-about#billing-and-credits).

### How do I buy credits?

To create clusters with Compatibility Matrix, you must have credits in your Vendor Portal account. If you have a contract, you can purchase credits by logging in to the Vendor Portal and going to **[Compatibility Matrix > Buy additional credits](https://vendor.replicated.com/compatibility-matrix)**. Otherwise, to request credits, log in to the Vendor Portal and go to **[Compatibility Matrix > Request more credits](https://vendor.replicated.com/compatibility-matrix)**.

### How do I add Comaptibility Matrix to my CI/CD pipelines?

You can use Replicated CLI commands to integrate Compatibility Matrix into your CI/CD development and production workflows. This allows you to programmatically create multiple different types of clusters where you can deploy and test your application before releasing.

For more information, see [About Integrating with CI/CD](/vendor/ci-overview).

## KOTS and Embedded Cluster FAQs

### What is the Admin Console?

The Admin Console is the user interface deployed by the Replicated KOTS installer. Users log in to the Admin Console to configure and install the application. Users also access to the Admin Console after installation to complete application mangement tasks such as performing updates, syncing their license, and generating support bundles. For installations with Embedded Cluster, the Admin Console also includes a **Cluster Management** tab where users can manage the nodes in the cluster.

The Admin Console is available in installations with Replicated Embedded Cluster and Replicated KOTS.

The following shows an example of the Admin Console dashboard for an Embedded Cluster installation of an application named "Gitea":

<img src="/images/gitea-ec-ready.png" width="800px" alt="admin console dashboard"/>

[View a larger version of this image](/images/gitea-ec-ready.png)

### How do Embedded Cluster installations work?

To install with Embedded Cluster, users first download and extract the Embedded Cluster installation assets for the target application release on their VM or bare metal server. Then, they run an Embedded Cluster installation command to provision the cluster. During installation, Embedded Cluster also installs Replicated KOTS in the cluster, which deploys the Admin Console.

After the installation command finishes, users log in to the Admin Console to provide application configuration values, optionally join more nodes to the cluster, run preflight checks, and deploy the application.

Customer-specific Embedded Cluster installation instructions are provided in the Replicated Vendor Portal. For more information, see [Installing with Embedded Cluster](/enterprise/installing-embedded).

### Does Replicated support installations into air gap environments?

Yes. The Embedded Cluster and KOTS installers support installation in _air gap_ environments with no outbound internet access.

To support air gap installations, vendors can build air gap bundles for their application in the Vendor Portal that contain all the required assets for a specific release of the application. Additionally, Replicated provides bundles that contain the assets for the Replicated installers.

For more information about how to install with Embedded Cluster and KOTS in air gap environments, see [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap) and [Air Gap Installation in Existing Clusters with KOTS](/enterprise/installing-existing-cluster-airgapped).

### Can I deploy Helm charts with KOTS?

Yes. An application deployed with KOTS can use one or more Helm charts, can include Helm charts as components, and can use more than a single instance of any Helm chart. Each Helm chart requires a unique HelmChart custom resource (`apiVersion: kots.io/v1beta2`) in the release.

For more information, see [About Distributing Helm Charts with KOTS](/vendor/helm-native-about).

### What's the difference between Embedded Cluster and kURL?

Replicated Embedded Cluster is a successor to Replicated kURL. Compared to kURL, Embedded Cluster feature offers significantly faster installation, updates, and node joins, a redesigned Admin Console UI, improved support for multi-node clusters, one-click updates that update the application and the cluster at the same time, and more.

<KurlAvailability/>

For more information, see [Embedded Cluster Overview](/vendor/embedded-overview).

### How do I enable Embedded Cluster and KOTS installations for my application?

Releases that support installation with KOTS include the manifests required by KOTS to define the Admin Console experience and install the application.

In addition to the KOTS manifests, releases that support installation with Embedded Cluster also include the Embedded Cluster Config. The Embedded Cluster Config defines aspects of the cluster that will be provisioned and also sets the version of KOTS that will be installed.

For more information, see [Embedded Cluster Overview](/vendor/embedded-overview).

### Can I use my own branding?

The KOTS Admin Console and the Replicated Download Portal support the use of a custom logo. Additionally, software vendors can use custom domains to alias the endpoints for Replicated services.

For more information, see [Customizing the Admin Console and Download Portal](/vendor/admin-console-customize-app-icon) and [About Custom Domains](custom-domains).

## Replicated SDK FAQs

### What is the SDK?

<SDKOverview/>

### Is the SDK supported in air gap environments?

Yes. The Replicated SDK has an _air gap mode_ that allows it to run in environments with no outbound internet access. When installed in air gap mode, the SDK does not attempt to connect to the internet. This avoids any failures that would occur when the SDK is unable to make outbound requests in air gap environments.

For more information, see [Installing the SDK in Air Gap Environments](/vendor/replicated-sdk-airgap).

### How do I develop against the SDK API?

You can use the Replicated SDK in _integration mode_ to develop locally against the SDK API without needing to make real changes in the Replicated Vendor Portal or in your environment.

For more information, see [Developing Against the SDK API](/vendor/replicated-sdk-development).

### How does the Replicated SDK work with KOTS?

The Replicated SDK is a Helm chart that can be installed as a small service alongside an application, or as a standalone component. The SDK can be installed using the Helm CLI or KOTS.

Replicated recommends that all applications include the SDK because it provides access to key functionality not available through KOTS, such as support for sending custom metrics from application instances. When both the SDK and KOTS are installed in a cluster alongside an application, both send instance telemetry to the Vendor Portal.

For more information about the SDK installation options, see [Installing the Replicated SDK](/vendor/replicated-sdk-installing).

## Vendor Portal FAQs

### How do I add and remove team members?

Admins can add, remove, and manage team members from the Vendor Portal. For more information, see [Managing Team Members](/vendor/team-management).

### How do I manage RBAC policies for my team members?

By default, every team has two policies created automatically: Admin and Read Only. If you have an Enterprise plan, you will also have the Sales and Support policies created automatically. These default policies are not configurable.

You can also configure custom RBAC policies if you are on the Enterprise pricing plan. Creating custom RBAC policies lets you limit which areas of the Vendor Portal are accessible to team members, and control read and read/write privileges to groups based on their role. 

For more information, see [Configuring RBAC Policies](/vendor/team-management-rbac-configuring).

### Can I alias Replicated endpoints?

Yes. Replicated supports the use of custom domains to alias the endpoints for Replicated services, such as the Replicated app service and the Replicated proxy registry.

Replicated domains are external to your domain and can require additional security reviews by your customer. Using custom domains as aliases can bring the domains inside an existing security review and reduce your exposure.

For more information, see [Using Custom Domains](/vendor/custom-domains-using).

### How does Replicated collect telemetry from instances of my application?

For instances running in online (internet-connected) customer environments, either Replicated KOTS or the Replicated SDK periodically sends a small amount of data to the Vendor Portal, depending on which is installed in the cluster alongside the application. If both KOTS and the SDK are installed in the cluster, then both send instance data.

For air gap instances, Replicated KOTS and the Replicated SDK collect and store instance telemetry in a Kubernetes Secret in the customer environment. The telemetry stored in the Secret is collected when a support bundle is generated in the environment. When the support bundle is uploaded to the Vendor Portal, the telemetry is associated with the correct customer and instance ID, and the Vendor Portal updates the instance insights and event data accordingly.

For more information, see [About Instance and Event Data](/vendor/instance-insights-event-data).


---


import DependencyYaml from "../partials/replicated-sdk/_dependency-yaml.mdx"
import HelmPackage from "../partials/helm/_helm-package.mdx"
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import HelmChartCr from "../partials/getting-started/_gitea-helmchart-cr-ec.mdx"
import KotsCr from "../partials/getting-started/_gitea-kots-app-cr-ec.mdx"
import K8sCr from "../partials/getting-started/_gitea-k8s-app-cr.mdx"
import EcCr from "../partials/embedded-cluster/_ec-config.mdx"
import Requirements from "../partials/embedded-cluster/_requirements.mdx"

# Replicated Quick Start

This topic provides a quick start workflow to help new users learn about the Replicated Platform. Complete this quick start before you onboard your application to the platform.

## Introduction

This quick start shows how to create, install, and update releases for a sample Helm chart in the Replicated Platform. You will repeat these same basic steps to create and test releases throughout the onboarding process to integrate Replicated features with your own application.

The goals of this quick start are to introduce new Replicated users to the following common tasks for the purpose of preparing to onboard to the Replicated Platform:

* Working with _applications_, _channels_, _releases_, and _customers_ in the Replicated Vendor Portal

* Working with the Replicated CLI

* Installing and updating applications on a VM with Replicated Embedded Cluster

* Managing an installation with the Replicated KOTS Admin Console

## Set Up the Environment

Before you begin, ensure that you have access to a VM that meets the requirements for Embedded Cluster:

<Requirements/>

## Quick Start

1. Create an account in the Vendor Portal. You can either create a new team or join an existing team. For more information, see [Creating a Vendor Account](vendor-portal-creating-account).

1. Create an application using the Replicated CLI:

   1. On your local machine, install the Replicated CLI:

      ```bash
      brew install replicatedhq/replicated/cli
      ```
      For more installation options, see [Installing the Replicated CLI](/reference/replicated-cli-installing).

   1. Authorize the Replicated CLI:

      ```bash
      replicated login
      ```
      In the browser window that opens, complete the prompts to log in to your Vendor Portal account and authorize the CLI.

   1. Create an application named `Gitea`:

      ```bash
      replicated app create Gitea
      ```

   1. Set the `REPLICATED_APP` environment variable to the application that you created:

      ```bash
      export REPLICATED_APP=APP_SLUG
      ```
      Where `APP_SLUG` is the unique application slug provided in the output of the `app create` command. For example, `export REPLICATED_APP=gitea-kite`.

      This allows you to interact with the application using the Replicated CLI without needing to use the `--app` flag with every command.

1. Get the sample Bitnami Gitea Helm chart and add the Replicated SDK as a dependency:

   1. Run the following command to pull and untar version 1.0.6 of the Bitnami Gitea Helm chart:

      ```
      helm pull --untar oci://registry-1.docker.io/bitnamicharts/gitea --version 1.0.6
      ```
      For more information about this chart, see the [bitnami/gitea](https://github.com/bitnami/charts/tree/main/bitnami/gitea) repository in GitHub.

   1. Change to the new `gitea` directory that was created:
      
      ```bash
      cd gitea
      ```

   1. In the Helm chart `Chart.yaml`, add the Replicated SDK as a dependency:

      <DependencyYaml/>

      The Replicated SDK is a Helm chart that provides access to Replicated features and can be installed as a small service alongside your application. For more information, see [About the Replicated SDK](/vendor/replicated-sdk-overview).

   1. Update dependencies and package the Helm chart to a `.tgz` chart archive:

      ```bash
      helm package -u .
      ```
      Where `-u` or `--dependency-update` is an option for the helm package command that updates chart dependencies before packaging. For more information, see [Helm Package](https://helm.sh/docs/helm/helm_package/) in the Helm documentation.

1. Add the chart archive to a release:

   1. In the `gitea` directory, create a subdirectory named `manifests`:

      ```
      mkdir manifests
      ```

      You will add the files required to support installation with Replicated KOTS and Replicated Embedded Cluster to this subdirectory.

   1. Move the Helm chart archive that you created to `manifests`:

      ```
      mv gitea-1.0.6.tgz manifests
      ```

   1. In `manifests`, create the following YAML files:
      ```
      cd manifests
      ```
      ```
      touch gitea.yaml kots-app.yaml k8s-app.yaml embedded-cluster.yaml
      ```

   1. In each of the files that you created, paste the corresponding YAML provided in the tabs below:

      <Tabs>
      <TabItem value="helmchart" label="gitea.yaml" default>
      <h5>Description</h5>
      <p>The KOTS HelmChart custom resource provides instructions to KOTS about how to deploy the Helm chart. The <code>name</code> and <code>chartVersion</code> listed in the HelmChart custom resource must match the name and version of a Helm chart archive in the release. The <a href="/vendor/helm-optional-value-keys#conditionally-set-values"><code>optionalValues</code></a> field sets the specified Helm values when a given conditional statement evaluates to true. In this case, if the application is installed with Embedded Cluster, then the Gitea service type is set to `NodePort` and the node port is set to `"32000"`. This will allow Gitea to be accessed from the local machine after deployment for the purpose of this quick start.</p>
      <h5>YAML</h5>
      <HelmChartCr/>
      </TabItem>
      <TabItem value="kots-app" label="kots-app.yaml">
      <h5>Description</h5>
      <p>The KOTS Application custom resource enables features in the Replicated Admin Console such as branding, release notes, application status indicators, and custom graphs.</p><p>The YAML below provides a name for the application to display in the Admin Console, adds a custom <em>status informer</em> that displays the status of the <code>gitea</code> Deployment resource in the Admin Console dashboard, adds a custom application icon, and adds the port where the Gitea service can be accessed so that the user can open the application after installation.</p>
      <h5>YAML</h5>
      <KotsCr/>
      </TabItem>
      <TabItem value="k8s-app" label="k8s-app.yaml">
      <h5>Description</h5>
      <p>The Kubernetes SIG Application custom resource supports functionality such as including buttons and links on the Replicated Admin Console dashboard. The YAML below adds an <strong>Open App</strong> button to the Admin Console dashboard that opens the application using the service port defined in the KOTS Application custom resource.</p>
      <h5>YAML</h5>
      <K8sCr/>
      </TabItem>
      <TabItem value="ec" label="embedded-cluster.yaml">
      <h5>Description</h5>
      <p>To install your application with Embedded Cluster, an Embedded Cluster Config must be present in the release. At minimum, the Embedded Cluster Config sets the version of Embedded Cluster that will be installed. You can also define several characteristics about the cluster.</p>
      <h5>YAML</h5>
      <EcCr/>
      </TabItem>
      </Tabs>

   1. Lint the YAML files:

      ```bash
      replicated release lint --yaml-dir .
      ```
      **Example output:**
      ```bash
      RULE                                  TYPE    FILENAME         LINE    MESSAGE
      config-spec                           warn                                                                                                                                                    Missing config spec
      preflight-spec                        warn                                                                                                                                                    Missing preflight spec
      troubleshoot-spec                     warn                                                                                                                                                    Missing troubleshoot spec
      nonexistent-status-informer-object    warn    kots-app.yaml    8       Status informer points to a nonexistent kubernetes object. If this is a Helm resource, this warning can be ignored.
      ```
      :::note
      You can ignore any warning messages for the purpose of this quick start.
      :::

   1. Create the release and promote it to the Unstable channel:

      ```bash
      replicated release create --yaml-dir . --promote Unstable
      ```
      **Example output**:
      ```bash
        • Reading manifests from . ✓
        • Creating Release ✓
          • SEQUENCE: 1
        • Promoting ✓
          • Channel 2kvjwEj4uBaCMoTigW5xty1iiw6 successfully set to release 1
      ```

1. Create a customer so that you can install the release on your VM with Embedded Cluster:

   1. In the [Vendor Portal](https://vendor.replicated.com), under the application drop down, select the Gitea application that you created.

      <img alt="App drop down" src="/images/quick-start-select-gitea-app.png" width="250px"/>

      [View a larger version of this image](/images/quick-start-select-gitea-app.png)
   
   1. Click **Customers > Create customer**.

      The **Create a new customer** page opens:

      ![Customer a new customer page in the Vendor Portal](/images/create-customer.png)

      [View a larger version of this image](/images/create-customer.png)

   1. For **Customer name**, enter a name for the customer. For example, `Example Customer`.

   1. For **Channel**, select **Unstable**. This allows the customer to install releases promoted to the Unstable channel.

   1. For **License type**, select **Development**.

   1. For **License options**, enable the following entitlements:
      * **KOTS Install Enabled**
      * **Embedded Cluster Enabled**

   1. Click **Save Changes**.

1. Install the application with Embedded Cluster:
     
    1. On the page for the customer that you created, click **Install instructions > Embedded Cluster**.

       ![Customer install instructions dropdown](/images/customer-install-instructions-dropdown.png)

       [View a larger image](/images/customer-install-instructions-dropdown.png)

    1. On the command line, SSH onto your VM and run the commands in the **Embedded cluster install instructions** dialog to download the latest release, extract the installation assets, and install.

       <img width="500px" src="/images/embedded-cluster-install-dialog-latest.png" alt="embedded cluster install instructions dialog"/>

       [View a larger version of this image](/images/embedded-cluster-install-dialog-latest.png)

   1. When prompted, enter a password for accessing the Admin Console.

      The installation command takes a few minutes to complete.

      **Example output:**

      ```bash
      ? Enter an Admin Console password: ********
      ? Confirm password: ********
      ✔  Host files materialized!
      ✔  Running host preflights
      ✔  Node installation finished!
      ✔  Storage is ready!
      ✔  Embedded Cluster Operator is ready!
      ✔  Admin Console is ready!
      ✔  Additional components are ready!
      Visit the Admin Console to configure and install gitea-kite: http://104.155.145.60:30000
      ```

      At this point, the cluster is provisioned and the Admin Console is deployed, but the application is not yet installed.

   1. Go to the URL provided in the output to access to the Admin Console.
   
   1. On the Admin Console landing page, click **Start**.

   1. On the **Secure the Admin Console** screen, review the instructions and click **Continue**. In your browser, follow the instructions that were provided on the **Secure the Admin Console** screen to bypass the warning.

   1. On the **Certificate type** screen, either select **Self-signed** to continue using the self-signed Admin Console certificate or click **Upload your own** to upload your own private key and certificacte.

       By default, a self-signed TLS certificate is used to secure communication between your browser and the Admin Console. You will see a warning in your browser every time you access the Admin Console unless you upload your own certificate.

   1. On the login page, enter the Admin Console password that you created during installation and click **Log in**.

   1. On the **Configure the cluster** screen, you can view details about the VM where you installed, including its node role, status, CPU, and memory. Users can also optionally add additional nodes on this page before deploying the application. Click **Continue**. 

       The Admin Console dashboard opens.

   1. On the Admin Console dashboard, next to the version, click **Deploy** and then **Yes, Deploy**. 

      The application status changes from Missing to Unavailable while the `gitea` Deployment is being created.

   1. After a few minutes when the application status is Ready, click **Open App** to view the Gitea application in a browser.
   
      For example:

      ![Admin console dashboard showing ready status](/images/gitea-ec-ready.png)

      [View a larger version of this image](/images/gitea-ec-ready.png)

      <img alt="Gitea app landing page" src="/images/gitea-app.png" width="600px"/>

      [View a larger version of this image](/images/gitea-app.png) 

1. Return to the Vendor Portal and go to **Customers**. Under the name of the customer, confirm that you can see an active instance.

   This instance telemetry is automatically collected and sent back to the Vendor Portal by both KOTS and the Replicated SDK. For more information, see [About Instance and Event Data](/vendor/instance-insights-event-data).

1. Under **Instance ID**, click on the ID to view additional insights including the versions of Kubernetes and the Replicated SDK running in the cluster where you installed the application. For more information, see [Instance Details](/vendor/instance-insights-details).    

1. Create a new release that adds preflight checks to the application:

   1. In your local filesystem, go to the `gitea` directory.

   1. Create a `gitea-preflights.yaml` file in the `templates` directory:

      ```
      touch templates/gitea-preflights.yaml
      ```

   1. In the `gitea-preflights.yaml` file, add the following YAML to create a Kubernetes Secret with a simple preflight spec: 

      ```yaml
      apiVersion: v1
      kind: Secret
      metadata:
        labels:
          troubleshoot.sh/kind: preflight
        name: "{{ .Release.Name }}-preflight-config"
      stringData:
        preflight.yaml: |
          apiVersion: troubleshoot.sh/v1beta2
          kind: Preflight
          metadata:
            name: preflight-sample
          spec:
            collectors:
              - http:
                  collectorName: slack
                  get:
                    url: https://api.slack.com/methods/api.test
            analyzers:
              - textAnalyze:
                  checkName: Slack Accessible
                  fileName: slack.json
                  regex: '"status": 200,'
                  outcomes:
                    - pass:
                        when: "true"
                        message: "Can access the Slack API"
                    - fail:
                        when: "false"
                        message: "Cannot access the Slack API. Check that the server can reach the internet and check [status.slack.com](https://status.slack.com)."
      ```
      The YAML above defines a preflight check that confirms that an HTTP request to the Slack API at `https://api.slack.com/methods/api.test` made from the cluster returns a successful response of `"status": 200,`.

   1. In the `Chart.yaml` file, increment the version to 1.0.7:

      ```yaml
      # Chart.yaml
      version: 1.0.7
      ```

   1. Update dependencies and package the chart to a `.tgz` chart archive:

      ```bash
      helm package -u .
      ```

   1. Move the chart archive to the `manifests` directory:

      ```bash
      mv gitea-1.0.7.tgz manifests
      ```

   1. In the `manifests` directory, open the KOTS HelmChart custom resource (`gitea.yaml`) and update the `chartVersion`:

      ```yaml
      # gitea.yaml KOTS HelmChart
      chartVersion: 1.0.7
      ```  

   1. Remove the chart archive for version 1.0.6 of the Gitea chart from the `manifests` directory:

      ```
      rm gitea-1.0.6.tgz
      ```        

   1. From the `manifests` directory, create and promote a new release, setting the version label of the release to `0.0.2`:  

      ```bash
      replicated release create --yaml-dir . --promote Unstable --version 0.0.2
      ```
      **Example output**:
      ```bash
        • Reading manifests from . ✓
        • Creating Release ✓
          • SEQUENCE: 2
        • Promoting ✓
          • Channel 2kvjwEj4uBaCMoTigW5xty1iiw6 successfully set to release 2
      ```

1. On your VM, update the application instance to the new version that you just promoted:

   1. In the Admin Console, go to the **Version history** tab.

      The new version is displayed automatically.

   1. Click **Deploy** next to the new version.

      The Embedded Cluster upgrade wizard opens.

   1. In the Embedded Cluster upgrade wizard, on the **Preflight checks** screen, note that the "Slack Accessible" preflight check that you added was successful. Click **Next: Confirm and deploy**.   

      ![preflight page of the embedded cluster upgrade wizard](/images/quick-start-ec-upgrade-wizard-preflight.png)

      [View a larger version of this image](/images/quick-start-ec-upgrade-wizard-preflight.png)

      :::note
      The **Config** screen in the upgrade wizard is bypassed because this release does not contain a KOTS Config custom resource. The KOTS Config custom resource is used to set up the Config screen in the KOTS Admin Console.
      :::   

   1. On the **Confirm and Deploy** page, click **Deploy**.

1. Reset and reboot the VM to remove the installation:

   ```bash
   sudo ./APP_SLUG reset
   ```
   Where `APP_SLUG` is the unique slug for the application.
   
   :::note
   You can find the application slug by running `replicated app ls` on your local machine.
   :::

## Next Steps

Congratulations! As part of this quick start, you:
* Added the Replicated SDK to a Helm chart
* Created a release with the Helm chart
* Installed the release on a VM with Embedded Cluster
* Viewed telemetry for the installed instance in the Vendor Portal
* Created a new release to add preflight checks to the application
* Updated the application from the Admin Console

Now that you are familiar with the workflow of creating, installing, and updating releases, you can begin onboarding your own application to the Replicated Platform.

To get started, see [Replicated Onboarding](replicated-onboarding).

## Related Topics

For more information about the Replicated Platform features mentioned in this quick start, see:

* [About Distributing Helm Charts with KOTS](/vendor/helm-native-about)
* [About Preflight Checks and Support Bundles](/vendor/preflight-support-bundle-about)
* [About the Replicated SDK](/vendor/replicated-sdk-overview)
* [Introduction to KOTS](/intro-kots)
* [Managing Releases with the CLI](/vendor/releases-creating-cli)
* [Packaging a Helm Chart for a Release](/vendor/helm-install-release)
* [Using Embedded Cluster](/vendor/embedded-overview)

## Related Tutorials

For additional tutorials related to this quick start, see:

* [Deploying a Helm Chart on a VM with Embedded Cluster](/vendor/tutorial-embedded-cluster-setup)
* [Adding Preflight Checks to a Helm Chart](/vendor/tutorial-preflight-helm-setup)
* [Deploying a Helm Chart with KOTS and the Helm CLI](/vendor/tutorial-kots-helm-setup)

---


import CreateRelease from "../partials/getting-started/_create-promote-release.mdx"
import DependencyYaml from "../partials/replicated-sdk/_dependency-yaml.mdx"
import EcCr from "../partials/embedded-cluster/_ec-config.mdx"
import HelmPackage from "../partials/helm/_helm-package.mdx"
import Requirements from "../partials/embedded-cluster/_requirements.mdx"
import SDKOverview from "../partials/replicated-sdk/_overview.mdx"
import TestYourChanges from "../partials/getting-started/_test-your-changes.mdx"
import UnauthorizedError from "../partials/replicated-sdk/_401-unauthorized.mdx"

# Replicated Onboarding 

This topic describes how to onboard applications to the Replicated Platform.

## Before You Begin

This section includes guidance and prerequisites to review before you begin onboarding your application.  

### Best Practices and Recommendations

The following are some best practices and recommendations for successfully onboarding with Replicated:

* When integrating new Replicated features with an application, make changes in small iterations and test frequently by installing or upgrading the application in a development environment. This will help you to more easily identify issues and troubleshoot. This onboarding workflow will guide you through the process of integrating features in small iterations.

* Use the Replicated CLI to create and manage your application and releases. Getting familiar with the Replicated CLI will also help later on when integrating Replicated workflows into your CI/CD pipelines. For more information, see [Installing the Replicated CLI](/reference/replicated-cli-installing).

* These onboarding tasks assume that you will test the installation of each release on a VM with the Replicated Embedded Cluster installer _and_ in a cluster with the Replicated KOTS installer. If you do not intend to offer existing cluster installations with KOTS (for example, if you intend to support only Embedded Cluster and Helm installations for your users), then can choose to test with Embedded Cluster only.

* Ask for help from the Replicated community. For more information, see [Getting Help from the Community](#community) below.

### Getting Help from the Community {#community}

The [Replicated community site](https://community.replicated.com/) is a forum where Replicated team members and users can post questions and answers related to working with the Replicated Platform. It is designed to help Replicated users troubleshoot and learn more about common tasks involved with distributing, installing, observing, and supporting their application. 

Before posting in the community site, use the search to find existing knowledge base articles related to your question. If you are not able to find an existing article that addresses your question, create a new topic or add a reply to an existing topic so that a member of the Replicated community or team can respond.

To search and participate in the Replicated community, see https://community.replicated.com/.

### Prerequisites

* Create an account in the Vendor Portal. You can either create a new team or join an existing team. For more information, see [Creating a Vendor Account](vendor-portal-creating-account).

* Install the Replicated CLI. See [Installing the Replicated CLI](/reference/replicated-cli-installing).

* Complete a basic quick start workflow to create an application with a sample Helm chart and then promote and install releases in a development environment. This helps you get familiar with the process of creating, installing, and updating releases in the Replicated Platform. See [Replicated Quick Start](/vendor/quick-start).

* Ensure that you have access to a VM that meets the requirements for the Replicated Embedded Cluster installer. You will use this VM to test installation with Embedded Cluster.

  Embedded Cluster has the following requirements:

  <Requirements/>

* (Optional) Ensure that you have kubectl access to a Kubernetes cluster. You will use this cluster to test installation with KOTS. If you do not intend to offer existing cluster installations with KOTS (for example, if you intend to support only Embedded Cluster and Helm installations for your users), then you do not need access to a cluster for the main onboarding tasks.

  You can use any cloud provider or tool that you prefer to create a cluster, such as [Replicated Compatibility Matrix](/vendor/testing-how-to), Google Kubernetes Engine (GKE), or minikube. 

## Onboard

Complete the tasks in this section to onboard your application. When you are done, you can continue to [Next Steps](#next-steps) to integrate other Replicated features with your application.

### Task 1: Create An Application

To get started with onboarding, first create a new application. This will be the official Vendor Portal application used by your team to create and promote both internal and customer-facing releases.

To create an application:

1. Create a new application using the Replicated CLI or the Vendor Portal. Use an official name for your application. See [Create an Application](/vendor/vendor-portal-manage-app#create-an-application).
   
   <details>
   <summary>Can I change the application name in the future?</summary>

   You can change the application name, but you cannot change the application _slug_.

   The Vendor Portal automatically generates and assigns a unique slug for each application based on the application's name. For example, the slug for "Example App" would be `example-app`.
   
   Application slugs are unique across all of Replicated. This means that, if necessary, the Vendor Portal will append a random word to the end of slug to ensure uniqueness. For example, `example-app-flowers`.
   </details>

1. Set the `REPLICATED_APP` environment variable to the unique slug of the application that you created. This will allow you to interact with the application from the Replicated CLI throughout onboarding. See [Set Environment Variables](/reference/replicated-cli-installing#replicated_app) in _Installing the Replicated CLI_.

   For example:

   ```bash
   export REPLICATED_APP=my-app
   ```

### Task 2: Connect Your Image Registry

Add credentials for your image registry to the Vendor Portal. This will allow you to use the Replicated proxy registry in a later step so that you can grant proxy access to application images without exposing registry credentials to your customers.

For more information, see [Connecting to an External Registry](/vendor/packaging-private-images).

### Task 3: Add the Replicated SDK and Package your Chart

Next, add the Replicated SDK as a dependency of your Helm chart and package the chart as a `.tgz` archive.

The Replicated SDK is a Helm chart that can be installed as a small service alongside your application. The SDK provides access to key Replicated functionality, including an in-cluster API and automatic access to insights and operational telemetry for instances running in customer environments. For more information, see [About the Replicated SDK](/vendor/replicated-sdk-overview).

To package your Helm chart with the Replicated SDK:

1. Go to the local directory where your Helm chart is.

1. In your application Helm chart `Chart.yaml` file, add the YAML below to declare the SDK as a dependency.
   
   If your application is installed as multiple charts, declare the SDK as a dependency of the chart that customers install first. Do not declare the SDK in more than one chart. For more information, see [Packaging a Helm Chart for a Release](helm-install-release).

   <DependencyYaml/>   

1. Update dependencies and package the chart as a `.tgz` file:

    <HelmPackage/>

    <UnauthorizedError/>

1. If your application is deployed as multiple Helm charts, package each chart as a separate `.tgz` archive using the `helm package -u PATH_TO_CHART` command. Do not declare the SDK in more than one chart.

### Task 4: Create the Initial Release with KOTS HelmChart and Embedded Cluster Config {#first-release}

After packaging your Helm chart, you can create a release. The initial release for your application will include the minimum files required to install a Helm chart with the Embedded Cluster installer:
* The Helm chart `.tgz` archive
* [KOTS HelmChart custom resource](/reference/custom-resource-helmchart-v2)
* [Embedded Cluster Config](/reference/embedded-config)

If you have multiple charts, you will add each chart archive to the release, plus a corresponding KOTS HelmChart custom resource for each archive.

:::note
Configuring the KOTS HelmChart custom resource includes several tasks, and involves the use of KOTS template functions. Depending on how many Helm charts your application uses, Replicated recommends that you allow about two to three hours for configuring the HelmChart custom resource and creating and testing your initial release.
:::

To create the first release for your application:

1. In the local directory for your Helm chart, create a subdirectory named `manifests` where you will add the files for the release.

1. In the `manifests` directory:

   1. Move the `.tgz` chart archive that you packaged. If your application is deployed as multiple Helm charts, move each `.tgz` archive to `manifests`.

   1. Create an `embedded-cluster.yaml` file with the following default Embedded Cluster Config:

      <EcCr/>

      <details>
      <summary>What is the Embedded Cluster Config?</summary>
      
      The Embedded Cluster Config is required to install with Embedded Cluster.
      </details>
    
      For more information, see [Using Embedded Cluster](/vendor/embedded-overview).

   1. Create a new YAML file. In this file, configure the KOTS HelmChart custom resource by completing the workflow in [Configuring the HelmChart Custom Resource](helm-native-v2-using).
   
      <details>
      <summary>What is the KOTS HelmChart custom resource?</summary>
      
      The KOTS HelmChart custom resource is required to install Helm charts with KOTS and Embedded Cluster. As part of configuring the KOTS HelmChart custom resource, you will rewrite image names and add image pull secrets to allow your application images to be accessed through the Replicated proxy registry.
      </details>

   1. If your application is deployed as multiple Helm charts, repeat the step above to add a separate HelmChart custom resource for each Helm chart archive in the release.

   1. If there are values in any of your Helm charts that need to be set for the installation to succeed, you can set those values using the `values` key in the corresponding HelmChart custom resource. See [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys).
   
      This is a temporary measure to ensure the values get passed to the Helm chart during installation until you configure the Admin Console Config screen in a later onboarding task. If your default Helm values are sufficient for installation, you can skip this step.  

   1. If your application requires that certain components are deployed before the application and as part of the Embedded Cluster itself, then update the Embedded Cluster Config to add [extensions](/reference/embedded-config#extensions). Extensions allow you to provide Helm charts that are deployed before your application. For example, one situation where this is useful is if you want to ship an ingress controller because Embedded Cluster does not include one.

      For more information, see [extensions](/reference/embedded-config#extensions) in _Embedded Cluster Config_.   

1. From the `manifests` directory, create a release and promote it to the Unstable channel. For more information, see [Managing Releases with the Vendor Portal](releases-creating-releases) or [Managing Releases with the CLI](releases-creating-cli).

    ```bash
    replicated release create --yaml-dir . --promote Unstable
    ```

1. Install the release in your development environment to test:

   1. Install with Embedded Cluster on a VM. See [Online Installation with Embedded Cluster](/enterprise/installing-embedded).
   
   1. (Optional) Install in an existing cluster with KOTS. See [Online Installation in Existing Clusters with KOTS](/enterprise/installing-existing-cluster).

After successfully installing the initial release on a VM with Embedded Cluster (and optionally in an existing cluster with KOTS), go to the next task. You will continue to iterate throughout the rest of the onboarding process by creating and promoting new releases, then upgrading to the new version in your development environment.

### Task 5: Customize the KOTS Admin Console {#admin-console}

Configure the KOTS Application custom resource to add an application name, icon, and status informers. The name and icon will be displayed in the Admin Console and the Replicated Download Portal. The status informers will be used to display the application status on the Admin Console dashboard.

To configure the KOTS Application custom resource:

1. In your `manifests` directory, create a new `kots-app.yaml` file.

1. In the `kots-app.yaml` file, add the [KOTS Application](/reference/custom-resource-application) custom resource YAML and set the `title`, `icon`, and `statusInformers` fields.

   **Example:**

    ```yaml
    apiVersion: kots.io/v1beta1
    kind: Application
    metadata:
      name: gitea
    spec:
      title: Gitea
      # Base64 encoded image string
      icon: fyJINrigNkt5VsRiub9nXICdsYyVd2NcVvA3ScE5t2rb5JuEeyZnAhmLt9NK63vX1O
      statusInformers:
        - deployment/gitea
    ```
    For more information, see:
    * [Customizing the Application Icon](/vendor/admin-console-customize-app-icon)
    * [Enabling and Understanding Application Status](/vendor/insights-app-status)
    * [Application](/reference/custom-resource-application)
    <br/>
    <details>
    <summary>Can I preview the icon before installing the release?</summary>

     Yes. The Vendor Portal includes a **Application icon preview** in the **Help** pane on the **Edit release** page.

     ![Icon preview](/images/icon-preview.png)

     [View a larger version of this image](/images/icon-preview.png)

    </details>  

1. <CreateRelease/>

1. <TestYourChanges/>

### Task 6: Set Up the Admin Console Config Screen and Map to Helm Values

The KOTS Admin Console Config screen is used to collect required and optional application configuration values from your users. User-supplied values provided on the Config screen can be mapped to your Helm values.

Before you begin this task, you can complete the [Set Helm Values with KOTS](/vendor/tutorial-config-setup) tutorial to learn how to map user-supplied values from the Admin Console Config screen to a Helm chart.

:::note
Setting up the Admin Console config screen can include the use of various types of input fields, conditional statements, and KOTS template functions. Depending on your application's configuration options, Replicated recommends that you allow about two to three hours for configuring the Config custom resource and testing the Admin Console config screen.
:::

To set up the Admin Console Config screen for your application:

1. In your `manifests` directory, create a new file named `kots-config.yaml`.

1. In `kots-config.yaml`, add the KOTS Config custom resource. Configure the KOTS Config custom resource based on the values that you need to collect from users.

    **Example:**

    ```yaml
    apiVersion: kots.io/v1beta1
    kind: Config
    metadata:
      name: my-application
    spec:
      groups:
        - name: example_group
          title: Example Group
          items:
            - name: example_item
              title: Example Item
              type: text
              default: "Hello World"
    ```

    For more information, see:
    * [Creating and Editing Configuration Fields](/vendor/admin-console-customize-config-screen) 
    * [Using Conditional Statements in Configuration Fields](/vendor/config-screen-conditional)  
    * [Config](/reference/custom-resource-config)

    <br/>

    <details>
    <summary>Can I preview the Admin Console config screen before installing the release?</summary>

     Yes. The Vendor Portal includes a **Config preview** in the **Help** pane on the **Edit release** page.

     For example:

     ![Config preview](/images/config-preview.png)

     [View a larger version of this image](/images/config-preview.png)
    </details>  

1. <CreateRelease/>

1. <TestYourChanges/>

1. In `manifests`, open the KOTS HelmChart custom resource that you configured in a previous step. Configure the `values` key of the HelmChart custom resource to map the fields in the KOTS Config custom resource to your Helm values.

   For more information, see:
   * [Mapping User-Supplied Values](/vendor/config-screen-map-inputs)
   * [Tutorial: Set Helm Chart Values with KOTS](/vendor/tutorial-config-setup)
   * [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys)
   * [`values`](/reference/custom-resource-helmchart-v2#values) in _HelmChart v2_

1. <CreateRelease/>

1. <TestYourChanges/>

1. Continue to create and test new releases with new config fields until you are ready to move on to the next task.

### Task 7: Define Preflight Checks

In the next two tasks, you will add specs for _preflight checks_ and _support bundles_.

Preflight checks and support bundles are provided by the Troubleshoot open source project, which is maintained by Replicated. Troubleshoot is a kubectl plugin that provides diagnostic tools for Kubernetes applications. For more information, see the open source [Troubleshoot](https://troubleshoot.sh/docs/) documentation.

Preflight checks and support bundles analyze data from customer environments to provide insights that help users to avoid or troubleshoot common issues with an application:
* **Preflight checks** run before an application is installed to check that the customer environment meets the application requirements.
* **Support bundles** collect troubleshooting data from customer environments to help users diagnose problems with application deployments.

:::note
Before you begin this task, you can complete the [Add Preflight Checks to a Helm Chart](/vendor/tutorial-preflight-helm-setup) tutorial to learn how to add a preflight spec to a Helm chart in a Kubernetes secret and run the preflight checks before installation.
:::

To define preflight checks for your application:

1. In your Helm chart `templates` directory, add a Kubernetes Secret that includes a preflight spec. For more information, see [Defining Preflight Checks](/vendor/preflight-defining). For examples, see [Example Preflight Specs](/vendor/preflight-examples).
     :::note
     If your application is deployed as multiple Helm charts, add the Secret to the `templates` directory for the chart that is installed first.
     :::

1. Update dependencies and package the chart as a `.tgz` file:

   <HelmPackage/>

1. Move the `.tgz` file to the `manifests` directory.

1. <CreateRelease/>

1. <TestYourChanges/>

    Preflight checks run automatically during installation.

1. Continue to create and test new releases with additional preflight checks until you are ready to move on to the next task.

### Task 8: Add a Support Bundle Spec

To add the default support bundle spec to your application:

1. In your Helm chart `templates` directory, add the following YAML to a Kubernetes Secret to enable the default support bundle spec for your application:

    ```yaml
    apiVersion: v1
    kind: Secret
    metadata:
      labels:
        troubleshoot.sh/kind: support-bundle
      name: example
    stringData:
      support-bundle-spec: |
        apiVersion: troubleshoot.sh/v1beta2
        kind: SupportBundle
        metadata:
          name: support-bundle
        spec:
          collectors: []
          analyzers: []
    ```
    :::note
    If your application is installed as multiple Helm charts, you can optionally create separate support bundle specs in each chart. The specs are automatically merged when a support bundle is generated. Alternatively, continue with a single support bundle spec and then optionally revisit how you organize your support bundle specs after you finish onboarding.
    :::
   
1. (Recommended) At a minimum, Replicated recommends that all support bundle specs include the `logs` collector. This collects logs from running Pods in the cluster.

   **Example:**

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: example
     labels:
       troubleshoot.sh/kind: support-bundle
   stringData: 
     support-bundle-spec: |-
       apiVersion: troubleshoot.sh/v1beta2
       kind: SupportBundle
       metadata:
         name: example
       spec:
         collectors:
           - logs:
               selector:
                 - app.kubernetes.io/name=myapp
               namespace: {{ .Release.Namespace }}
               limits:
                 maxAge: 720h
                 maxLines: 10000
   ```

   For more information, see:
   * [Adding and Customizing Support Bundles](/vendor/support-bundle-customizing)
   * [Example Support Bundle Specs](/vendor/support-bundle-examples)
   * [Pod Logs](https://troubleshoot.sh/docs/collect/logs/) in the Troubleshoot documentation.

1. (Recommended) Ensure that any preflight checks that you added are also include in your support bundle spec. This ensures that support bundles collect at least the same information collected when running preflight checks. 

1. Update dependencies and package the chart as a `.tgz` file:

   <HelmPackage/>

1. Move the `.tgz` file to the `manifests` directory.

1. <CreateRelease/>

1. <TestYourChanges/>

   For information about how to generate support bundles, see [Generating Support Bundles](/vendor/support-bundle-generating).

1. (Optional) Customize the support bundle spec by adding additional collectors and analyzers.

### Task 9: Alias Replicated Endpoints with Your Own Domains

Your customers are exposed to several Replicated domains by default. Replicated recommends you use custom domains to unify the customer's experience with your brand and simplify security reviews.

For more information, see [Using Custom Domains](/vendor/custom-domains-using).

## Next Steps

After completing the main onboarding tasks, Replicated recommends that you also complete the following additional tasks to integrate other Replicated features with your application. You can complete these next recommended tasks in any order and at your own pace.

### Add Support for Helm Installations

Existing KOTS releases that include one or more Helm charts can be installed with the Helm CLI; it is not necessary to create and manage separate releases or channels for each installation method.

To enable Helm installations for Helm charts distributed with Replicated, the only extra step is to add a Secret to your chart to authenticate with the Replicated proxy registry.  

This is the same secret that is passed to KOTS in the HelmChart custom resource using `'{{repl ImagePullSecretName }}'`, which you did as part of [Task 4: Create and Install the Initial Release](#first-release). So, whereas this Secret is created automatically for KOTS and Embedded Cluster installations, you need to create it and add it to your Helm chart for Helm installations.

:::note
Before you test Helm installations for your application, you can complete the [Deploy a Helm Chart with KOTS and the Helm CLI](tutorial-kots-helm-setup) tutorial to learn how to install a single release with both KOTS and Helm.
:::

To support and test Helm installations:

1. Follow the steps in [Using the Proxy Registry with Helm Installations](/vendor/helm-image-registry) to authenticate with the Replicated proxy registry by creating a Secret with `type: kubernetes.io/dockerconfigjson` in your Helm chart.

1. Update dependencies and package the chart as a `.tgz` file:

    <HelmPackage/>

1. Add the `.tgz` file to a release. For more information, see [Managing Releases with the Vendor Portal](releases-creating-releases) or [Managing Releases with the CLI](releases-creating-cli).

1. Install the release in a cluster with the Helm CLI to test your changes. For more information, see [Installing with Helm](/vendor/install-with-helm).

### Add Support for Air Gap Installations

Replicated Embedded Cluster and KOTS support installations in _air gap_ environments with no outbound internet access. Users can install with Embedded Cluster and KOTS in air gap environments by providing air gap bundles that contain the required images for the installers and for your application.

:::note
Replicated also offers Alpha support for air gap installations with Helm. If you are interested in trying Helm air gap installations and providing feedback, please reach out to your account rep to enable this feature.
:::

To add support for air gap installations:

1. If there are any images for your application that are not listed in your Helm chart, list these images in the `additionalImages` attribute of the KOTS Application custom resource. This ensures that the images are included in the air gap bundle for the release. One common use case for this is applications that use Kubernetes Operators. See [Define Additional Images](/vendor/operator-defining-additional-images).

1. In the KOTS HelmChart custom resource `builder` key, pass any values that are required in order for `helm template` to yield all the images needed to successfully install your application. See [Packaging Air Gap Bundles for Helm Charts](/vendor/helm-packaging-airgap-bundles).

    :::note
    If the default values in your Helm chart already enable all the images needed to successfully deploy, then you do not need to configure the `builder` key.
    ::: 

   <details>
   <summary>How do I know if I need to configure the `builder` key?</summary>
   
   When building an air gap bundle, the Vendor Portal templates the Helm charts in a release with `helm template` in order to detect the images that need to be included in the bundle. Images yielded by `helm template` are included in the bundle for the release.

   For many applications, running `helm template` with the default values would not yield all the images required to install. In these cases, vendors can pass the additional values in the `builder` key to ensure that the air gap bundle includes all the necessary images.
   </details>

1. If you have not done so already as part of [Task 4: Create and Install the Initial Release](#first-release), ensure that the `values` key in the KOTS HelmChart custom resource correctly rewrites image names for air gap installations. This is done using the KOTS HasLocalRegistry, LocalRegistryHost, and LocalRegistryNamespace template functions to render the location of the given image in the user's own local registry.

   For more information, see [Rewrite Image Names](/vendor/helm-native-v2-using#rewrite-image-names) in _Configuring the HelmChart Custom Resource v2_.

1. Create and promote a new release with your changes. For more information, see [Managing Releases with the Vendor Portal](releases-creating-releases) or [Managing Releases with the CLI](releases-creating-cli).

1. In the [Vendor Portal](https://vendor.replicated.com), go the channel where the release was promoted to build the air gap bundle. Do one of the following:
     * If the **Automatically create airgap builds for newly promoted releases in this channel** setting is enabled on the channel, watch for the build status to complete.
     * If automatic air gap builds are not enabled, go to the **Release history** page for the channel and build the air gap bundle manually.

1. Create a customer with the **Airgap Download Enabled** entitlement enabled so that you can test air gap installations. See [Creating and Managing Customers](/vendor/releases-creating-customer).

1. Download the Embedded Cluster air gap installation assets, then install with Embedded Cluster on an air gap VM to test. See [Installing in Air Gap Environments with Embedded Cluster](/enterprise/installing-embedded-air-gap).

1. (Optional) Download the `.airgap` bundle for the release and the air gap bundle for the KOTS Admin Console. You can also download both bundles from the Download Portal for the target customer. Then, install in an air gap existing cluster to test. See [Air Gap Installation in Existing Clusters with KOTS](/enterprise/installing-existing-cluster-airgapped).

1. (Optional) Follow the steps in [Installing and Updating with Helm in Air Gap Environments (Alpha)](/vendor/helm-install-airgap) to test air gap installation with Helm. 

    :::note
    Air gap Helm installations are an Alpha feature. If you are interested in trying Helm air gap installations and providing feedback, please reach out to your account rep to enable this feature.
    :::

### Add Roles for Multi-Node Clusters in Embedded Cluster Installations

The Embedded Cluster Config supports roles for multi-node clusters. One or more roles can be selected and assigned to a node when it is joined to the cluster. Node roles can be used to determine which nodes run the Kubernetes control plane, and to assign application workloads to particular nodes.

For more information, see [roles](/reference/embedded-config#roles) in _Embedded Cluster Config_.

### Add and Map License Entitlements

You can add custom license entitlements for your application in the Vendor Portal. Custom license fields are useful when there is entitlement information that applies to a subset of customers. For example, you can use entitlements to:
* Limit the number of active users permitted
* Limit the number of nodes a customer is permitted on their cluster
* Identify a customer on a "Premium" plan that has access to additional features or functionality not available with your base plan

For more information about how to create and assign custom entitlements in the Vendor Portal, see [Managing Customer License Fields](/vendor/licenses-adding-custom-fields) and [Creating and Managing Customers](/vendor/releases-creating-customer).

#### Map Entitlements to Helm Values

You can map license entitlements to your Helm values using KOTS template functions. This can be useful when you need to set certain values based on the user's license information. For more information, see [Using KOTS Template Functions](/vendor/helm-optional-value-keys#using-kots-template-functions) in _Setting Helm Values with KOTS_.

#### Query Entitlements Before Installation and at Runtime

You can add logic to your application to query license entitlements both before deployment and at runtime. For example, you might want to add preflight checks that verify a user's entitlements before installing. Or, you can expose additional product functionality dynamically at runtime based on a customer's entitlements.

For more information, see:
* [Querying Entitlements with the Replicated SDK API](/vendor/licenses-reference-sdk)
* [Checking Entitlements in Preflights with KOTS Template Functions](/vendor/licenses-referencing-fields)

### Add Application Links to the Admin Console Dashboard

You can add the Kubernetes SIG Application custom resource to your release to add a link to your application from the Admin Console dashboard. This makes it easier for users to access your application after installation.

You can also configure the Kubernetes SIG Application resource add links to other resources like documentation or dashboards.

For more information, see [Adding Application Links to the Dashboard](/vendor/admin-console-adding-buttons-links).

### Update the Preflight and Support Bundles Specs

After adding basic specs for preflights and support bundles, you can continue to add more collectors and analyzers as needed.

Consider the following recommendations and best practices:

* Revisit your preflight and support bundle specs when new support issues arise that are not covered by your existing specs.

* Your support bundles should include all of the same collectors and analyzers that are in your preflight checks. This ensures that support bundles include all the necessary troubleshooting information, including any failures in preflight checks.

* Your support bundles will most likely need to include other collectors and analyzers that are not in your preflight checks. This is because some of the information used for troubleshooting (such as logs) is not necessary when running preflight checks before installation.

* If your application is installed as multiple Helm charts, you can optionally add separate support bundle specs in each chart. This can make it easier to keep the specs up-to-date and to avoid merge conflicts that can be caused when multiple team members contribute to a single, large support bundle spec. When an application has multiple support bundle specs, the specs are automatically merged when generating a support bundle so that only a single support bundle is provided to the user.

The documentation for the open-source Troubleshoot project includes the full list of available collectors and analyzers that you can use. See [All Collectors](https://troubleshoot.sh/docs/collect/all/) and the [Analyze](https://troubleshoot.sh/docs/analyze/) section in the Troubleshoot documentation.

You can also view common examples of collectors and analyzers used in preflight checks and support bundles in [Preflight Spec Examples](preflight-examples) and [Support Bundle Spec Examples](support-bundle-examples).

### Configure Backup and Restore

Enable backup and restore with Velero for your application so that users can back up and restore their KOTS Admin Console and application data. 

There are different steps to configure backup and restore for Embedded Cluster and for existing cluster installations with KOTS:
* To configure the disaster recovery feature for Embedded Cluster, see [Disaster Recovery for Embedded Cluster](/vendor/embedded-disaster-recovery)
* To configure the snapshots feature for existing cluster KOTS installations, see [Configuring Snapshots](snapshots-configuring-backups).

### Add Custom Metrics

In addition to the built-in insights displayed in the Vendor Portal by default (such as uptime and time to install), you can also configure custom metrics to measure instances of your application running in customer environments. Custom metrics can be collected for application instances running in online or air gap environments using the Replicated SDK.

For more information, see [Configuring Custom Metrics](/vendor/custom-metrics).

### Integrate with CI/CD

Replicated recommends that teams integrate the Replicated Platform into their existing develeopment and production CI/CD workflows. This can be useful for automating the processes of creating new releases, promoting releases, and testing releases with the Replicated Compatibility Matrix.

For more information, see:
* [About Integrating with CI/CD](/vendor/ci-overview)
* [About Compatibility Matrix](/vendor/testing-about)
* [Recommended CI/CD Workflows](/vendor/ci-workflows)

### Customize Release Channels

By default, the Vendor Portal includes Unstable, Beta, and Stable channels. You can customize the channels in the Vendor Portal based on your application needs.

Consider the following recommendations:
* Use the Stable channel for your primary release cadence. Releases should be promoted to the Stable channel only as frequently as your average customer can consume new releases. Typically, this is no more than monthly. However, this cadence varies depending on the customer base.
* If you have a SaaS product, you might want to create an "Edge" channel where you promote the latest SaaS releases.
* You can consider a “Long Term Support” channel where you promote new releases less frequently and support those releases for longer.
* It can be useful to create channels for each feature branch so that internal teams reviewing a PR can easily get the installation artifacts as well as review the code. You can automate channel creation as part of a pipeline or Makefile.

For more information, see:
* [About Channels and Releases](/vendor/releases-about)
* [Creating and Editing Channels](/vendor/releases-creating-channels)

### Write Your Documentation

Before distributing your application to customers, ensure that your documentation is up-to-date. In particular, be sure to update the installation documentation to include the procedures and requirements for installing with Embedded Cluster, Helm, and any other installation methods that you support. 

For guidance on how to get started with documentation for applications distributed with Replicated, including key considerations, examples, and templates, see [Writing Great Documentation for On-Prem Software Distributed with Replicated](https://www.replicated.com/blog/writing-great-documentation-for-on-prem-software-distributed-with-replicated) in the Replicated blog.

---


# Installing the KOTS CLI

Users can interact with the Replicated KOTS CLI to install and manage applications with Replicated KOTS. The KOTS CLI is a kubectl plugin that runs locally on any computer.


## Prerequisite

Install kubectl, the Kubernetes command-line tool. See [Install Tools](https://kubernetes.io/docs/tasks/tools/) in the Kubernetes documentation.

:::note
If you are using a cluster created with Replicated kURL, kURL already installed both kubectl and the KOTS CLI when provisioning the cluster. For more information, see [Online Installation with kURL](/enterprise/installing-kurl) and [Air Gap Installation with kURL](/enterprise/installing-kurl-airgap).
:::

## Install

To install the latest version of the KOTS CLI to `/usr/local/bin`, run:

```bash
curl https://kots.io/install | bash
```

To install to a directory other than `/usr/local/bin`, run:

```bash
curl https://kots.io/install | REPL_INSTALL_PATH=/path/to/cli bash
```

To install a specific version of the KOTS CLI, run:

```bash
curl https://kots.io/install/<version> | bash
```

To verify your installation, run:

```bash
kubectl kots --help
```

## Install without Root Access

You can install the KOTS CLI on computers without root access or computers that cannot write to the `/usr/local/bin` directory.

To install the KOTS CLI without root access, you can do any of the following:

* (Online Only) [Install to a Different Directory](#install-to-a-different-directory)
* (Online Only) [Install Using Sudo](#install-using-sudo)
* (Online or Air Gap) [Manually Download and Install](#manually-download-and-install)

### Install to a Different Directory

You can set the `REPL_INSTALL_PATH` environment variable to install the KOTS CLI to a directory other than `/usr/local/bin` that does not require elevated permissions.

**Example:**

In the following example, the installation script installs the KOTS CLI to `~/bin` in the local directory. You can use the user home symbol `~` in the `REPL_INSTALL_PATH` environment variable. The script expands `~` to `$HOME`.

```bash
curl -L https://kots.io/install | REPL_INSTALL_PATH=~/bin bash
```

### Install Using Sudo

If you have sudo access to the directory where you want to install the KOTS CLI, you can set the `REPL_USE_SUDO` environment variable so that the installation script prompts you for your sudo password.

When you set the `REPL_USE_SUDO` environment variable to any value, the installation script uses sudo to create and write to the installation directory as needed. The script prompts for a sudo password if it is required for the user executing the script in the specified directory.

**Example:**

In the following example, the script uses sudo to install the KOTS CLI to the default `/usr/local/bin` directory.

```bash
curl -L https://kots.io/install | REPL_USE_SUDO=y bash
```

**Example:**

In the following example, the script uses sudo to install the KOTS CLI to the `/replicated/bin` directory.

```bash
curl -L https://kots.io/install | REPL_INSTALL_PATH=/replicated/bin REPL_USE_SUDO=y bash
```

### Manually Download and Install

You can manually download and install the KOTS CLI binary to install without root access, rather than using the installation script.

Users in air gap environments can also follow this procedure to install the KOTS CLI.

To manually download and install the KOTS CLI:

1. Download the KOTS CLI release for your operating system.

   You can run one of the following commands to download the latest version of the KOTS CLI from the [Releases](https://github.com/replicatedhq/kots/releases/latest) page in the KOTS GitHub repository:

   * **MacOS (AMD and ARM)**:

      ```bash
      curl -L https://github.com/replicatedhq/kots/releases/latest/download/kots_darwin_all.tar.gz
      ```

   * **Linux (AMD)**:

      ```bash
      curl -L https://github.com/replicatedhq/kots/releases/latest/download/kots_linux_amd64.tar.gz
      ```

   * **Linux (ARM)**:

      ```bash
      curl -L https://github.com/replicatedhq/kots/releases/latest/download/kots_linux_arm64.tar.gz
      ```

1. Unarchive the `.tar.gz` file that you downloaded:

   * **MacOS (AMD and ARM)**:

      ```bash
      tar xvf kots_darwin_all.tar.gz
      ```
   * **Linux (AMD)**:

      ```bash
      tar xvf kots_linux_amd64.tar.gz
      ```
   * **Linux (ARM)**:

      ```bash
      tar xvf kots_linux_arm64.tar.gz
      ```

1. Rename the `kots` executable to `kubectl-kots` and move it to one of the directories that is in your PATH environment variable. This ensures that the system can access the executable when you run KOTS CLI commands.

   :::note
   You can run `echo $PATH` to view the list of directories in your PATH.
   :::

   Run one of the following commands, depending on if you have write access to the target directory:

   * **You have write access to the directory**:

     ```bash
     mv kots /PATH_TO_TARGET_DIRECTORY/kubectl-kots
     ```
     Replace `PATH_TO_TARGET_DIRECTORY` with the path to a directory that is in your PATH environment variable. For example, `/usr/local/bin`.

   * **You do _not_ have write access to the directory**: 

     ```bash
     sudo mv kots /PATH_TO_TARGET_DIRECTORY/kubectl-kots
     ```
     Replace `PATH_TO_TARGET_DIRECTORY` with the path to a directory that is in your PATH environment variable. For example, `/usr/local/bin`.

1. Verify the installation:

   ```
   kubectl kots --help
   ```

## Uninstall

The KOTS CLI is a plugin for the Kubernetes kubectl command line tool. The KOTS CLI plugin is named `kubectl-kots`.

For more information about working with kubectl, see [Command line tool (kubectl)](https://kubernetes.io/docs/reference/kubectl/) in the Kubernetes documentation.

To uninstall the KOTS CLI:

1. Find the location where the `kubectl-kots` plugin is installed on your `PATH`:

   ```
   kubectl plugin list kubectl-kots cli
   ```

2. Delete `kubectl-kots`:

   ```
   sudo rm PATH_TO_KOTS
   ```
   Replace `PATH_TO_KOTS` with the location where `kubectl-kots` is installed.

   **Example**:

   ```
   sudo rm /usr/local/bin/kubectl-kots
   ```


---


import IntroAirGap from "../partials/install/_intro-air-gap.mdx"
import PrereqsExistingCluster from "../partials/install/_prereqs-existing-cluster.mdx"
import BuildAirGapBundle from "../partials/install/_airgap-bundle-build.mdx"
import DownloadAirGapBundle from "../partials/install/_airgap-bundle-download.mdx"
import ViewAirGapBundle from "../partials/install/_airgap-bundle-view-contents.mdx"
import LicenseFile from "../partials/install/_license-file-prereq.mdx"
import AirGapLicense from "../partials/install/_airgap-license-download.mdx"
import DownloadKotsBundle from "../partials/install/_download-kotsadm-bundle.mdx"
import InstallCommandPrompts from "../partials/install/_kots-install-prompts.mdx"
import AppNameUI from "../partials/install/_placeholder-app-name-UI.mdx"
import InstallKotsCliAirGap from "../partials/install/_install-kots-cli-airgap.mdx"
import PushKotsImages from "../partials/install/_push-kotsadm-images.mdx"
import PlaceholderRoCreds from "../partials/install/_placeholder-ro-creds.mdx"
import KotsVersionMatch from "../partials/install/_kots-airgap-version-match.mdx"

# Air Gap Installation in Existing Clusters with KOTS

This topic describes how to use Replicated KOTS to install an application in an existing Kubernetes cluster in an air-gapped environment.

<IntroAirGap/>

## Prerequisites

Complete the following prerequisites:

<PrereqsExistingCluster/>

* Ensure that there is a compatible Docker image registry available inside the network. For more information about Docker registry compatibility, see [Compatible Image Registries](/enterprise/installing-general-requirements#registries).

    KOTS rewrites the application image names in all application manifests to read from the on-premises registry, and it re-tags and pushes the images to the on-premises registry. When authenticating to the registry, credentials with `push` permissions are required.

    A single application expects to use a single namespace in the Docker image registry. The namespace name can be any valid URL-safe string, supplied at installation time. A registry typically expects the namespace to exist before any images can be pushed into it.

    :::note
    Amazon Elastic Container Registry (ECR) does not use namespaces.
    :::

## Install {#air-gap}

To install in an air gap cluster with KOTS:

1. Download the customer license:

   <AirGapLicense/>

1. Go the channel where the target release was promoted to build and download the air gap bundle for the release:

   <BuildAirGapBundle/>

1. <DownloadAirGapBundle/>

1. <ViewAirGapBundle/>

1. <DownloadKotsBundle/> 

1. <InstallKotsCliAirGap/>

    <KotsVersionMatch/>

1. <PushKotsImages/>

1. Install the KOTS Admin Console using the images that you pushed in the previous step:

   ```shell
   kubectl kots install APP_NAME \
     --kotsadm-registry REGISTRY_HOST \
     --registry-username RO-USERNAME \
     --registry-password RO-PASSWORD
   ```

   Replace:

    * `APP_NAME` with a name for the application. This is the unique name that KOTS will use to refer to the application that you install.
    <PlaceholderRoCreds/>

1. <InstallCommandPrompts/>

1. Access the Admin Console on port 8800. If the port forward is active, go to [http://localhost:8800](http://localhost:8800) to access the Admin Console.

    If you need to reopen the port forward to the Admin Console, run the following command:

    ```shell
    kubectl kots admin-console -n NAMESPACE
    ```
    Replace `NAMESPACE` with the namespace where KOTS is installed.

1. Log in with the password that you created during installation.

1. Upload your license file.

1. Upload the `.airgap` application air gap bundle.

1. On the config screen, complete the fields for the application configuration options and then click **Continue**.

1. On the **Preflight checks** page, the application-specific preflight checks run automatically. Preflight checks  are conformance tests that run against the target namespace and cluster to ensure that the environment meets the minimum requirements to support the application. Click **Deploy**.

    :::note
    Replicated recommends that you address any warnings or failures, rather than dismissing them. Preflight checks help ensure that your environment meets the requirements for application deployment.
    :::
    
1. (Minimal RBAC Only) If you are installing with minimal role-based access control (RBAC), KOTS recognizes if the preflight checks failed due to insufficient privileges. When this occurs, a kubectl CLI preflight command displays that lets you manually run the preflight checks. The Admin Console then automatically displays the results of the preflight checks. Click **Deploy**.

    ![kubectl CLI preflight command](/images/kubectl-preflight-command.png)

    [View a larger version of this image](/images/kubectl-preflight-command.png)

The Admin Console dashboard opens.

On the Admin Console dashboard, the application status changes from Missing to Unavailable while the Deployment is being created. When the installation is complete, the status changes to Ready. For example:

![Admin Console dashboard](/images/kotsadm-dashboard-graph.png)

[View a larger version of this image](/images/kotsadm-dashboard-graph.png)

---


import DockerCompatibility from "../partials/image-registry/_docker-compatibility.mdx"
import KubernetesCompatibility from "../partials/install/_kubernetes-compatibility.mdx"
import FirewallOpeningsIntro from "../partials/install/_firewall-openings-intro.mdx"

# KOTS Installation Requirements

This topic describes the requirements for installing in a Kubernetes cluster with Replicated KOTS.

:::note
This topic does not include any requirements specific to the application. Ensure that you meet any additional requirements for the application before installing.
:::

## Supported Browsers

The following table lists the browser requirements for the Replicated KOTS Admin Console with the latest version of KOTS.

| Browser              | Support     |
|----------------------|-------------|
| Chrome               | 66+         |
| Firefox              | 58+         |
| Opera                | 53+         |
| Edge                 | 80+         |
| Safari (Mac OS only) | 13+         |
| Internet Explorer    | Unsupported |

## Kubernetes Version Compatibility

Each release of KOTS maintains compatibility with the current Kubernetes version, and the two most recent versions at the time of its release. This includes support against all patch releases of the corresponding Kubernetes version.

Kubernetes versions 1.25 and earlier are end-of-life (EOL). For more information about Kubernetes versions, see [Release History](https://kubernetes.io/releases/) in the Kubernetes documentation.

Replicated recommends using a version of KOTS that is compatible with Kubernetes 1.26 and higher.

<KubernetesCompatibility/>

## Minimum System Requirements

To install KOTS in an existing cluster, your environment must meet the following minimum requirements:

* **KOTS Admin Console minimum requirements**: Clusters that have LimitRanges specified must support the following minimum requirements for the Admin Console:

  * **CPU resources and memory**: The Admin Console pod requests 100m CPU resources and 100Mi memory.

  * **Disk space**: The Admin Console requires a minimum of 5GB of disk space on the cluster for persistent storage, including:

    * **4GB for S3-compatible object store**: The Admin Console requires 4GB for an S3-compatible object store to store appplication archives, support bundles, and snapshots that are configured to use a host path and NFS storage destination. By default, KOTS deploys MinIO to satisfy this object storage requirement. During deployment, MinIO is configured with a randomly generated `AccessKeyID` and `SecretAccessKey`, and only exposed as a ClusterIP on the overlay network.

      :::note
      You can optionally install KOTS without MinIO by passing `--with-minio=false` with the `kots install` command. This installs KOTS as a StatefulSet using a persistent volume (PV) for storage. For more information, see [Installing KOTS in Existing Clusters Without Object Storage](/enterprise/installing-stateful-component-requirements).
      :::

    * **1GB for rqlite PersistentVolume**: The Admin Console requires 1GB for a rqlite StatefulSet to store version history, application metadata, and other small amounts of data needed to manage the application(s). During deployment, the rqlite component is secured with a randomly generated password, and only exposed as a ClusterIP on the overlay network.  

* **Supported operating systems**: The following are the supported operating systems for nodes:
  * Linux AMD64
  * Linux ARM64

* **Available StorageClass**: The cluster must have an existing StorageClass available. KOTS creates the required stateful components using the default StorageClass in the cluster. For more information, see [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) in the Kubernetes documentation.  

* **Kubernetes version compatibility**: The version of Kubernetes running on the cluster must be compatible with the version of KOTS that you use to install the application. This compatibility requirement does not include any specific and additional requirements defined by the software vendor for the application.

  For more information about the versions of Kubernetes that are compatible with each version of KOTS, see [Kubernetes Version Compatibility](#kubernetes-version-compatibility) above.

* **OpenShift version compatibility**: For Red Hat OpenShift clusters, the version of OpenShift must use a supported Kubernetes version. For more information about supported Kubernetes versions, see [Kubernetes Version Compatibility](#kubernetes-version-compatibility) above.

* **Storage class**: The cluster must have an existing storage class available. For more information, see [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) in the Kubernetes documentation.

* **Port forwarding**: To support port forwarding, Kubernetes clusters require that the SOcket CAT (socat) package is installed on each node.

  If the package is not installed on each node in the cluster, you see the following error message when the installation script attempts to connect to the Admin Console: `unable to do port forwarding: socat not found`.

  To check if the package that provides socat is installed, you can run `which socat`. If the package is installed, the `which socat` command prints the full path to the socat executable file. For example, `usr/bin/socat`.

  If the output of the `which socat` command is `socat not found`, then you must install the package that provides the socat command. The name of this package can vary depending on the node's operating system.

## RBAC Requirements

The user that runs the installation command must have at least the minimum role-based access control (RBAC) permissions that are required by KOTS. If the user does not have the required RBAC permissions, then an error message displays: `Current user has insufficient privileges to install Admin Console`.

The required RBAC permissions vary depending on if the user attempts to install KOTS with cluster-scoped access or namespace-scoped access:
* [Cluster-scoped RBAC Requirements (Default)](#cluster-scoped)
* [Namespace-scoped RBAC Requirements](#namespace-scoped)

### Cluster-scoped RBAC Requirements (Default) {#cluster-scoped}

By default, KOTS requires cluster-scoped access. With cluster-scoped access, a Kubernetes ClusterRole and ClusterRoleBinding are created that grant KOTS access to all resources across all namespaces in the cluster.

To install KOTS with cluster-scoped access, the user must meet the following RBAC requirements:
* The user must be able to create workloads, ClusterRoles, and ClusterRoleBindings.
* The user must have cluster-admin permissions to create namespaces and assign RBAC roles across the cluster.

### Namespace-scoped RBAC Requirements {#namespace-scoped}

KOTS can be installed with namespace-scoped access rather than the default cluster-scoped access. With namespace-scoped access, a Kubernetes Role and RoleBinding are automatically created that grant KOTS permissions only in the namespace where it is installed.

:::note
Depending on the application, namespace-scoped access for KOTS is required, optional, or not supported. Contact your software vendor for application-specific requirements.
:::

To install or upgrade KOTS with namespace-scoped access, the user must have _one_ of the following permission levels in the target namespace:
* Wildcard Permissions (Default)
* Minimum KOTS RBAC Permissions

See the sections below for more information.

#### Wildcard Permissions (Default)

By default, when namespace-scoped access is enabled, KOTS attempts to automatically create the following Role to acquire wildcard (`* * *`) permissions in the target namespace:

  ```yaml
  apiVersion: "rbac.authorization.k8s.io/v1"
  kind: "Role"
  metadata:
    name: "kotsadm-role"
  rules:
    - apiGroups: ["*"]
      resources: ["*"]
      verb: "*"
  ```

   To support this default behavior, the user must also have `* * *` permissions in the target namespace.

#### Minimum KOTS RBAC Permissions

In some cases, it is not possible to grant the user `* * *` permissions in the target namespace. For example, an organization might have security policies that prevent this level of permissions.

  If the user installing or upgrading KOTS cannot be granted `* * *` permissions in the namespace, then they can instead request the minimum RBAC permissions required by KOTS. Using the minimum KOTS RBAC permissions also requires manually creating a ServiceAccount, Role, and RoleBinding for KOTS, rather than allowing KOTS to automatically create a Role with `* * *` permissions.

  To use the minimum KOTS RBAC permissions to install or upgrade:

  1. Ensure that the user has the minimum RBAC permissions required by KOTS. The following lists the minimum RBAC permissions:

     ```yaml
      - apiGroups: [""]
        resources: ["configmaps", "persistentvolumeclaims", "pods", "secrets", "services", "limitranges"]
        verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
      - apiGroups: ["apps"]
        resources: ["daemonsets", "deployments", "statefulsets"]
        verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
      - apiGroups: ["batch"]
        resources: ["jobs", "cronjobs"]
        verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
      - apiGroups: ["networking.k8s.io", "extensions"]
        resources: ["ingresses"]
        verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
      - apiGroups: [""]
        resources: ["namespaces", "endpoints", "serviceaccounts"]
        verbs: ["get"]
      - apiGroups: ["authorization.k8s.io"]
        resources: ["selfsubjectaccessreviews", "selfsubjectrulesreviews"]
        verbs: ["create"]
      - apiGroups: ["rbac.authorization.k8s.io"]
        resources: ["roles", "rolebindings"]
        verbs: ["get"]
      - apiGroups: [""]
        resources: ["pods/log", "pods/exec"]
        verbs: ["get", "list", "watch", "create"]
      - apiGroups: ["batch"]
        resources: ["jobs/status"]
        verbs: ["get", "list", "watch"]
     ```

     :::note
     The minimum RBAC requirements can vary slightly depending on the cluster's Kubernetes distribution and the version of KOTS. Contact your software vendor if you have the required RBAC permissions listed above and you see an error related to RBAC during installation or upgrade.
     ::: 

   1. Save the following ServiceAccount, Role, and RoleBinding to a single YAML file, such as `rbac.yaml`:

      ```yaml
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        labels:
          kots.io/backup: velero
          kots.io/kotsadm: "true"
        name: kotsadm
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        labels:
          kots.io/backup: velero
          kots.io/kotsadm: "true"
        name: kotsadm-role
      rules:
        - apiGroups: [""]
          resources: ["configmaps", "persistentvolumeclaims", "pods", "secrets", "services", "limitranges"]
          verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
        - apiGroups: ["apps"]
          resources: ["daemonsets", "deployments", "statefulsets"]
          verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
        - apiGroups: ["batch"]
          resources: ["jobs", "cronjobs"]
          verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
        - apiGroups: ["networking.k8s.io", "extensions"]
          resources: ["ingresses"]
          verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
        - apiGroups: [""]
          resources: ["namespaces", "endpoints", "serviceaccounts"]
          verbs: ["get"]
        - apiGroups: ["authorization.k8s.io"]
          resources: ["selfsubjectaccessreviews", "selfsubjectrulesreviews"]
          verbs: ["create"]
        - apiGroups: ["rbac.authorization.k8s.io"]
          resources: ["roles", "rolebindings"]
          verbs: ["get"]
        - apiGroups: [""]
          resources: ["pods/log", "pods/exec"]
          verbs: ["get", "list", "watch", "create"]
        - apiGroups: ["batch"]
          resources: ["jobs/status"]
          verbs: ["get", "list", "watch"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        labels:
          kots.io/backup: velero
          kots.io/kotsadm: "true"
        name: kotsadm-rolebinding
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: kotsadm-role
      subjects:
        - kind: ServiceAccount
          name: kotsadm
      ```

    1. If the application contains any Custom Resource Definitions (CRDs), add the CRDs to the Role in the YAML file that you created in the previous step with as many permissions as possible: `["get", "list", "watch", "create", "update", "patch", "delete"]`.

       :::note
       Contact your software vendor for information about any CRDs that are included in the application.
       :::

       **Example**

       ```yaml
       rules:
       - apiGroups: ["stable.example.com"]
         resources: ["crontabs"]
         verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
       ```
    
    1. Run the following command to create the RBAC resources for KOTS in the namespace:

        ```
        kubectl apply -f RBAC_YAML_FILE -n TARGET_NAMESPACE
        ```

        Replace:
        * `RBAC_YAML_FILE` with the name of the YAML file with the ServiceAccount, Role, and RoleBinding and that you created.
        * `TARGET_NAMESPACE` with the namespace where the user will install KOTS.

:::note
After manually creating these RBAC resources, the user must include both the `--ensure-rbac=false` and `--skip-rbac-check` flags when installing or upgrading. These flags prevent KOTS from checking for or attempting to create a Role with `* * *` permissions in the namespace. For more information, see [Prerequisites](installing-existing-cluster#prerequisites) in _Online Installation in Existing Clusters with KOTS_.
:::

## Compatible Image Registries {#registries}

A private image registry is required for air gap installations with KOTS in existing clusters. You provide the credentials for a compatible private registry during installation. You can also optionally configure a local private image registry for use with installations in online (internet-connected) environments.

Private registry settings can be changed at any time. For more information, see [Configuring Local Image Registries](image-registry-settings).

KOTS has been tested for compatibility with the following registries:

<DockerCompatibility/>

## Firewall Openings for Online Installations with KOTS in an Existing Cluster {#firewall}

<FirewallOpeningsIntro/>

<table>
  <tr>
      <th width="50%">Domain</th>
      <th>Description</th>
  </tr>
  <tr>
      <td>Docker Hub</td>
      <td><p>Some dependencies of KOTS are hosted as public images in Docker Hub. The required domains for this service are `index.docker.io`, `cdn.auth0.com`, `*.docker.io`, and `*.docker.com.`</p></td>
  </tr>
  <tr>
      <td>`proxy.replicated.com` &#42;</td>
      <td><p>Private Docker images are proxied through `proxy.replicated.com`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `proxy.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L52-L57) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`replicated.app`</td>
      <td><p>Upstream application YAML and metadata is pulled from `replicated.app`. The current running version of the application (if any), as well as a license ID and application ID to authenticate, are all sent to `replicated.app`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `replicated.app`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L60-L65) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`registry.replicated.com` &#42;&#42;</td>
      <td><p>Some applications host private images in the Replicated registry at this domain. The on-prem docker client uses a license ID to authenticate to `registry.replicated.com`. This domain is owned by Replicated, Inc which is headquartered in Los Angeles, CA.</p><p> For the range of IP addresses for `registry.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L20-L25) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`kots.io`</td>
      <td><p>Requests are made to this domain when installing the Replicated KOTS CLI. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p></td>
  </tr>
  <tr>
     <td>`github.com`</td>
     <td>Requests are made to this domain when installing the Replicated KOTS CLI. For information about retrieving GitHub IP addresses, see [About GitHub&#39;s IP addresses](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/about-githubs-ip-addresses) in the GitHub documentation.</td>
  </tr>
</table>

&#42; Required only if the application uses the [Replicated proxy registry](/vendor/private-images-about).

&#42;&#42; Required only if the application uses the [Replicated registry](/vendor/private-images-replicated).


---


# Creating and Scheduling Backups

This topic describes how to use the Replicated snapshots feature to create backups. It also includes information about how to use the Replicated KOTS Admin Console create a schedule for automatic backups. For information about restoring, see [Restoring from Backups](snapshots-restoring-full).

## Prerequisites

- Before you can create backups, you must configure a storage destination:

   - [Configuring a Host Path Storage Destination](snapshots-configuring-hostpath)
   - [Configuring an NFS Storage Destination](snapshots-configuring-nfs)
   - [Configuring Other Storage Destinations](snapshots-storage-destinations)

- If you have multiple applications in the Admin Console, make sure that each application has its own Backup custom resource file so that they can be included in the full backup. Use the **View file** tab to check for the Backup custom resources (`kind: Backup`, `apiVersion: velero.io/v1`). 

   If any Backup custom resource files are missing, contact your vendor.

## Create a Full Backup (Recommended) {#full}

Full backups, or _instance snapshots_, back up the Admin Console and all application data, including application volumes and manifest files. If you manage multiple applications with the Admin Console, data from all applications that support backups is included in a full backup.

From a full backup, you can:
* Restore application and Admin Console data
* Restore only application data
* Restore only Admin Console data

You can create a full backup with the following methods:
* [Create a Backup with the CLI](#cli-backup)
* [Create a Backup in the Admin Console](#admin-console-backup)

### Create a Backup with the CLI {#cli-backup}

To create a full backup with the Replicated KOTS CLI, run the following command:

   ```
   kubectl kots backup --namespace NAMESPACE
   ```
   Replace `NAMESPACE` with the namespace where the Admin Console is installed.
   
For more information, see [backup](/reference/kots-cli-backup-index) in _KOTS CLI_.

### Create a Backup in the Admin Console {#admin-console-backup}

To create a full backup in the Admin Console:

1. To check if backups are supported for an application, go to the **View files** page, open the `upstream` folder, and confirm that the application includes a manifest file with `kind: Backup` and `apiVersion: velero.io/v1`. This manifest also shows which pod volumes are being backed up.

1. Go to **Snapshots > Full Snapshots (Instance)**.
1. Click **Start a snapshot**.
   
   When the backup is complete, it appears in the list of backups on the page, as shown in the following image:
   
   ![Full snapshot page with one completed snapshot](/images/snapshot-instance-list.png)

## Create a Partial Backup {#partial}

Partial backups, or _application snapshots_, back up application volumes and application manifests only. Partial backups do not back up Admin Console data.

:::note
Replicated recommends that you create full backups instead of partial backups because partial backups are not suitable for disaster recovery. See [Create a Full Backup](#full) above.
:::

To create a partial backup in the Admin Console:

1. Go to **Snapshots > Partial Snapshots (Application)**.

1. If you manage multiple applications in the Admin Console, use the dropdown to select the application that you want to back up. 

1. Click **Start a snapshot**.

   When the snapshot is complete, it appears in the list of snapshots on the page as shown in the following image:

   ![Partial snapshot page with one completed snapshot](/images/snapshot-application-list.png)

## Schedule Automatic Backups

You can use the Admin Console to schedule full or partial backups. This is useful for automatically creating regular backups of Admin Console and application data.

To schedule automatic backups in the Admin Console:

1. Go to **Snapshots > Settings & Schedule**.

1. Under **Automatic snapshots**, select **Full snapshots (Instance)** or **Partial snapshots (Application)** depending on the type of backup that you want to schedule.

   ![Snapshot Settings and Schedule page](/images/snapshot-schedule.png)

1. (Partial Backups Only) If you manage multiple applications in the Admin Console, use the dropdown to select the application that you want to back up.

1. Select **Enable automatic scheduled snapshots**. 

1. Configure the automatic backup schedule for the type of snapshots that you selected:

   * For **Schedule**, select Hourly, Daily, Weekly, or Custom.
   * For **Cron Expression**, enter a cron expression to create a custom automatic backup schedule. For information about supported cron expressions, see [Cron Expressions](/reference/cron-expressions).

1. (Optional) For **Retention Policy**, edit the amount of time that backup data is saved. By default, backup data is saved for 30 days.

   The retention policy applies to all backups, including both automatically- and manually-created backups. Changing the retention policy affects only backups created after the time of the change.
## Additional Resources

[Troubleshooting Snapshots](snapshots-troubleshooting-backup-restore)


---


import RestoreTable from "../partials/snapshots/_restoreTable.mdx"
import RestoreTypes from "../partials/snapshots/_restore-types.mdx"
import GetBackups from "../partials/snapshots/_step-get-backups.mdx"
import Restore from "../partials/snapshots/_step-restore.mdx"
import Dr from "../partials/snapshots/_limitation-dr.mdx"
import Os from "../partials/snapshots/_limitation-os.mdx"
import InstallMethod from "../partials/snapshots/_limitation-install-method.mdx"
import CliRestores from "../partials/snapshots/_limitation-cli-restores.mdx"

# Restoring from Backups

This topic describes how to restore from full or partial backups using Replicated snapshots.

## Overview

<RestoreTypes/>

You can do any type of restore from a full backup using the KOTS CLI. You can also restore an application from a full or partial backup using the Admin Console.

## Limitations

The following limitations apply to restoring from backups using snapshots:

* <Dr/>
* <Os/>
* <InstallMethod/>
* <CliRestores/>

For a full list of limitations and considerations related to the snapshots feature, see  [Limitations and Considerations](/vendor/snapshots-overview#limitations-and-considerations) in _About Backup and Restore_. 

## Restore From a Full Backup Using the CLI {#full-cli}

You can use the KOTS CLI to restore both the Admin Console and the application, the Admin Console only, or the application only. If you need to restore the Admin Console, you must use the KOTS CLI because the Admin Console gets recreated and is disconnected during the restore process.

:::note
<CliRestores/>
:::

To restore using the CLI, see the corresponding procedure for your environment:

- [Existing Clusters](#existing)
- [Online kURL Clusters](#online)
- [Air Gap kURL Clusters](#air-gapped)

### Existing Clusters {#existing}

:::note
If you are restoring to a healthy cluster, you can skip reinstalling Velero and continue to running the `get backups` and `restore` commands in the last two steps.
:::

To restore a full backup in an existing cluster:

1. (New or Unhealthy Clusters Only) In the cluster where you will do the restore, install a version of Velero that is compatible with the version that was used to make the snapshot backup.

    The Velero installation command varies depending on the storage destination for the backup. For the Velero installation command, see one of the following:

    * **Host Path:** See [Configuring a Host Path Storage Destination](snapshots-configuring-hostpath)
    * **NFS:** See [Configuring an NFS Storage Destination](snapshots-configuring-nfs) or  for the configuration steps and how to set up Velero.
    * **AWS, GCP, Azure, or other S3:** See [Configuring Other Storage Destinations](snapshots-storage-destinations). 

1. <GetBackups/>

1. <Restore/>

### Online Embedded kURL Clusters {#online}

:::note
If you are restoring to a healthy cluster, you can skip the installation and configuration steps and continue to running the `get backups` and `restore` commands in the last two steps.
:::

To restore a full backup in a kURL cluster:

1. (New or Unhealthy Clusters Only) Provision a cluster with kURL and install the target application in the cluster. See [Online Installation with kURL](installing-kurl).

1. (New or Unhealthy Clusters Only) In the new kURL cluster, configure a storage destination that holds the backup you want to use:

    * **Host Path:** See [Configuring a Host Path Storage Destination](snapshots-configuring-hostpath)
    * **NFS:** See [Configuring an NFS Storage Destination](snapshots-configuring-nfs) or  for the configuration steps and how to set up Velero.
    * **AWS, GCP, Azure, or other S3:** See [Configuring Other Storage Destinations](snapshots-storage-destinations).

1. <GetBackups/>

1. <Restore/>

### Air Gap kURL Clusters {#air-gapped}

To restore a full backup in an air gap kURL cluster:

1. Run the following command to install a new cluster and provide kURL with the correct registry IP address. kURL must be able to assign the same IP address to the embedded private image registry in the new cluster.

    ```bash
    cat install.sh | sudo bash -s airgap kurl-registry-ip=IP
    ```

    Replace `IP` with the registry IP address.

1. Use the KOTS CLI to configure Velero to use a storage destination. The storage backend used for backups must be accessible from the new cluster. 

    * **Host Path:** See [Configuring a Host Path Storage Destination](snapshots-configuring-hostpath)
    * **NFS:** See [Configuring an NFS Storage Destination](snapshots-configuring-nfs) or  for the configuration steps and how to set up Velero.
    * **S3-Compatible:** See [Configure S3-Compatible Storage for Air Gapped Environments](snapshots-storage-destinations#configure-s3-compatible-storage-for-air-gapped-environments) in _Configuring Other Storage Destinations_.

1. <GetBackups/>

1. <Restore/>

## Restore the Application Only Using the Admin Console {#admin-console}

You can restore an application from a full or partial backup using the Admin Console.

### Restore an Application From a Full Backup

To restore an application from a full backup:

1. Select **Full Snapshots (Instance)** from the Snapshots tab.

    ![Full Snapshot tab](/images/full-snapshot-tab.png)

    [View a larger version of this image](/images/full-snapshot-tab.png)

1. Click the **Restore from this backup** icon (the circular blue arrows) for the backup that you want to restore.

1. In the **Restore from backup** dialog, select **Partial restore**.

    ![Restore Full Snapshot dialog](/images/restore-backup-dialog.png)

    [View a larger version of this image](/images/restore-backup-dialog.png)

    :::note
    You can also get the CLI commands for full restores or Admin Console only restores from this dialog.
    :::     

1. At the bottom of the dialog, enter the application slug provided by your software vendor. For more information, see [Get the Application Slug](/vendor/vendor-portal-manage-app#slug) in _Managing Applications_.

1. Click **Confirm and restore**.

### Restore an Application From a Partial Backup 

To restore an application from a partial backup:

1. Select **Partial Snapshots (Application)** from the Snapshots tab.

    ![Partial Snapshot tab](/images/partial-snapshot-tab.png)

    [View a larger version of this image](/images/partial-snapshot-tab.png)

1. Click the **Restore from this backup** icon (the circular blue arrows) for the backup that you want to restore.

    The **Restore from Partial backup (Application)** dialog opens.

1. Under **Type your application slug to continue**, enter the application slug provided by your software vendor. For more information, see [Get the Application Slug](/vendor/vendor-portal-manage-app#slug) in _Managing Applications_.

    ![Restore Partial Snapshot dialog](/images/restore-partial-dialog.png)

    [View a larger version of this image](/images/restore-partial-dialog.png)

1. Click **Confirm and restore**.

## Additional Resources

[Troubleshooting Snapshots](snapshots-troubleshooting-backup-restore)

---


# Installing the Velero CLI

You install the Velero CLI before installing Velero and configuring a storage destination for backups.

:::note
For embedded clusters created with Replicated kURL, if the kURL Installer spec included the Velero add-on, then Velero was automatically installed with default internal storage. Replicated recommends that you proceed to change the default internal storage because it is insufficient for disaster recovery. See [Updating Storage Settings in the Admin Console](snapshots-updating-with-admin-console).
:::

## Install the Velero CLI in an Online Cluster

To install the Velero CLI in an online cluster:

1. Do one of the following:

    - (Embedded kURL cluster) Run an SSH command to access and authenticate to your cluster node.
    - (Existing cluster) Open a terminal in the environment that you manage the cluster from, which can be a local machine that has kubectl installed.

1. Check for the latest supported release of the Velero CLI for **Linux AMD64** in the Velero GitHub repo at https://github.com/vmware-tanzu/velero/releases. Although earlier versions of Velero are supported, Replicated recommends using the latest supported version. For more information about supported versions, see [Velero Version Compatibility](/vendor/snapshots-overview#velero-version-compatibility).

    Note the version number for the next step.

1. Run the following command to download the latest supported Velero CLI version for the **Linux AMD64** operating system to the cluster:

   ```
   curl -LO https://github.com/vmware-tanzu/velero/releases/download/VERSION/velero-VERSION-linux-amd64.tar.gz
   ```

   Replace VERSION with the version number using the format `vx.x.x`

   **Example:**

   ```
   curl -LO https://github.com/vmware-tanzu/velero/releases/download/v1.10.1/velero-v1.10.1-linux-amd64.tar.gz
   ```

1. Run the following command to uncompress the TAR file:

   ```
   tar zxvf velero-VERSION-linuxamd64.tar.gz
   ```
   Replace VERSION with the version number using the format `vx.x.x`.

1. Run the following command to install the Velero CLI:
  
   ```
   sudo mv velero-VERSION-linux-amd64/velero /usr/local/bin/velero
   ```
   Replace VERSION with the version number using the format `vx.x.x`.

1. Run `velero version` to test that the Velero CLI installation worked correctly.

   You might get an error message stating that there are no matches for the server version. This is acceptable, as long as you get a confirmation for the client version. After the Velero installation, you also see the server version.

## Install the Velero CLI in an Air Gapped Cluster

To install the Velero CLI in an air gapped cluster:

1. From a computer with internet access, check for the latest supported release of the Velero CLI for **Linux AMD64** in the Velero GitHub repo at https://github.com/vmware-tanzu/velero/releases. Although earlier versions of Velero are supported, Replicated recommends using the latest supported version. See [Velero Version Compatibility](/vendor/snapshots-overview#velero-version-compatibility).

    Note the version number for the next step.

1. Run the following command to download the latest supported Velero CLI version for the **Linux AMD64** operating system to the cluster:

   ```
   curl -LO https://github.com/vmware-tanzu/velero/releases/download/VERSION/velero-VERSION-linux-amd64.tar.gz
   ```

   Replace VERSION with the version number using the format `vx.x.x`

   **Example:**

   ```
   curl -LO https://github.com/vmware-tanzu/velero/releases/download/v1.10.1/velero-v1.10.1-linux-amd64.tar.gz
   ```

1. Copy the TAR file to the air gapped node.

1. Run the following command to uncompress the TAR file:

   ```
   tar zxvf velero-VERSION-linuxamd64.tar.gz
   ```
   Replace VERSION with the version number using the format `vx.x.x`.

1. Run the following command to install the Velero CLI:
  
   ```
   sudo mv velero-VERSION-linux-amd64/velero /usr/local/bin/velero
   ```

   Replace VERSION with the version number using the format `vx.x.x`.

1. Run `velero version` to test that the Velero CLI installation worked correctly.

   You might get an error message stating that there are no matches for the server version. This is acceptable, as long as you get a confirmation for the client version. After the Velero installation, you should see the server version also.


## Next Step

Install Velero and configure a storage destination using one of the following procedures:

- [Configuring a Host Path Storage Destination](snapshots-configuring-hostpath)
- [Configuring an NFS Storage Destination](snapshots-configuring-nfs)
- [Configuring Other Storage Destinations](snapshots-storage-destinations)

---


import AdminConsole from "../partials/updating/_admin-console.mdx"
import AdminConsoleAirGap from "../partials/updating/_admin-console-air-gap.mdx"
import PushKotsImages from "../partials/install/_push-kotsadm-images.mdx"
import BuildAirGapBundle from "../partials/install/_airgap-bundle-build.mdx"
import DownloadAirGapBundle from "../partials/install/_airgap-bundle-download.mdx"
import ViewAirGapBundle from "../partials/install/_airgap-bundle-view-contents.mdx"

# Performing Updates in Existing Clusters

This topic describes how to perform updates in existing cluster installations with Replicated KOTS. It includes information about how to update applications and the version of KOTS running in the cluster.

## Update an Application

You can perform an application update using the KOTS Admin Console or the KOTS CLI. You can also set up automatic updates. See [Configuring Automatic Updates](/enterprise/updating-apps).

### Using the Admin Console

#### Online Environments

<AdminConsole/>

#### Air Gap Environments

<AdminConsoleAirGap/>

### Using the KOTS CLI

You can use the KOTS CLI [upstream upgrade](/reference/kots-cli-upstream-upgrade) command to update an application in existing cluster installations.

#### Online Environments

To update an application in online environments:

```bash
kubectl kots upstream upgrade APP_SLUG -n ADMIN_CONSOLE_NAMESPACE
```
Where:
* `APP_SLUG` is the unique slug for the application. See [Get the Application Slug](/vendor/vendor-portal-manage-app#slug) in _Managing Applications_.
* `ADMIN_CONSOLE_NAMESPACE` is the namespace where the Admin Console is running.

:::note
Add the `--deploy` flag to automatically deploy this version.
:::

#### Air Gap Environments

To update an application in air gap environments:

1. In the [Vendor Portal](https://vendor.replicated.com), go the channel where the target release is promoted to build and download the new `.airgap` bundle:
   
   <BuildAirGapBundle/>

1. <DownloadAirGapBundle/>

1. <ViewAirGapBundle/>

1. Run the following command to update the application:

    ```bash
    kubectl kots upstream upgrade APP_SLUG \
      --airgap-bundle NEW_AIRGAP_BUNDLE \
      --kotsadm-registry REGISTRY_HOST[/REGISTRY_NAMESPACE] \
      --registry-username RO_USERNAME \
      --registry-password RO_PASSWORD \
      -n ADMIN_CONSOLE_NAMESPACE
    ```
    Replace:
    * `APP_SLUG` with the unique slug for the application. See [Get the Application Slug](/vendor/vendor-portal-manage-app#slug) in _Managing Applications_.
    * `NEW_AIRGAP_BUNDLE` with the `.airgap` bundle for the target application version.
    * `REGISTRY_HOST` with the private registry that contains the Admin Console images.
    * `REGISTRY_NAMESPACE` with the registry namespace where the images are hosted (Optional). 
    * `RO_USERNAME` and `RO_PASSWORD` with the username and password for an account that has read-only access to the private registry.
    * `ADMIN_CONSOLE_NAMESPACE` with the namespace where the Admin Console is running.

:::note
Add the `--deploy` flag to automatically deploy this version.
:::

## Update KOTS

This section describes how to update the version of Replicated KOTS running in your cluster. For information about the latest versions of KOTS, see [KOTS Release Notes](/release-notes/rn-app-manager).

:::note
Downgrading KOTS to a version earlier than what is currently deployed is not supported.
:::

### Online Environments

To update KOTS in an online existing cluster:

1. Run _one_ of the following commands to update the KOTS CLI to the target version of KOTS:

    - **Install or update to the latest version**:

      ```
      curl https://kots.io/install | bash
      ```

    - **Install or update to a specific version**:

      ```
      curl https://kots.io/install/VERSION | bash
      ```
      Where `VERSION` is the target KOTS version.

    For more KOTS CLI installation options, including information about how to install or update without root access, see [Installing the KOTS CLI](/reference/kots-cli-getting-started).

1. Run the following command to update the KOTS Admin Console to the same version as the KOTS CLI:

   ```bash
   kubectl kots admin-console upgrade -n NAMESPACE
   ```
   Replace `NAMESPACE` with the namespace in your cluster where KOTS is installed.

### Air Gap Environments

To update KOTS in an existing air gap cluster:

1. Download the target version of the following assets from the [Releases](https://github.com/replicatedhq/kots/releases/latest) page in the KOTS GitHub repository:
   * KOTS Admin Console `kotsadm.tar.gz` bundle
   * KOTS CLI plugin
   
   Ensure that you can access the downloaded bundles from the environment where the Admin Console is running.

1. Install or update the KOTS CLI to the version that you downloaded. See [Manually Download and Install](/reference/kots-cli-getting-started#manually-download-and-install) in _Installing the KOTS CLI_.

1. <PushKotsImages/>

1. Run the following command using registry read-only credentials to update the KOTS Admin Console:

    ```
    kubectl kots admin-console upgrade \
      --kotsadm-registry REGISTRY_HOST \
      --registry-username RO_USERNAME \
      --registry-password RO_PASSWORD \
      -n NAMESPACE
    ```
    Replace:
    * `REGISTRY_HOST` with the same private registry from the previous step.
    * `RO_USERNAME` with the username for credentials with read-only permissions to the registry.
    * `RO_PASSWORD` with the password associated with the username.
    * `NAMESPACE` with the namespace on your cluster where KOTS is installed.

    For help information, run `kubectl kots admin-console upgrade -h`. 

---


# About Custom Resources

You can include custom resources in releases to control the experience for applications installed with Replicated KOTS.

Custom resources are consumed by KOTS, the Admin Console, or by other kubectl plugins. Custom resources are packaged as part of the application, but are _not_ deployed to the cluster.

## KOTS Custom Resources

The following are custom resources in the `kots.io` API group:

| API Group/Version | Kind | Description |
|---------------|------|-------------|
| kots.io/v1beta1 | [Application](custom-resource-application) | Adds additional metadata (branding, release notes and more) to an application |
| kots.io/v1beta1 | [Config](custom-resource-config)| Defines a user-facing configuration screen in the Admin Console |
| kots.io/v1beta2 | [HelmChart](custom-resource-helmchart-v2) | Identifies an instantiation of a Helm Chart |
| kots.io/v1beta1 | [LintConfig](custom-resource-lintconfig) | Customizes the default rule levels for the KOTS release linter |

## Other Custom Resources

The following are custom resources in API groups other than `kots.io` that can be included in a KOTS release to configure additional functionality:

| API Group/Version | Kind | Description |
|---------------|------|-------------|
| app.k8s.io/v1beta1 | [SIG Application](https://github.com/kubernetes-sigs/application#kubernetes-applications) | Defines metadata about the application |
| cluster.kurl.sh/v1beta1 | [Installer](https://kurl.sh/docs/create-installer/) | Defines a Replicated kURL distribution |
| embeddedcluster.replicated.com/v1beta1 | [Config](/reference/embedded-config) | Defines a Replicated Embedded Cluster distribution |
| troubleshoot.replicated.com/v1beta2 | [Preflight](custom-resource-preflight) | Defines the data to collect and analyze for custom preflight checks |
| troubleshoot.replicated.com/v1beta2 | [Redactor](https://troubleshoot.sh/reference/redactors/overview/) | Defines custom redactors that apply to support bundles and preflight checks |
| troubleshoot.sh/v1beta2 | [Support Bundle](custom-resource-preflight) | Defines the data to collect and analyze for a support bundle |
| velero.io/v1 | [Backup](https://velero.io/docs/v1.10/api-types/backup/) | A Velero backup request, triggered when the user initiates a backup with Replicated snapshots |



---


import Title from "../partials/custom-resource-application/_title.mdx"
import Icon from "../partials/custom-resource-application/_icon.mdx"
import ReleaseNotes from "../partials/custom-resource-application/_releaseNotes.mdx"
import AllowRollback from "../partials/custom-resource-application/_allowRollback.mdx"
import AdditionalNamespaces from "../partials/custom-resource-application/_additionalNamespaces.mdx"
import AdditionalImages from "../partials/custom-resource-application/_additionalImages.mdx"
import RequireMinimalRBACPrivileges from "../partials/custom-resource-application/_requireMinimalRBACPrivileges.mdx"
import SupportMinimalRBACPrivileges from "../partials/custom-resource-application/_supportMinimalRBACPrivileges.mdx"
import Ports from "../partials/custom-resource-application/_ports.mdx"
import StatusInformers from "../partials/custom-resource-application/_statusInformers.mdx"
import Graphs from "../partials/custom-resource-application/_graphs.mdx"
import GraphsTemplates from "../partials/custom-resource-application/_graphs-templates.mdx"
import TargetKotsVersion from "../partials/custom-resource-application/_targetKotsVersion.mdx"
import MinKotsVersion from "../partials/custom-resource-application/_minKotsVersion.mdx"
import ProxyRegistryDomain from "../partials/custom-resource-application/_proxyRegistryDomain.mdx"
import ReplicatedRegistryDomain from "../partials/custom-resource-application/_replicatedRegistryDomain.mdx"
import ServicePortNote from "../partials/custom-resource-application/_servicePort-note.mdx"
import PortsServiceName from "../partials/custom-resource-application/_ports-serviceName.mdx"
import PortsLocalPort from "../partials/custom-resource-application/_ports-localPort.mdx"
import PortsServicePort from "../partials/custom-resource-application/_ports-servicePort.mdx"
import PortsApplicationURL from "../partials/custom-resource-application/_ports-applicationURL.mdx"
import KurlNote from "../partials/custom-resource-application/_ports-kurl-note.mdx"

# Application

The Application custom resource enables features such as branding, release notes, port forwarding, dashboard buttons, app status indicators, and custom graphs.

There is some overlap between the Application custom resource manifest file and the [Kubernetes SIG Application custom resource](https://github.com/kubernetes-sigs/application/blob/master/docs/api.md). For example, enabling features such as [adding a button to the dashboard](/vendor/admin-console-adding-buttons-links) requires the use of both the Application and SIG Application custom resources.

The following is an example manifest file for the Application custom resource:

```yaml
apiVersion: kots.io/v1beta1
kind: Application
metadata:
  name: my-application
spec:
  title: My Application
  icon: https://support.io/img/logo.png
  releaseNotes: These are our release notes
  allowRollback: false
  targetKotsVersion: "1.60.0"
  minKotsVersion: "1.40.0"
  requireMinimalRBACPrivileges: false
  additionalImages:
    - jenkins/jenkins:lts
  additionalNamespaces:
    - "*"
  ports:
    - serviceName: web
      servicePort: 9000
      localPort: 9000
      applicationUrl: "http://web"
  statusInformers:
    - deployment/my-web-svc
    - deployment/my-worker
  graphs:
    - title: User Signups
      query: 'sum(user_signup_events_total)'
```

## title

<table>
  <tr>
    <th>Description</th>
    <td>The application title. Used on the license upload and in various places in the Replicated Admin Console.</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><Title/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr>     
</table>

## icon

<table>
  <tr>
    <th>Description</th>
    <td>The icon file for the application. Used on the license upload, in various places in the Admin Console, and in the Download Portal. The icon can be a remote URL or a Base64 encoded image. Base64 encoded images are required to display the image in air gap installations with no outbound internet access.</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><Icon/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr>    
</table>


## releaseNotes

<table>
  <tr>
    <th>Description</th>
    <td>The release notes for this version. These can also be set when promoting a release.</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><ReleaseNotes/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr>    
</table>

## allowRollback

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Enable this flag to create a <strong>Rollback</strong> button on the Admin Console Version History page.</p>
      <p>If an application is guaranteed not to introduce backwards-incompatible versions, such as through database migrations, then the <code>allowRollback</code> flag can allow end users to easily roll back to previous versions from the Admin Console.</p>
      <p>Rollback does not revert any state. Rather, it recovers the YAML manifests that are applied to the cluster.</p>
    </td>
  </tr>
  <tr>
    <th>Example</th>
    <td><AllowRollback/></td>
  </tr>
  <tr>
    <th>Default</th>
    <td><code>false</code></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Embedded Cluster 1.17.0 and later supports partial rollbacks of the application version. Partial rollbacks are supported only when rolling back to a version where there is no change to the [Embedded Cluster Config](/reference/embedded-config) compared to the currently-installed version. For example, users can roll back to release version 1.0.0 after upgrading to 1.1.0 only if both 1.0.0 and 1.1.0 use the same Embedded Cluster Config.</td>
  </tr>    
</table>


## additionalNamespaces

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>An array of additional namespaces as strings that Replicated KOTS creates on the cluster. For more information, see <a href="/vendor/operator-defining-additional-namespaces">Defining Additional Namespaces</a>.</p>
      <p>In addition to creating the additional namespaces, KOTS ensures that the application secret exists in the namespaces. KOTS also ensures that this application secret has access to pull the application images, including both images that are used and any images you add in the <code>additionalImages</code> field. This pull secret is automatically added to all manifest files that use private images.</p>
      <p>For dynamically created namespaces, specify <code>"*"</code>.</p>
    </td>
  </tr>
  <tr>
    <th>Example</th>
    <td><AdditionalNamespaces/></td>
  </tr>  
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr>
</table>

## additionalImages

<table>
  <tr>
    <th>Description</th>
    <td><p>An array of strings that reference images to be included in air gap bundles and pushed to the local registry during installation.</p><p>KOTS detects images from the PodSpecs in the application. Some applications, such as Operators, might need to include additional images that are not referenced until runtime. For more information, see <a href="/vendor/operator-defining-additional-images">Defining Additional Images</a>.</p></td>
  </tr>
  <tr>
    <th>Example</th>
    <td><AdditionalImages/></td>
  </tr>  
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>   
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr> 
</table>

## requireMinimalRBACPrivileges

<table>
  <tr>
    <th>Description</th>
    <td><p><code>requireMinimalRBACPrivileges</code> applies to existing clusters only.</p><p>Requires minimal role-based access control (RBAC) be used for all customer installations. When set to <code>true</code>, KOTS creates a namespace-scoped Role and RoleBinding, instead of the default cluster-scoped ClusterRole and ClusterRoleBinding.</p><p>For additional requirements and limitations related to using namespace-scoped RBAC, see <a href="/vendor/packaging-rbac#min-rbac">About Namespace-scoped RBAC</a> in <em>Configuring KOTS RBAC</em>.</p></td>
  </tr>
  <tr>
    <th>Example</th>
    <td><RequireMinimalRBACPrivileges/></td>
  </tr>
  <tr>
    <th>Default</th>
    <td><code>false</code></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>No</td>
  </tr>
</table>

## supportMinimalRBACPrivileges

<table>
  <tr>
    <th>Description</th>
    <td><p><code>supportMinimalRBACPrivileges</code> applies to existing clusters only.</p><p>Allows minimal role-based access control (RBAC) be used for all customer installations. When set to <code>true</code>, KOTS supports creating a namespace-scoped Role and RoleBinding, instead of the default cluster-scoped ClusterRole and ClusterRoleBinding.</p><p> Minimal RBAC is not used by default. It is only used when the <code>--use-minimal-rbac</code> flag is passed to the <code>kots install</code> command.</p><p>For additional requirements and limitations related to using namespace-scoped RBAC, see <a href="/vendor/packaging-rbac#min-rbac">About Namespace-scoped RBAC</a> in <em>Configuring KOTS RBAC</em>.</p></td>
  </tr>
  <tr>
    <th>Example</th>
    <td><SupportMinimalRBACPrivileges/></td>
  </tr>
  <tr>
    <th>Default</th>
    <td><code>false</code></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>No</td>
  </tr>
</table>

## ports

<table>
<tr>
    <th>Description</th>
    <td>
      <p>Extra ports (additional to the <code>8800</code> Admin Console port) that are port-forwarded when running the <code>kubectl kots admin-console</code> command. With ports specified, KOTS can establish port forwarding to simplify connections to the deployed application. When the application starts and the service is ready, the KOTS CLI will print a message in the terminal with the URL where the port-forwarded service can be accessed. For more information, see <a href="/vendor/admin-console-port-forward">Port Forwarding Services with KOTS</a>.</p>
      <KurlNote/>
      <p>The <code>ports</code> key has the following fields:</p>
      <ul>
        <PortsServiceName/>
        <PortsServicePort/>
        <ServicePortNote/>
        <PortsLocalPort/>
        <PortsApplicationURL/>
        For more information about adding links to port forwarded services, see <a href="/vendor/admin-console-port-forward#add-link">Add a Link to a Port-Forwarded Service in the Admin Console</a>.
      </ul> 
    </td>
  </tr>
  <tr>
    <th>Example</th>
    <td><Ports/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td><p>Go templates are supported in the `serviceName` and `applicationUrl` fields only.</p><p>Using Go templates in the `localPort` or `servicePort` fields results in an installation error similar to the following: `json: cannot unmarshal string into Go struct field ApplicationPort.spec.ports.servicePort of type int`.</p></td>
  </tr>    
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr>
</table>

## statusInformers

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Resources to watch and report application status back to the user. When you include <code>statusInformers</code>, the dashboard can indicate when the application deployment is complete and the application is ready for use.</p>
      <p><code>statusInformers</code> use the format <code>[namespace/]type/name</code>, where namespace is optional.</p>
      <p>For more information about including statusInformers, see <a href="/vendor/admin-console-display-app-status">Adding Resource Status Informers</a>.</p>
    </td>
  </tr>
  <tr>
    <th>Example</th>
    <td><StatusInformers/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>Yes</td>
  </tr>    
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr>
</table>

## graphs

<table>
  <tr>
    <th>Description</th>
    <td><p>Custom graphs to include on the Admin Console application dashboard.For more information about how to create custom graphs, see <a href="/vendor/admin-console-prometheus-monitoring">Adding Custom Graphs</a>.</p><p><code>graphs</code> has the following fields:</p><ul><li><code>graphs.title</code>: The graph title.</li><li><code>graphs.query</code>: The Prometheus query.</li><li><code>graphs.legend</code>: The legend to use for the query line. You can use Prometheus templating in the <code>legend</code> fields with each element returned from the Prometheus query. <p><GraphsTemplates/></p></li><li><code>graphs.queries</code>: A list of queries containing a <code>query</code> and <code>legend</code>.</li>  <li><code>graphs.yAxisFormat</code>: The format of the Y axis labels with support for all Grafana units. For more information, see <a href="https://grafana.com/docs/features/panels/graph/#left-y-right-y">Visualizations</a> in the Grafana documentation.</li><li><code>graphs.yAxisTemplate</code>: Y axis labels template.</li></ul></td>
  </tr>
  <tr>
    <th>Example</th>
    <td><Graphs/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>
      <p>Yes</p>
    </td>
  </tr>   
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>No</td>
  </tr> 
</table>

## proxyRegistryDomain

:::important
`proxyRegistryDomain` is deprecated. For information about how to use a custom domain for the Replicated proxy registry, see [Using Custom Domains](/vendor/custom-domains-using).
:::

<table>	
  <tr>	
    <th>Description</th>	
    <td><p>The custom domain used for proxy.replicated.com. For more information, see <a href="/vendor/custom-domains-using">Using Custom Domains</a>.</p>	<p>Introduced in KOTS v1.91.1.</p>	</td>	
  </tr>	
  <tr>	
    <th>Example</th>	
    <td><ProxyRegistryDomain/></td>	
  </tr>	
  <tr>	
    <th>Supports Go templates?</th>	
    <td>No</td>	
  </tr>
</table>

## replicatedRegistryDomain

:::important
`replicatedRegistryDomain` is deprecated. For information about how to use a custom domain for the Replicated registry, see [Using Custom Domains](/vendor/custom-domains-using).
:::

<table>	
  <tr>	
    <th>Description</th>	
    <td><p>The custom domain used for registry.replicated.com. For more information, see <a href="/vendor/custom-domains-using">Using Custom Domains</a>.</p><p>Introduced in KOTS v1.91.1.</p>	</td>	
  </tr>	
  <tr>	
    <th>Example</th>	
    <td><ReplicatedRegistryDomain/></td>	
  </tr>	
  <tr>	
    <th>Supports Go templates?</th>	
    <td>No</td>	
  </tr>
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>Yes</td>
  </tr>	
</table>

## targetKotsVersion

<table>
  <tr>
    <th>Description</th>
    <td><p>The KOTS version that is targeted by the release. For more information, see <a href="/vendor/packaging-kots-versions">Setting Minimum and Target Versions for KOTS</a>.</p></td>
  </tr>
  <tr>
    <th>Example</th>
    <td><TargetKotsVersion/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>No. Setting <code>targetKotsVersion</code> to a version earlier than the KOTS version included in the specified version of Embedded Cluster will cause Embedded Cluster installations to fail with an error message like: <code>Error: This version of App Name requires a different version of KOTS from what you currently have installed.</code>. To avoid installation failures, do not use <code>targetKotsVersion</code> in releases that support installation with Embedded Cluster.</td>
  </tr>
</table>

## minKotsVersion (Beta)

<table>
  <tr>
    <th>Description</th>
    <td><p>The minimum KOTS version that is required by the release. For more information, see <a href="/vendor/packaging-kots-versions">Setting Minimum and Target Versions for KOTS</a>.</p></td>
  </tr>
  <tr>
    <th>Example</th>
    <td><MinKotsVersion/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>  
  <tr>
    <th>Supported for <a href="/vendor/embedded-overview">Embedded Cluster</a>?</th>
    <td>No. Setting <code>minKotsVersion</code> to a version later than the KOTS version included in the specified version of Embedded Cluster will cause Embedded Cluster installations to fail with an error message like: <code>Error: This version of App Name requires a different version of KOTS from what you currently have installed.</code>. To avoid installation failures, do not use <code>minKotsVersion</code> in releases that support installation with Embedded Cluster.</td>
  </tr>  
</table>


---


import ItemTypes from "../partials/config/_item-types.mdx"
import PropertyWhen from "../partials/config/_property-when.mdx"
import RandomStringNote from "../partials/config/_randomStringNote.mdx"
import NameExample from "../partials/config/_nameExample.mdx"
import TypeExample from "../partials/config/_typeExample.mdx" 
import DefaultExample from "../partials/config/_defaultExample.mdx" 
import ValueExample from "../partials/config/_valueExample.mdx" 
import RequiredExample from "../partials/config/_requiredExample.mdx" 
import RecommendedExample from "../partials/config/_recommendedExample.mdx" 
import HiddenExample from "../partials/config/_hiddenExample.mdx" 
import ReadonlyExample from "../partials/config/_readonlyExample.mdx" 
import WhenExample from "../partials/config/_whenExample.mdx" 
import AffixExample from "../partials/config/_affixExample.mdx" 
import HelpTextExample from "../partials/config/_helpTextExample.mdx"
import RegexValidationExample from "../partials/config/_regexValidationExample.mdx"
import WhenRequirements from "../partials/config/_when-requirements.mdx"
import WhenNote from "../partials/config/_when-note.mdx"

# Config

The Config custom resource can be provided by a vendor to specify a Config page in the Replicated Admin Console for collecting customer supplied values and template function rendering.

The settings that appear on the Admin Console Config page are specified as an array configuration _groups_ and _items_.

The following example shows three groups defined in the Config custom resource manifest file, and how these groups are displayed on the Admin Console Config page.

For more information about the properties of groups and items, see [Group Properties](#group-properties) and [Item Properties](#item-properties) below.

```yaml
apiVersion: kots.io/v1beta1
kind: Config
metadata:
  name: my-application
spec:
  groups:
    - name: example_group
      title: First Group
      items:
      - name: http_enabled
        title: HTTP Enabled
        type: bool
        default: "0"
    - name: example_group_2
      title: Second Group
      when: false
      items:
      - name: key
        title: Key
        type: textarea
      - name: hostname
        title: Hostname
        type: text  
    - name: example_group_3
      title: Third Group
      items:
      - name: email-address
        title: Email Address
        type: text  
      - name: password_text
        title: Password
        type: password
        value: '{{repl RandomString 10}}'     
```
![Three groups of items on the config page](/images/config-screen-groups.png)
[View a larger version of this image](/images/config-screen-groups.png)

## Group Properties

Groups have a `name`, `title`, `description` and an array of `items`.

### `description`

Descriptive help text for the group that displays on the Admin Console Config page. Supports markdown formatting.

To provide help text for individual items on the Config page, use the item `help-text` property. See [help_text](#help_text) below.

```yaml
spec:
  groups:
    - name: example_group
      title: First Group
      # Provide a description of the input fields in the group
      description: Select whether or not to enable HTTP.
      items:
      - name: http_enabled
        title: HTTP Enabled
        type: bool
        default: "0"
```

### `name`

A unique identifier for the group.

```yaml
spec:
  groups:
    # The name must be unique
    - name: example_group
      title: First Group
      items:
      - name: http_enabled
        title: HTTP Enabled
        type: bool
        default: "0"
```

### `title`

The title of the group that displays on the Admin Console Config page.

```yaml
spec:
  groups:
    - name: example_group
    # First Group is the heading that appears on the Config page
      title: First Group
      items:
      - name: http_enabled
        title: HTTP Enabled
        type: bool
        default: "0"
```

### `when`

The `when` property denotes groups that are displayed on the Admin Console **Config** page only when a condition evaluates to true. When the condition evaluates to false, the group is not displayed.

<PropertyWhen/>

:::note
`when` is a property of both groups and items. See [Item Properties > `when`](/reference/custom-resource-config#when-item) below.
:::

#### Requirements and Limitations

The `when` group property has the following requirements and limitations:

<WhenRequirements/>

#### Example

In the following example, the `example_group_2` group of items will be displayed on the **Config** page only when the user enables the `http_enabled` configuration field. This example uses the KOTS [ConfigOptionEquals](/reference/template-functions-config-context#configoptionequals) template function to evaluate the value of the `http_enabled` configuration field.

```yaml
spec:
  groups:
    - name: example_group
      title: First Group
      items:
      - name: http_enabled
        title: HTTP Enabled
        type: bool
        default: "0"
    - name: example_group_2
      title: Second Group
      # This group is displayed only when the `http_enabled` field is selected 
      when: repl{{ ConfigOptionEquals "http_enabled" "1" }}
      items:
      - name: key
        title: Key
        type: textarea
      - name: hostname
        title: Hostname
        type: text  
    - name: example_group_3
      title: Third Group
      items:
      - name: email-address
        title: Email Address
        type: text  
      - name: password_text
        title: Password
        type: password
        value: '{{repl RandomString 10}}'     
```

![Only the first and third groups appear on the config screen](/images/config-screen-group-when-false.png)
[View a larger version of this image](/images/config-screen-group-when-false.png)

For additional examples, see [Using Conditional Statements in Configuration Fields](/vendor/config-screen-conditional).

### `items`

Each group contains an array of items that map to input fields on the Admin Console Config screen. All items have `name`, `title`, and `type` properties and belong to a single group.

For more information, see [Item Properties](#item-properties) and [Item Types](#item-types) below.

## Item Properties

Items have a `name`, `title`, `type`, and other optional properties.

### `affix`

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Items can be affixed <code>left</code> or <code>right</code>. Affixing items allows them to appear in the Admin Console on the same line.</p><p>Specify the <code>affix</code> field to all of the items in a particular group to preserve the line spacing and prevent the appearance of crowded text.</p>
    </td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><AffixExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>Yes</td>
  </tr>    
</table>

### `default`

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Defines the default value for the config item. If the user does not provide a value for the item, then the <code>default</code> value is applied.</p>
      <p>If the <code>default</code> value is not associated with a <code>password</code> type config item, then it appears as placeholder text in the Admin Console.</p>
    </td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><DefaultExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td><p>Yes. Every time the user makes a change to their configuration settings for the application, any template functions used in the <code>default</code> property are reevaluated.</p></td>
  </tr>    
</table>

### `help_text`

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Displays a helpful message below the <code>title</code> for the config item in the Admin Console.</p>
      <p>Markdown syntax is supported. For more information about markdown syntax, see <a href="https://guides.github.com/features/mastering-markdown/">Basic writing and formatting syntax</a> in the GitHub Docs.</p>
    </td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><HelpTextExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>Yes</td>
  </tr>    
</table> 

### `hidden`

<table>
  <tr>
    <th>Description</th>
    <td>
       <p>Hidden items are not visible in the Admin Console.</p>
       <p><RandomStringNote/></p>
    </td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><HiddenExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
</table> 

### `name`

<table>
  <tr>
    <th>Description</th>
    <td><p>A unique identifier for the config item. Item names must be unique both within the group and across all groups. The item <code>name</code> is not displayed in the Admin Console.</p><p> The item <code>name</code> can be used with KOTS template functions in the Config context (such as ConfigOption or ConfigOptionEquals) to return the value of the item. For more information, see <a href="/reference/template-functions-config-context">Config Context</a>.</p></td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><NameExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>Yes</td>
  </tr>    
</table>

### `readonly`

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Readonly items are displayed in the Admin Console and users cannot edit their value.</p>
      <p><RandomStringNote/></p>
    </td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><ReadonlyExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
</table> 

### `recommended`

<table>
  <tr>
    <th>Description</th>
    <td><p>Displays a Recommended tag for the config item in the Admin Console.</p></td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td>
      <RecommendedExample/>
    </td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
</table> 

### `required`

<table>
  <tr>
    <th>Description</th>
    <td><p>Displays a Required tag for the config item in the Admin Console. A required item prevents the application from starting until it has a value.</p></td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><RequiredExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
</table> 

### `title`

<table>
  <tr>
    <th>Description</th>
    <td><p>The title of the config item that displays in the Admin Console.</p></td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><HelpTextExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>Yes</td>
  </tr>    
</table>

### `type`

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Each item has a <code>type</code> property that defines the type of user input accepted by the field.</p>
      <p>The <code>type</code> property supports the following values: <ItemTypes/></p>
      <p>For information about each type, see <a href="#item-types">Item Types</a>.</p>
    </td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>Yes</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><TypeExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
</table>

### `value`

<table>
  <tr>
    <th>Description</th>
    <td>
      <p>Defines the value of the config item. Data that you add to <code>value</code> appears as the HTML input value for the config item in the Admin Console.</p>
      <p>If the config item is not readonly, then the data that you add to <code>value</code> is overwritten by any user input for the item. If the item is readonly, then the data that you add to <code>value</code> cannot be overwritten.</p>
    </td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td><ValueExample/></td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td><p>Yes</p><RandomStringNote/></td>
  </tr>    
</table>

### `when` {#when-item}

<table>
  <tr>
    <th>Description</th>
    <td><p>The <code>when</code> property denotes items that are displayed on the Admin Console <strong>Config</strong> page only when a condition evaluates to true. When the condition evaluates to false, the item is not displayed.</p><PropertyWhen/><p>The `when` item property has the following requirements and limitations:</p><WhenRequirements/><ul><li><code>when</code> cannot be applied to the items nested under a <code>radio</code>, <code>dropdown</code> or <code>select_one</code> item. To conditionally show or hide <code>radio</code>, <code>dropdown</code> or <code>select_one</code> items, apply the <code>when</code> property to the item itself.</li></ul><WhenNote/></td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td>
      <p>Display the <code>database_host</code> and <code>database_password</code> items only when the user selects <code>external</code> for the <code>db_type</code> item:</p><p><WhenExample/></p><p>For additional examples, see <a href="/vendor/config-screen-conditional">Using Conditional Statements in Configuration Fields</a>.</p>
    </td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>Yes</td>
  </tr>    
</table>

### `validation`

<table>
  <tr>
    <th>Description</th>
    <td><p>The <code>validation</code> property can be used to validate an item's value, <br/>allowing you to specify custom validation rules that determine whether the value is valid or not.</p></td>
  </tr>
  <tr>
    <th>Required?</th>
    <td>No</td>
  </tr>
  <tr>
    <th>Example</th>
    <td>
      <p>Validates and returns if <code>password</code> value is not matching the regex. <br/>The <code>jwt_token</code> file content is only validated if the file is uploaded since it is optional.</p>
      <RegexValidationExample/>
    </td>
  </tr>
  <tr>
    <th>Supports Go templates?</th>
    <td>No</td>
  </tr>    
</table>

For information about supported validation types, see [Item Validation](#item-validation).

## Item Types

The section describes each of the item types:
<ItemTypes/>

### `bool`
The `bool` input type should use a "0" or "1" to set the value
```yaml
    - name: group_title
      title: Group Title
      items:
      - name: http_enabled
        title: HTTP Enabled
        type: bool
        default: "0"
```

![Boolean selector on the configuration screen](/images/config-screen-bool.png)

[View a larger version of this image](/images/config-screen-bool.png)

### `dropdown`

> Introduced in KOTS v1.114.0

The `dropdown` item type includes one or more nested items that are displayed in a dropdown on the Admin Console config screen. Dropdowns are especially useful for displaying long lists of options. You can also use the [`radio`](#radio) item type to display radio buttons for items with shorter lists of options.

To set a default value for `dropdown` items, set the `default` field to the name of the target nested item.

```yaml
spec:
  groups:
  - name: example_settings
    title: My Example Config
    items:
    - name: version
      title: Version
      default: version_latest
      type: dropdown
      items:
      - name: version_latest
        title: latest
      - name: version_123
        title: 1.2.3
      - name: version_124
        title: 1.2.4
      - name: version_125
        title: 1.2.5    
```

![Dropdown item type on config screen](/images/config-screen-dropdown.png)

[View a larger version of this image](/images/config-screen-dropdown.png)

![Dropdown item type expanded](/images/config-screen-dropdown-open.png)

[View a larger version of this image](/images/config-screen-dropdown-open.png)

### `file`
A `file` is a special type of form field that renders an [`<input type="file" />`](https://www.w3schools.com/tags/tag_input.asp) HTML element.
Only the contents of the file, not the name, are captured.
See the [`ConfigOptionData`](template-functions-config-context#configoptiondata) template function for examples on how to use the file contents in your application.

```yaml
    - name: certs
      title: TLS Configuration
      items:
      - name: tls_private_key_file
        title: Private Key
        type: file
      - name: tls_certificate_file
        title: Certificate
        type: file
```

![File input field on the configuration screen](/images/config-screen-file.png)

[View a larger version of this image](/images/config-screen-file.png)

### `heading`
The `heading` type allows you to display a group heading as a sub-element within a group.
This is useful when you would like to use a config group to group items together, but still separate the items visually.

```yaml
    - name: ldap_settings
      title: LDAP Server Settings
      items:
      ...
      - name: ldap_schema
        type: heading
        title: LDAP schema
      ...
```

![Heading on the configuration screen](/images/config-screen-heading.png)

[View a larger versio of this image](/images/config-screen-heading.png)

### `label`
The `label` type allows you to display an input label.
```yaml
    - name: email
      title: Email
      items:
      - name: email-address
        title: Email Address
        type: text
      - name: description
        type: label
        title: "Note: The system will send you an email every hour."
```
![Email address label on the configuration screen](/images/config-screen-label.png)

[View a larger version of this image](/images/config-screen-label.png)

### `password`
The `password` type is a text field that hides the character input.

```yaml
    - name: password_text
      title: Password Text
      type: password
      value: '{{repl RandomString 10}}'
```

![Password text field on the configuration screen](/images/config-screen-password.png)

[View a larger version of this image](/images/config-screen-password.png)

### `radio`

> Introduced in KOTS v1.114.0

The `radio` item type includes one or more nested items that are displayed as radio buttons on the Admin Console config screen. Radio buttons are especially useful for displaying short lists of options. You can also use the [`dropdown`](#dropdown) item type for items with longer lists of options.

To set a default value for `radio` items, set the `default` field to the name of the target nested item.

```yaml
spec:
  groups:
  - name: example_settings
    title: My Example Config
    items:
    - name: authentication_type
      title: Authentication Type
      default: authentication_type_anonymous
      type: radio
      items:
      - name: authentication_type_anonymous
        title: Anonymous
      - name: authentication_type_password
        title: Password
```

### `select_one` (Deprecated)

:::important
The `select_one` item type is deprecated and is not recommended for use. To display config items with multiple options, use the [`radio`](#radio) or [`dropdown`](#dropdown) item types instead.
:::

`select_one` items must contain nested items. The nested items are displayed as radio buttons in the Admin Console.

You can use the `name` field of a `select_one` item with KOTS template functions in the Config context (such as ConfigOption or ConfigOptionEquals) to return the option selected by the user.

For example, if the user selects the **Password** option for the `select_one` item shown below, then the template function `'{{repl ConfigOption "authentication_type"}}'` is rendered as `authentication_type_password`. For more information about working with template functions in the Config context, see [Config Context](/reference/template-functions-config-context).

```yaml
spec:
  groups:
  - name: example_settings
    title: My Example Config
    description: Configuration to serve as an example for creating your own. See [https://kots.io/reference/v1beta1/config/](https://kots.io/reference/v1beta1/config/) for configuration docs. In this case, we provide example fields for configuring an Nginx welcome page.
    items:
    - name: authentication_type
      title: Authentication Type
      default: authentication_type_anonymous
      type: select_one
      items:
      - name: authentication_type_anonymous
        title: Anonymous
      - name: authentication_type_password
        title: Password  
```

![Select one field on the configuration screen](/images/config-screen-selectone.png)

### `text`
A `text` input field allows users to enter a string value.
Optionally, all additional properties are available for this input type.

```yaml
    - name: example_text_input
      title: Example Text Input
      type: text
```

![Text field on the configuration screen](/images/config-screen-text.png)

:::important
Do not store secrets or passwords in `text` items because they are not encrypted or masked and can be easily accessed. Instead, use [`password`](#password) items.
:::

### `textarea`
A `textarea` items creates a multi-line text input for when users have to enter a sizeable amount of text.

```yaml
    - name: custom_key
      title: Set your secret key for your app
      description: Paste in your Custom Key
      items:
      - name: key
        title: Key
        type: textarea
      - name: hostname
        title: Hostname
        type: text
```
![Text area field on the configuration screen](/images/config-screen-textarea.png)

## Item Validation

A `validation` can be specified to validate the value of an item. `regex` is the supported validation type.

Based on specified validation rules, the item is validated and a validation message is returned if the validation rule is not satisfied. A default message is returned if there is an empty validation message.

The validation rules are as follows:

- An item is validated only when its value is not empty.
- Items of types `text`, `textarea`, `password`, and `file` are validated, but `repeatable` items are not validated.
- If an item is marked as `hidden` or if its `when` condition is set to `false`, the item is not validated.
- If a group `when` condition is set to `false`, the items in the group are not validated.

### `regex`
For applications installed with KOTS v1.98.0 or later, a `regex` can be used to validate whether an item's value matches the provided regular expression `pattern`. The regex pattern should be of the [RE2 regular expression](https://github.com/google/re2/wiki/Syntax) type and can validate the `text`, `textarea`, `password`, and `file` field types.

 The default validation message is `Value does not match regex`.

<RegexValidationExample/>

![Password validation error](/images/regex_password_validation_error.png)

![File validation error only when uploaded](/images/regex_file_validation_error.png)

## Repeatable Items

A repeatable config item copies a YAML array entry or YAML document for as many values as are provided. Any number of values can be added to a repeatable item to generate additional copies.

To make an item repeatable, set `repeatable` to true:

```yaml
    - name: ports_group
      items:
      - name: serviceport
        title: Service Port
        type: text
        repeatable: true
```

Repeatable items do not use the `default` or `value` fields, but instead a `valuesByGroup` field.
`valuesByGroup` must have an entry for the parent Config Group name, with all of the default `key:value` pairs nested in the group. At least one default entry is required for the repeatable item:

```yaml
    valuesByGroup:
      ports_group:
        port-default-1: "80"
```

### Limitations

* Repeatable items work only for text, textarea, and file types.
* Repeatable item names must only consist of lower case alphanumeric characters.
* Repeatable items are only supported for Kubernetes manifests, not Helm charts.

### Template Targets

Repeatable items require that you provide at least one `template`. The `template` defines a YAML target in the manifest to duplicate for each repeatable item.

Required fields for a template target are `apiVersion`, `kind`, and `name`.

`namespace` is an optional template target field to match a YAML document's `metadata.namespace` property when the same filename is used across multiple namespaces.

The entire YAML node at the target is duplicated, including nested fields.

The `yamlPath` field of the `template` must denote index position for arrays using square brackets.  For example, `spec.ports[0]` selects the first port entry for duplication. All duplicate YAML is appended to the final array in the `yamlPath`.

`yamlPath` must end with an array.

**Example:**

```yaml
    templates:
    - apiVersion: v1
      kind: Service
      name: my-service
      namespace: my-app
      yamlPath: 'spec.ports[0]'
```

If the `yamlPath` field is not present, the entire YAML document matching the `template` is replaced with a copy for each of the repeatable item entries. The `metadata.name` field of the new document reflects the repeatable item `key`.

### Templating

The repeat items are called with the delimeters `repl[[ .itemName ]]` or `[[repl .itemName ]]`. These delimiters can be placed anywhere inside of the `yamlPath` target node:

```yaml
    - port: repl{{ ConfigOption "[[repl .serviceport ]]" | ParseInt }}
      name: '[[repl .serviceport ]]'
```
This repeatable templating is not compatible with sprig templating functions. It is designed for inserting repeatable `keys` into the manifest. Repeatable templating can be placed inside of Replicated config templating.

### Ordering

Repeatable templates are processed before config template rendering.

Repeatable items are processed in order of the template targets in the Config Spec file. Effectively, this ordering is from the top of the Config Spec, by Config Group, by Config Item, and then by template target.

```yaml
    - name: ports_group
      items:
      - name: serviceport
        title: Service Port
        type: text
        repeatable: true
        templates:
        - apiVersion: v1 #processed first
          kind: Service
          name: my-service
          namespace: my-app
          yamlPath: 'spec.ports[0]'
        - apiVersion: v1 #processed second
          kind: Service
          name: my-service
          namespace: my-app
        {other item properties ...}
      - name: other_ports
        title: Other Service Port
        type: text
        repeatable: true
        templates:
        - apiVersion: v1 #processed third
          kind: Service
          name: my-other-service
          namespace: my-app
        {other item properties ...}
    - name: deployments
      items:
      - name: deployment-name
        title: Deployment Names
        type: text
        repeatable: true
        templates:
        - apiVersion: apps/v1 #processed fourth
          kind: Deployment
          name: my-deployment
          namespace: my-app
        {other item properties ...}
```

## Repeatable Examples

In these examples, the default service port of "80" is included with the release. Port 443 is added as an additional port on the Admin Console configuration page, which is stored in the ConfigValues file.

### Repeatable Item Example for a yamlPath

**Config custom resource manifest file:**

```yaml
    - name: ports_group
      items:
      - name: serviceport
        title: Service Port
        type: text
        repeatable: true
        templates:
        - apiVersion: v1
          kind: Service
          name: my-service
          namespace: my-app
          yamlPath: spec.ports[0]
        valuesByGroup:
          ports_group:
            port-default-1: "80"
```

**Config values:**
```yaml
apiVersion: kots.io/v1beta1
kind: ConfigValues
metadata:
  name: example_app
spec:
  values:
    port-default-1:
      repeatableItem: serviceport
      value: "80"
    serviceport-8jdn2bgd:
      repeatableItem: serviceport
      value: "443"
```

**Template manifest:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: repl{{ ConfigOption "[[repl .serviceport ]]" | ParseInt }}
    name: '[[repl .serviceport ]]'
  selector:
    app: repeat_example
    component: my-deployment
```

**After repeatable config processing:**

**Note**: This phase is internal to configuration rendering for KOTS. This example is only provided to further explain the templating process.*

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: repl{{ ConfigOption "port-default-1" | ParseInt }}
    name: 'port-default-1'
  - port: repl{{ ConfigOption "serviceport-8jdn2bgd" | ParseInt }}
    name: 'serviceport-8jdn2bgd'
  selector:
    app: repeat_example
    component: my-deployment
```

**Resulting manifest:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: 80
    name: port-default-1
  - port: 443
    name: serviceport-8jdn2bgd
  selector:
    app: repeat_example
    component: my-deployment
```

### Repeatable Item Example for an Entire Document
**Config spec:**
```yaml
    - name: ports_group
      items:
      - name: serviceport
        title: Service Port
        type: text
        repeatable: true
        templates:
        - apiVersion: v1
          kind: Service
          name: my-service
          namespace: my-app
        valuesByGroup:
          ports_group:
            port-default-1: "80"
```

**Config values:**
```yaml
apiVersion: kots.io/v1beta1
kind: ConfigValues
metadata:
  name: example_app
spec:
  values:
    port-default-1:
      repeatableItem: serviceport
      value: "80"
    serviceport-8jdn2bgd:
      repeatableItem: serviceport
      value: "443"
```

**Template manifest:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: repl{{ ConfigOption "[[repl .serviceport ]]" | ParseInt }}
  selector:
    app: repeat_example
    component: repl[[ .serviceport ]]
```

**After repeatable config processing:**

**Note**: This phase is internal to configuration rendering for KOTS. This example is only provided to further explain the templating process.*

```yaml
apiVersion: v1
kind: Service
metadata:
  name: port-default-1
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: repl{{ ConfigOption "port-default-1" | ParseInt }}
  selector:
    app: repeat_example
    component: port-default-1
---
apiVersion: v1
kind: Service
metadata:
  name: serviceport-8jdn2bgd
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: repl{{ ConfigOption "serviceport-8jdn2bgd" | ParseInt }}
  selector:
    app: repeat_example
    component: serviceport-8jdn2bgd
```

**Resulting manifest:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: port-default-1
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: 80
  selector:
    app: repeat_example
    component: port-default-1
---
apiVersion: v1
kind: Service
metadata:
  name: serviceport-8jdn2bgd
  namespace: my-app
spec:
  type: NodePort
  ports:
  - port: 443
  selector:
    app: repeat_example
    component: serviceport-8jdn2bgd
```


---


import VersionLimitation from "../partials/helm/_helm-version-limitation.mdx"
import HelmBuilderRequirements from "../partials/helm/_helm-builder-requirements.mdx"
import Chart from "../partials/helm/_helm-cr-chart.mdx"
import ChartName from "../partials/helm/_helm-cr-chart-name.mdx"
import ChartVersion from "../partials/helm/_helm-cr-chart-version.mdx"
import ChartReleaseName from "../partials/helm/_helm-cr-chart-release-name.mdx"
import HelmUpgradeFlags from "../partials/helm/_helm-cr-upgrade-flags.mdx"
import Values from "../partials/helm/_helm-cr-values.mdx"
import Weight from "../partials/helm/_helm-cr-weight.mdx"
import Exclude from "../partials/helm/_helm-cr-exclude.mdx"
import OptionalValues from "../partials/helm/_helm-cr-optional-values.mdx"
import OptionalValuesWhen from "../partials/helm/_helm-cr-optional-values-when.mdx"
import OptionalValuesRecursiveMerge from "../partials/helm/_helm-cr-optional-values-recursive-merge.mdx"
import Namespace from "../partials/helm/_helm-cr-namespace.mdx"
import BuilderAirgapIntro from "../partials/helm/_helm-cr-builder-airgap-intro.mdx"
import BuilderExample from "../partials/helm/_helm-cr-builder-example.mdx"
import V2Example from "../partials/helm/_v2-native-helm-cr-example.mdx"
import KotsHelmCrDescription from "../partials/helm/_kots-helm-cr-description.mdx"

# HelmChart v2

This topic describes the KOTS HelmChart v2 custom resource.

<KotsHelmCrDescription/>

For more information, see [About Distributing Helm Charts with KOTS](/vendor/helm-native-about).

## Example

The following is an example manifest file for the HelmChart v2 custom resource:

<V2Example/>

## chart

<Chart/>

### chart.name

<ChartName/>

### chart.chartVersion

<ChartVersion/>

## releaseName

<ChartReleaseName/>

## weight

<Weight/>

## helmUpgradeFlags

<HelmUpgradeFlags/>

## exclude

<Exclude/>

## values

<Values/>

For more information about using `values`, see [Setting Helm Chart Values with KOTS](/vendor/helm-optional-value-keys).

## optionalValues

<OptionalValues/>

For more information about using `optionalValues`, see [Setting Helm Chart Values with KOTS](/vendor/helm-optional-value-keys).

### optionalValues.when

<OptionalValuesWhen/>

### optionalValues.recursiveMerge

<OptionalValuesRecursiveMerge/>

**Default**: False

For an example of recursive and non-recursive merging, see [About Recursive Merge](/vendor/helm-optional-value-keys#recursive-merge).

## namespace

<Namespace/>

## builder

The `builder` key is used to provide Helm values that are used during various stages of processing the Helm chart.

The `builder` key is required for the following use cases:

* To create an `.airgap` bundle for installations into air gap environments.

  <BuilderAirgapIntro/>

  For more information, see [Packaging Air Gap Bundles for Helm Charts](/vendor/helm-packaging-airgap-bundles).

* To support online installations that use a local private registry, the `builder` field renders the Helm chart with all of the necessary images so that KOTS knows where to pull the images.

  You cannot prevent customers from configuring a local private registry in the Admin Console. If you think any of your customers will use a local private registry, you should use the `builder` key. For more information, see [Configuring Local Image Registries](/enterprise/image-registry-settings).

<HelmBuilderRequirements/>

* Use the same `builder` configuration to support the use of local registries in both online and air gap installations. If you already configured the `builder` key to support air gap installations, then no additional configuration is required.

**Example:**

<BuilderExample/>


---


import UseCases from "../partials/template-functions/_use-cases.mdx"
import StaticContext from "../partials/template-functions/_static-context.mdx"
import ConfigContext from "../partials/template-functions/_config-context.mdx"
import LicenseContext from "../partials/template-functions/_license-context.mdx"
import KurlContext from "../partials/template-functions/_kurl-context.mdx"
import IdentityContext from "../partials/template-functions/_identity-context.mdx"
import KurlAvailability from "../partials/kurl/_kurl-availability.mdx"

# About Template Functions

This topic describes Replicated KOTS template functions, including information about use cases, template function contexts, syntax.

## Overview

For Kubernetes manifest files for applications deployed by Replicated KOTS, Replicated provides a set of custom template functions based on the Go text/template library. 

<UseCases/>

All functionality of the Go templating language, including if statements, loops, and variables, is supported with KOTS template functions. For more information about the Go library, see [text/template](https://golang.org/pkg/text/template/) in the Go documentation.

### Supported File Types

You can use KOTS template functions in Kubernetes manifest files for applications deployed by KOTS, such as:
* Custom resources in the `kots.io` API group like Application, Config, or HelmChart
* Custom resources in other API groups like Preflight, SupportBundle, or Backup
* Kubernetes objects like Deployments, Services, Secrets, or ConfigMaps
* Kubernetes Operators

### Limitations

* Not all fields in the Config and Application custom resources support templating. For more information, see [Application](/reference/custom-resource-application) and [Item Properties](/reference/custom-resource-config#item-properties) in _Config_.

* Templating is not supported in the [Embedded Cluster Config](/reference/embedded-config) resource.

* KOTS template functions are not directly supported in Helm charts. For more information, see [Helm Charts](#helm-charts) below.

### Helm Charts

KOTS template functions are _not_ directly supported in Helm charts. However, the HelmChart custom resource provides a way to map values rendered by KOTS template functions to Helm chart values. This allows you to use KOTS template functions with Helm charts without making changes to those Helm charts.

For information about how to map values from the HelmChart custom resource to Helm chart `values.yaml` files, see [Setting Helm Chart Values with KOTS](/vendor/helm-optional-value-keys).

### Template Function Rendering

During application installation and upgrade, KOTS templates all Kubernetes manifest files in a release (except for the Config custom resource) at the same time during a single process.

For the [Config](/reference/custom-resource-config) custom resource, KOTS templates each item separately so that config items can be used in templates for other items. For examples of this, see [Using Conditional Statements in Configuration Fields](/vendor/config-screen-conditional) and [Template Function Examples](/reference/template-functions-examples).

## Syntax {#syntax}

The KOTS template function syntax supports the following functionally equivalent delimiters:
* [`repl{{ ... }}`](#syntax-integer)
* [`{{repl ... }}`](#syntax-string)

### Syntax Requirements

KOTS template function syntax has the following requirements:
* In both the `repl{{ ... }}` and `{{repl ... }}` syntaxes, there must be no whitespace between `repl` and the `{{` delimiter.
* The manifests where KOTS template functions are used must be valid YAML. This is because the YAML manifests are linted before KOTS template functions are rendered.

### `repl{{ ... }}` {#syntax-integer}

This syntax is recommended for most use cases.

Any quotation marks wrapped around this syntax are stripped during rendering. If you need the rendered value to be quoted, you can pipe into quote (`| quote`) or use the [`{{repl ... }}`](#syntax-string) syntax instead.

#### Integer Example

```yaml
http:
  port: repl{{ ConfigOption "load_balancer_port" }}
```
```yaml
http:
  port: 8888
```  

#### Example with `| quote`

```yaml
customTag: repl{{ ConfigOption "tag" | quote }}
```
```yaml
customTag: 'key: value'
```

#### If-Else Example

```yaml
http:
  port: repl{{ if ConfigOptionEquals "ingress_type" "load_balancer" }}repl{{ ConfigOption "load_balancer_port" }}repl{{ else }}8081repl{{ end }}
```
```yaml
http:
  port: 8081
```

For more examples, see [Template Function Examples](/reference/template-functions-examples).

### `{{repl ... }}` {#syntax-string}

This syntax can be useful when having the delimiters outside the template function improves readability of the YAML, such as in multi-line statements or if-else statements.

To use this syntax at the beginning of a value in YAML, it _must_ be wrapped in quotes because you cannot start a YAML value with the `{` character and manifests consumed by KOTS must be valid YAML. When this syntax is wrapped in quotes, the rendered value is also wrapped in quotes.

#### Example With Quotes

The following example is wrapped in quotes because it is used at the beginning of a statement in YAML:

```yaml
customTag: '{{repl ConfigOption "tag" }}'
```
```yaml
customTag: 'key: value'
```

#### If-Else Example
```yaml
my-service:
  type: '{{repl if ConfigOptionEquals "ingress_type" "load_balancer" }}LoadBalancer{{repl else }}ClusterIP{{repl end }}'
```
```yaml
my-service:
  type: 'LoadBalancer'
```

For more examples, see [Template Function Examples](/reference/template-functions-examples).

## Contexts {#contexts}

KOTS template functions are grouped into different contexts, depending on the phase of the application lifecycle when the function is available and the context of the data that is provided.

### Static Context

<StaticContext/>

For a list of all KOTS template functions available in the static context, see [Static Context](template-functions-static-context).

### Config Context

<ConfigContext/>

For a list of all KOTS template functions available in the config context, see [Config Context](template-functions-config-context).

### License Context

<LicenseContext/>

For a list of all KOTS template functions available in the license context, see [License Context](template-functions-license-context).

### kURL Context

<KurlAvailability/>

<KurlContext/>

For a list of all KOTS template functions available in the kURL context, see [kURL Context](template-functions-kurl-context).

### Identity Context

<IdentityContext/>

For a list of all KOTS template functions available in the identity context, see [Identity Context](template-functions-identity-context).


---


import IntegerComparison from "../partials/template-functions/_integer-comparison.mdx"
import StringComparison from "../partials/template-functions/_string-comparison.mdx"
import NeComparison from "../partials/template-functions/_ne-comparison.mdx"
import GoSprig from "../partials/template-functions/_go-sprig.mdx"
import UseCases from "../partials/template-functions/_use-cases.mdx"

# Template Function Examples

This topic provides examples of how to use Replicated KOTS template functions in various common use cases. For more information about working with KOTS template functions, including the supported syntax and the types of files where KOTS template functions can be used, see [About Template Functions](template-functions-about).

## Overview

<GoSprig/>

<UseCases/>

For examples demonstrating these use cases and more, see the sections below.

## Comparison Examples

This section includes examples of how to use KOTS template functions to compare different types of data.

### Boolean Comparison

Boolean values can be used in comparisons to evaluate if a given statement is true or false. Because many KOTS template functions return string values, comparing boolean values often requires using the KOTS [ParseBool](/reference/template-functions-static-context#parsebool) template function to return the boolean represented by the string.

One common use case for working with boolean values is to check that a given field is present in the customer's license. For example, you might need to show a configuration option on the KOTS Admin Console **Config** page only when the customer's license has a certain entitlement.

The following example creates a conditional statement in the KOTS Config custom resource that evaluates to true when a specified license field is present in the customer's license _and_ the customer enables a specified configuration option on the Admin Console **Config** page. 

```yaml
# KOTS Config custom resource
apiVersion: kots.io/v1beta1
kind: Config
metadata:
  name: config-sample
spec:
  groups:  
  - name: example_group
    title: Example Config
    items:
    - name: radio_example
      title: Select One
      type: radio
      items:
      - name: option_one
        title: Option One
      - name: option_two
        title: Option Two  
    - name: conditional_item
      title: Conditional Item
      type: text
      # Display this item only when the customer enables the option_one config field *and*
      # has the feature-1 entitlement in their license
      when: repl{{ and (LicenseFieldValue "feature-1" | ParseBool) (ConfigOptionEquals "radio_example" "option_one")}}  
```

This example uses the following KOTS template functions:
* [LicenseFieldValue](/reference/template-functions-license-context#licensefieldvalue) to return the string value of a boolean type license field named `feature-1`
   :::note
   The LicenseFieldValue template function always returns a string, regardless of the license field type.
   :::
* [ParseBool](/reference/template-functions-static-context#parsebool) to convert the string returned by the LicenseFieldValue template function to a boolean
* [ConfigOptionEquals](/reference/template-functions-config-context#configoptionequals) to return a boolean that evaluates to true if the configuration option value is equal to the supplied value

### Integer Comparison

Integer values can be compared using operators such as greater than, less than, equal to, and so on. Because many KOTS template functions return string values, working with integer values often requires using another function to return the integer represented by the string, such as:
* KOTS [ParseInt](/reference/template-functions-static-context#parseint), which returns the integer value represented by the string with the option to provide a `base` other than 10
* Sprig [atoi](https://masterminds.github.io/sprig/conversion.html), which is equivalent to ParseInt(s, 10, 0), converted to type integer

A common use case for comparing integer values with KOTS template functions is to display different configuration options on the KOTS Admin Console **Config** page depending on integer values from the customer's license. For example, licenses might include an entitlement that defines the number of seats the customer is entitled to. In this case, it can be useful to conditionally display or hide certain fields on the **Config** page depending on the customer's team size.

<IntegerComparison/>

### String Comparison

A common use case for string comparison is to compare the rendered value of a KOTS template function against a string to conditionally show or hide fields on the KOTS Admin Console **Config** page depending on details about the customer's environment. For example, a string comparison can be used to check the Kubernetes distribution of the cluster where an application is deployed.

<StringComparison/>

### Not Equal To Comparison

It can be useful to compare the rendered value of a KOTS template function against another value to check if the two values are different. For example, you can conditionally show fields on the KOTS Admin Console **Config** page only when the Kubernetes distribution of the cluster where the application is deployed is _not_ [Replicated embedded cluster](/vendor/embedded-overview).

<NeComparison/>

### Logical AND Comparison

Logical comparisons such as AND, OR, and NOT can be used with KOTS template functions. A common use case for logical AND comparisons is to construct more complex conditional statements where it is necessary that two different conditions are both true.

The following example shows how to use an `and` operator that evaluates to true when two different configuration options on the Admin Console **Config** page are both enabled. This example uses the KOTS [ConfigOptionEquals](/reference/template-functions-config-context#configoptionequals) template function to return a boolean that evaluates to true if the configuration option value is equal to the supplied value.

```yaml
# KOTS Config custom resource
apiVersion: kots.io/v1beta1
kind: Config
metadata:
  name: config-sample
spec:
  groups:  
  - name: example_group
    title: Example Config
    items:
    - name: radio_example
      title: Select One Example
      type: radio
      items:
      - name: option_one
        title: Option One
      - name: option_two
        title: Option Two
    - name: boolean_example
      title: Boolean Example
      type: bool
      default: "0"      
    - name: conditional_item
      title: Conditional Item
      type: text
      # Display this item only when *both* specified config options are enabled
      when: repl{{ and (ConfigOptionEquals "radio_example" "option_one") (ConfigOptionEquals "boolean_example" "1")}}
```

As shown below, when both `Option One` and `Boolean Example` are selected, the conditional statement evaluates to true and the `Conditional Item` field is displayed:

<img alt="Conditional item displayed" src="/images/conditional-item-true.png" width="550px"/>

[View a larger version of this image](/images/conditional-item-true.png)

Alternatively, if either `Option One` or `Boolean Example` is not selected, then the conditional statement evaluates to false and the `Conditional Item` field is not displayed:  

<img alt="Option two selected" src="/images/conditional-item-false-option-two.png" width="550px"/>

[View a larger version of this image](/images/conditional-item-false-option-two.png)

<img alt="Boolean field deselected" src="/images/conditional-item-false-boolean.png" width="550px"/>

[View a larger version of this image](/images/conditional-item-false-boolean.png)

## Conditional Statement Examples

This section includes examples of using KOTS template functions to construct conditional statements. Conditional statements can be used with KOTS template functions to render different values depending on a given condition. 

### If-Else Statements

A common use case for if-else statements with KOTS template functions is to set values for resources or objects deployed by your application, such as custom annotations or service types, based on user-specific data.

This section includes examples of both single line and multi-line if-else statements. Using multi-line formatting can be useful to improve the readability of YAML files when longer or more complex if-else statements are needed.

Multi-line if-else statements can be constructed using YAML block scalars and block chomping characters to ensure the rendered result is valid YAML. A _folded_ block scalar style is denoted using the greater than (`>`) character. With the folded style, single line breaks in the string are treated as a space. Additionally, the block chomping minus (`-`) character is used to remove all the line breaks at the end of a string. For more information about working with these characters, see [Block Style Productions](https://yaml.org/spec/1.2.2/#chapter-8-block-style-productions) in the YAML documentation.

:::note
For Helm-based applications that need to use more complex or nested if-else statements, you can alternatively use templating within your Helm chart `templates` rather than in the KOTS HelmChart custom resource. For more information, see [If/Else](https://helm.sh/docs/chart_template_guide/control_structures/#ifelse) in the Helm documentation.
:::

#### Single Line

The following example shows if-else statements used in the KOTS HelmChart custom resource `values` field to render different values depending on if the user selects a load balancer or an ingress controller as the ingress type for the application. This example uses the KOTS [ConfigOptionEquals](/reference/template-functions-config-context#configoptionequals) template function to return a boolean that evaluates to true if the configuration option value is equal to the supplied value.

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: my-app
spec:
  chart:
    name: my-app
    chartVersion: 0.23.0
  values:
    services:
      my-service:
        enabled: true
        appName: ["my-app"]
        # Render the service type based on the user's selection
        # '{{repl ...}}' syntax is used for `type` to improve readability of the if-else statement and render a string
        type: '{{repl if ConfigOptionEquals "ingress_type" "load_balancer" }}LoadBalancer{{repl else }}ClusterIP{{repl end }}'
        ports:
          http:
            enabled: true
            # Render the HTTP port for the service depending on the user's selection
            # repl{{ ... }} syntax is used for `port` to render an integer value
            port: repl{{ if ConfigOptionEquals "ingress_type" "load_balancer" }}repl{{ ConfigOption "load_balancer_port" }}repl{{ else }}8081repl{{ end }}
            protocol: HTTP
            targetPort: 8081
```

#### Multi-Line in KOTS HelmChart Values

The following example uses a multi-line if-else statement in the KOTS HelmChart custom resource to render the path to the Replicated SDK image depending on if the user pushed images to a local private registry.

This example uses the following KOTS template functions:
* [HasLocalRegistry](/reference/template-functions-config-context#haslocalregistry) to return true if the environment is configured to rewrite images to a local registry
* [LocalRegistryHost](/reference/template-functions-config-context#localregistryhost) to return the local registry host configured by the user
* [LocalRegistryNamespace](/reference/template-functions-config-context#localregistrynamespace) to return the local registry namespace configured by the user

:::note
This example uses the `{{repl ...}}` syntax rather than the `repl{{ ... }}` syntax to improve readability in the YAML file. However, both syntaxes are supported for this use case. For more information, see [Syntax](/reference/template-functions-about#syntax) in _About Template Functions_.
:::

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  values:
    images:
      replicated-sdk: >-
        {{repl if HasLocalRegistry -}}
          {{repl LocalRegistryHost }}/{{repl LocalRegistryNamespace }}/replicated-sdk:1.0.0-beta.29
        {{repl else -}}
          docker.io/replicated/replicated-sdk:1.0.0-beta.29
        {{repl end}}
```

Given the example above, if the user is _not_ using a local registry, then the `replicated-sdk` value in the Helm chart is set to the location of the image on the default docker registry, as shown below:

```yaml
# Helm chart values file

images:
  replicated-sdk: 'docker.io/replicated/replicated-sdk:1.0.0-beta.29'
```

#### Multi-Line in Secret Object

The following example uses multi-line if-else statements in a Secret object deployed by KOTS to conditionally set the database hostname, port, username, and password depending on if the customer uses the database embedded with the application or brings their own external database.

This example uses the following KOTS template functions:
* [ConfigOptionEquals](/reference/template-functions-config-context#configoptionequals) to return a boolean that evaluates to true if the configuration option value is equal to the supplied value
* [ConfigOption](/reference/template-functions-config-context#configoption) to return the user-supplied value for the specified configuration option 
* [Base64Encode](/reference/template-functions-static-context#base64encode) to encode the string with base64

:::note
This example uses the `{{repl ...}}` syntax rather than the `repl{{ ... }}` syntax to improve readability in the YAML file. However, both syntaxes are supported for this use case. For more information, see [Syntax](/reference/template-functions-about#syntax) in _About Template Functions_.
:::

```yaml
# Postgres Secret
apiVersion: v1
kind: Secret
metadata:
  name: postgres
data:
  # Render the value for the database hostname depending on if an embedded or
  # external db is used.
  # Also, base64 encode the rendered value.
  DB_HOST: >-
    {{repl if ConfigOptionEquals "postgres_type" "embedded_postgres" -}}
      {{repl Base64Encode "postgres" }}
    {{repl else -}}
      {{repl ConfigOption "external_postgres_host" | Base64Encode }}
    {{repl end}}
  DB_PORT: >-
    {{repl if ConfigOptionEquals "postgres_type" "embedded_postgres" -}}
      {{repl Base64Encode "5432" }}
    {{repl else -}}
      {{repl ConfigOption "external_postgres_port" | Base64Encode }}
    {{repl end}}
  DB_USER: >-
    {{repl if ConfigOptionEquals "postgres_type" "embedded_postgres" -}}
      {{repl Base64Encode "postgres" }}
    {{repl else -}}
      {{repl ConfigOption "external_postgres_user" | Base64Encode }}
    {{repl end}}
  DB_PASSWORD: >-
    {{repl if ConfigOptionEquals "postgres_type" "embedded_postgres" -}}
      {{repl ConfigOption "embedded_postgres_password" | Base64Encode }}
    {{repl else -}}
      {{repl ConfigOption "external_postgres_password" | Base64Encode }}
    {{repl end}}
```

### Ternary Operators

Ternary operators are useful for templating strings where certain values within the string must be rendered differently depending on a given condition. Compared to if-else statements, ternary operators are useful when a small portion of a string needs to be conditionally rendered, as opposed to rendering different values based on a conditional statement. For example, a common use case for ternary operators is to template the path to an image repository based on user-supplied values.

The following example uses ternary operators to render the registry and repository for a private nginx image depending on if a local image regsitry is used. This example uses the following KOTS template functions:
* [HasLocalRegistry](/reference/template-functions-config-context#haslocalregistry) to return true if the environment is configured to rewrite images to a local registry
* [LocalRegistryHost](/reference/template-functions-config-context#localregistryhost) to return the local registry host configured by the user
* [LocalRegistryNamespace](/reference/template-functions-config-context#localregistrynamespace) to return the local registry namespace configured by the user

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  values:
    image:
      # If a local registry is configured, use the local registry host.
      # Otherwise, use proxy.replicated.com
      registry: repl{{ HasLocalRegistry | ternary LocalRegistryHost "proxy.replicated.com" }}
      # If a local registry is configured, use the local registry's namespace.
      # Otherwise, use proxy/my-app/quay.io/my-org
      repository: repl{{ HasLocalRegistry | ternary LocalRegistryNamespace "proxy/my-app/quay.io/my-org" }}/nginx
      tag: v1.0.1
```

## Formatting Examples

This section includes examples of how to format the rendered output of KOTS template functions.

In addition to the examples in this section, KOTS template functions in the Static context include several options for formatting values, such as converting strings to upper or lower case and trimming leading and trailing space characters. For more information, see [Static Context](/reference/template-functions-static-context). 

### Indentation

When using template functions within nested YAML, it is important that the rendered template functions are indented correctly so that the YAML renders. A common use case for adding indentation to KOTS template functions is when templating annotations in the metadata of resources or objects deployed by your application based on user-supplied values. 

The [nindent](https://masterminds.github.io/sprig/strings.html) function can be used to prepend a new line to the beginning of the string and indent the string by a specified number of spaces.

#### Indent Templated Helm Chart Values

The following example shows templating a Helm chart value that sets annotations for an Ingress object. This example uses the KOTS [ConfigOption](/reference/template-functions-config-context#configoption) template function to return user-supplied annotations from the Admin Console **Config** page. It also uses [nindent](https://masterminds.github.io/sprig/strings.html) to indent the rendered value ten spaces.

```yaml
# KOTS HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: myapp
spec:
  values:
    services:
      myservice:
        annotations: repl{{ ConfigOption "additional_annotations" | nindent 10 }}
```

#### Indent Templated Annotations in Manifest Files

The following example shows templating annotations for an Ingress object. This example uses the KOTS [ConfigOption](/reference/template-functions-config-context#configoption) template function to return user-supplied annotations from the Admin Console **Config** page. It also uses [nindent](https://masterminds.github.io/sprig/strings.html) to indent the rendered value four spaces.

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    kots.io/placeholder: |-
      repl{{ ConfigOption "ingress_annotations" | nindent 4 }}
```

### Render Quoted Values

To wrap a rendered value in quotes, you can pipe the result from KOTS template functions with the `repl{{ ... }}` syntax into quotes using `| quote`. Or, you can use the `'{{repl ... }}'` syntax instead.

One use case for quoted values in YAML is when indicator characters are included in values. In YAML, indicator characters (`-`, `?`, `:`) have special semantics and must be escaped if used in values. For more information, see [Indicator Charactors](https://yaml.org/spec/1.2.2/#53-indicator-characters) in the YAML documentation.

#### Example with `'{{repl ... }}'` Syntax

```yaml
customTag: '{{repl ConfigOption "tag" }}'
```
#### Example with `| quote`

```yaml
customTag: repl{{ ConfigOption "tag" | quote }}
```

The result for both examples is:

```yaml
customTag: 'key: value'
```

## Variables Example

This section includes an example of using variables with KOTS template functions. For more information, see [Variables](https://pkg.go.dev/text/template#hdr-Variables) in the Go documentation.

### Using Variables to Generate TLS Certificates in JSON

You can use the Sprig [genCA](https://masterminds.github.io/sprig/crypto.html) and [genSignedCert](https://masterminds.github.io/sprig/crypto.html) functions with KOTS template functions to generate certificate authorities (CAs) and signed certificates in JSON. One use case for this is to generate default CAs, certificates, and keys that users can override with their own values on the Admin Console **Config** page.

The Sprig [genCA](https://masterminds.github.io/sprig/crypto.html) and [genSignedCert](https://masterminds.github.io/sprig/crypto.html) functions require the subject's common name and the certificate's validity duration in days. The `genSignedCert` function also requires the CA that will sign the certificate. You can use variables and KOTS template functions to provide the necessary parameters when calling these functions.

The following example shows how to use variables and KOTS template functions in the `default` property of a [`hidden`](/reference/custom-resource-config#hidden) item to pass parameters to the `genCA` and `genSignedCert` functions and generate a CA, certificate, and key. This example uses a `hidden` item (which is an item that is not displayed on the **Config** page) to generate the certificate chain because variables used in the KOTS Config custom resource can only be accessed from the same item where they were declared. For this reason, `hidden` items can be useful for evaluating complex templates.

This example uses the following:
* KOTS [ConfigOption](/reference/template-functions-config-context#configoption) template function to render the user-supplied value for the ingress hostname. This is passed as a parameter to the [genCA](https://masterminds.github.io/sprig/crypto.html) and [genSignedCert](https://masterminds.github.io/sprig/crypto.html) functions 
* Sprig [genCA](https://masterminds.github.io/sprig/crypto.html) and [genSignedCert](https://masterminds.github.io/sprig/crypto.html) functions to generate a CA and a certificate signed by the CA
* Sprig [dict](https://masterminds.github.io/sprig/dicts.html), [set](https://masterminds.github.io/sprig/dicts.html), and [dig](https://masterminds.github.io/sprig/dicts.html) dictionary functions to create a dictionary with entries for both the CA and the certificate, then traverse the dictionary to return the values of the CA, certificate, and key.
* [toJson](https://masterminds.github.io/sprig/defaults.html) and [fromJson](https://masterminds.github.io/sprig/defaults.html) Sprig functions to encode the CA and certificate into a JSON string, then decode the JSON for the purpose of displaying the values on the **Config** page as defaults

:::important
Default values are treated as ephemeral. The following certificate chain is recalculated each time the application configuration is modified. Before using this example with your application, be sure that your application can handle updating these parameters dynamically.
:::

```yaml
apiVersion: kots.io/v1beta1
kind: Config
metadata:
  name: config-sample
spec:
  groups:
    - name: example_settings
      title: My Example Config
      items:
        - name: ingress_hostname
          title: Ingress Hostname
          help_text: Enter a DNS hostname to use as the cert's CN.
          type: text
        - name: tls_json
          title: TLS JSON
          type: textarea
          hidden: true
          default: |-
            repl{{ $ca := genCA (ConfigOption "ingress_hostname") 365 }}
            repl{{ $tls := dict "ca" $ca }}
            repl{{ $cert := genSignedCert (ConfigOption "ingress_hostname") (list ) (list (ConfigOption "ingress_hostname")) 365 $ca }}
            repl{{ $_ := set $tls "cert" $cert }}
            repl{{ toJson $tls }}
        - name: tls_ca
          title: Signing Authority
          type: textarea
          default: repl{{ fromJson (ConfigOption "tls_json") | dig "ca" "Cert" "" }}
        - name: tls_cert
          title: TLS Cert
          type: textarea
          default: repl{{ fromJson (ConfigOption "tls_json") | dig "cert" "Cert" "" }}
        - name: tls_key
          title: TLS Key
          type: textarea
          default: repl{{ fromJson (ConfigOption "tls_json") | dig "cert" "Key" "" }}
```

The following image shows how the default values for the CA, certificate, and key are displayed on the **Config** page:

<img alt="Default values for CA, certificate, and key on the Config page" src="/images/certificate-chain-default-values.png" width="550px"/>

[View a larger version of this image](/images/certificate-chain-default-values.png)

## Additional Examples

The following topics include additional examples of using KOTS template functions in Kubernetes manifests deployed by KOTS or in KOTS custom resources: 

* [Add Status Informers](/vendor/admin-console-display-app-status#add-status-informers) in _Adding Resource Status Informers_
* [Conditionally Including or Excluding Resources](/vendor/packaging-include-resources)
* [Example: Including Optional Helm Charts](/vendor/helm-optional-charts)
* [Example: Adding Database Configuration Options](/vendor/tutorial-adding-db-config)
* [Templating Annotations](/vendor/resources-annotations-templating)
* [Tutorial: Set Helm Chart Values with KOTS](/vendor/tutorial-config-setup)

---


import ConfigContext from "../partials/template-functions/_config-context.mdx"

# Config Context

<ConfigContext/>

## ConfigOption

```go
func ConfigOption(optionName string) string
```

Returns the value of the specified option from the KOTS Config custom resource as a string.

For the `file` config option type, `ConfigOption` returns the base64 encoded file. To return the decoded contents of a file, use [ConfigOptionData](#configoptiondata) instead.

```yaml
'{{repl ConfigOption "hostname" }}'
```

#### Example

The following KOTS [HelmChart](/reference/custom-resource-helmchart-v2) custom resource uses the ConfigOption template function to set the port, node port, and annotations for a LoadBalancer service using the values supplied by the user on the KOTS Admin Console config screen. These values are then mapped to the `values.yaml` file for the associated Helm chart during deployment.

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  values:
    myapp:
      service:
        type: LoadBalancer
        port: repl{{ ConfigOption "myapp_load_balancer_port"}}
        nodePort: repl{{ ConfigOption "myapp_load_balancer_node_port"}}
        annotations: repl{{ ConfigOption `myapp_load_balancer_annotations` | nindent 14 }}
```
For more information, see [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys).

## ConfigOptionData

```go
func ConfigOptionData(optionName string) string
```

For the `file` config option type,  `ConfigOptionData` returns the base64 decoded contents of the file. To return the base64 encoded file, use [ConfigOption](#configoption) instead.

```yaml
'{{repl ConfigOptionData "ssl_key"}}'
```

#### Example

The following KOTS [HelmChart](/reference/custom-resource-helmchart-v2) custom resource uses the ConfigOptionData template function to set the TLS cert and key using the files supplied by the user on the KOTS Admin Console config screen. These values are then mapped to the `values.yaml` file for the associated Helm chart during deployment.

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  values:
    myapp:
      tls:
        enabled: true
        genSelfSignedCert: repl{{ ConfigOptionEquals "myapp_ingress_tls_type" "self_signed" }}
        cert: repl{{ print `|`}}repl{{ ConfigOptionData `tls_certificate_file` | nindent 12 }}
        key: repl{{ print `|`}}repl{{ ConfigOptionData `tls_private_key_file` | nindent 12 }}
```
For more information, see [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys).

## ConfigOptionFilename

```go
func ConfigOptionFilename(optionName string) string
```

`ConfigOptionFilename` returns the filename associated with a `file` config option.
It will return an empty string if used erroneously with other types.

```yaml
'{{repl ConfigOptionFilename "pom_file"}}'
```

#### Example

For example, if you have the following KOTS Config defined:

```yaml
apiVersion: kots.io/v1beta1
kind: Config
metadata:
  name: my-application
spec:
  groups:
    - name: java_settings
      title: Java Settings
      description: Configures the Java Server build parameters
      items:
        - name: pom_file
          type: file
          required: true
```

The following example shows how to use `ConfigOptionFilename` in a Pod Spec to mount a file:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo-pod
spec:
  containers:
    - name: some-java-app
      image: busybox
      command: ["bash"]
      args:
      - "-C"
      - "cat /config/{{repl ConfigOptionFilename pom_file}}"
      volumeMounts:
      - name: config
        mountPath: "/config"
        readOnly: true
  volumes:
    - name: config
      configMap:
        name: demo-configmap
        items:
        - key: data_key_one
          path: repl{{ ConfigOptionFilename pom_file }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-configmap
data:
  data_key_one: repl{{ ConfigOptionData pom_file }}
```

## ConfigOptionEquals

```go
func ConfigOptionEquals(optionName string, expectedValue string) bool
```

Returns true if the configuration option value is equal to the supplied value.

```yaml
'{{repl ConfigOptionEquals "http_enabled" "1" }}'
```

#### Example

The following KOTS [HelmChart](/reference/custom-resource-helmchart-v2) custom resource uses the ConfigOptionEquals template function to set the `postgres.enabled` value depending on if the user selected the `embedded_postgres` option on the KOTS Admin Console config screen. This value is then mapped to the `values.yaml` file for the associated Helm chart during deployment.

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  values:
    postgresql:
      enabled: repl{{ ConfigOptionEquals `postgres_type` `embedded_postgres`}}
```
For more information, see [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys).

## ConfigOptionNotEquals

```go
func ConfigOptionNotEquals(optionName string, expectedValue string) bool
```

Returns true if the configuration option value is not equal to the supplied value.

```yaml
'{{repl ConfigOptionNotEquals "http_enabled" "1" }}'
```

## LocalRegistryAddress

```go
func LocalRegistryAddress() string
```

Returns the local registry host or host/namespace that's configured.
This will always return everything before the image name and tag.

## LocalRegistryHost

```go
func LocalRegistryHost() string
```

Returns the host of the local registry that the user configured. Alternatively, for air gap installations with Replicated Embedded Cluster or Replicated kURL, LocalRegistryHost returns the host of the built-in registry.

Includes the port if one is specified.

#### Example

The following KOTS [HelmChart](/reference/custom-resource-helmchart-v2) custom resource uses the HasLocalRegistry, LocalRegistryHost, and LocalRegistryNamespace template functions to conditionally rewrite an image registry and repository depending on if a local registry is used. These values are then mapped to the `values.yaml` file for the associated Helm chart during deployment.

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  values:
    myapp:
      image:
        registry: '{{repl HasLocalRegistry | ternary LocalRegistryHost "images.mycompany.com" }}'
        repository: '{{repl HasLocalRegistry | ternary LocalRegistryNamespace "proxy/myapp/quay.io/my-org" }}/nginx'
        tag: v1.0.1
```
For more information, see [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys).

## LocalRegistryNamespace

```go
func LocalRegistryNamespace() string
```

Returns the namespace of the local registry that the user configured. Alternatively, for air gap installations with Embedded Cluster or kURL, LocalRegistryNamespace returns the namespace of the built-in registry.

#### Example

The following KOTS [HelmChart](/reference/custom-resource-helmchart-v2) custom resource uses the HasLocalRegistry, LocalRegistryHost, and LocalRegistryNamespace template functions to conditionally rewrite an image registry and repository depending on if a local registry is used. These values are then mapped to the `values.yaml` file for the associated Helm chart during deployment.

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  values:
    myapp:
      image:
        registry: '{{repl HasLocalRegistry | ternary LocalRegistryHost "images.mycompany.com" }}'
        repository: '{{repl HasLocalRegistry | ternary LocalRegistryNamespace "proxy/myapp/quay.io/my-org" }}/nginx'
        tag: v1.0.1
```
For more information, see [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys).

## LocalImageName

```go
func LocalImageName(remoteImageName string) string
```

Given a `remoteImageName`, rewrite the `remoteImageName` so that it can be pulled to local hosts.

A common use case for the `LocalImageName` function is to ensure that a Kubernetes Operator can determine the names of container images on Pods created at runtime. For more information, see [Referencing Images](/vendor/operator-referencing-images) in the _Packaging a Kubernetes Operator Application_ section.

`LocalImageName` rewrites the `remoteImageName` in one of the following ways, depending on if a private registry is configured and if the image must be proxied:

* If there is a private registry configured in the customer's environment, such as in air gapped environments, rewrite `remoteImageName` to reference the private registry locally. For example, rewrite `elasticsearch:7.6.0` as `registry.somebigbank.com/my-app/elasticsearch:7.6.0`.

* If there is no private registry configured in the customer's environment, but the image must be proxied, rewrite `remoteImageName` so that the image can be pulled through the proxy registry. For example, rewrite `"quay.io/orgname/private-image:v1.2.3"` as `proxy.replicated.com/proxy/app-name/quay.io/orgname/private-image:v1.2.3`.

* If there is no private registry configured in the customer's environment and the image does not need to be proxied, return `remoteImageName` without changes.

For more information about the Replicated proxy registry, see [About the Proxy Registry](/vendor/private-images-about).

## LocalRegistryImagePullSecret

```go
func LocalRegistryImagePullSecret() string
```

Returns the base64 encoded local registry image pull secret value.
This is often needed when an operator is deploying images to a namespace that is not managed by Replicated KOTS.
Image pull secrets must be present in the namespace of the pod.

#### Example

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-image-pull-secret
  namespace: my-namespace
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: '{{repl LocalRegistryImagePullSecret }}'
---
apiVersion: v1
kind: Pod
metadata:
  name: dynamic-pod
  namespace: my-namespace
spec:
  containers:
    - image: '{{repl LocalImageName "registry.replicated.com/my-app/my-image:abcdef" }}'
      name: my-container
  imagePullSecrets:
    - name: my-image-pull-secret
```

## ImagePullSecretName

```go
func ImagePullSecretName() string
```

Returns the name of the image pull secret that can be added to pod specs that use private images.
The secret will be automatically created in all application namespaces.
It will contain authentication information for any private registry used with the application.

#### Example

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  template:
    spec:
      imagePullSecrets:
      - name: repl{{ ImagePullSecretName }}
```

## HasLocalRegistry

```go
func HasLocalRegistry() bool
```

Returns true if the environment is configured to rewrite images to a local registry.
HasLocalRegistry is always true for air gap installations. HasLocalRegistry is true in online installations if the user pushed images to a local registry.

#### Example

The following KOTS [HelmChart](/reference/custom-resource-helmchart-v2) custom resource uses the HasLocalRegistry, LocalRegistryHost, and LocalRegistryNamespace template functions to conditionally rewrite an image registry and repository depending on if a local registry is used. These values are then mapped to the `values.yaml` file for the associated Helm chart during deployment.

```yaml
# KOTS HelmChart custom resource
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  values:
    myapp:
      image:
        registry: '{{repl HasLocalRegistry | ternary LocalRegistryHost "images.mycompany.com" }}'
        repository: '{{repl HasLocalRegistry | ternary LocalRegistryNamespace "proxy/myapp/quay.io/my-org" }}/nginx'
        tag: v1.0.1
```
For more information, see [Setting Helm Values with KOTS](/vendor/helm-optional-value-keys).

---


import LicenseContext from "../partials/template-functions/_license-context.mdx"

# License Context

<LicenseContext/>

## LicenseFieldValue
```go
func LicenseFieldValue(name string) string
```
LicenseFieldValue returns the value of the specified license field. LicenseFieldValue accepts custom license fields and all built-in license fields. For a list of all built-in fields, see [Built-In License Fields](/vendor/licenses-using-builtin-fields).

LicenseFieldValue always returns a string, regardless of the license field type. To return integer or boolean values, you need to use the [ParseInt](/reference/template-functions-static-context#parseint) or [ParseBool](/reference/template-functions-static-context#parsebool) template function to convert the string value.

#### String License Field

The following example returns the value of the built-in `customerName` license field:

```yaml
customerName: '{{repl LicenseFieldValue "customerName" }}'
```
#### Integer License Field

The following example returns the value of a custom integer license field named `numSeats`:

```yaml
numSeats: repl{{ LicenseFieldValue "numSeats" | ParseInt }}
```
This example uses [ParseInt](/reference/template-functions-static-context#parseint) to convert the returned value to an integer.

#### Boolean License Field

The following example returns the value of a custom boolean license field named `feature-1`:

```yaml
feature-1: repl{{ LicenseFieldValue "feature-1" | ParseBool }}
```
This example uses [ParseBool](/reference/template-functions-static-context#parsebool) to convert the returned value to a boolean.

## LicenseDockerCfg
```go
func LicenseDockerCfg() string
```
LicenseDockerCfg returns a value that can be written to a secret if needed to deploy manually.
Replicated KOTS creates and injects this secret automatically in normal conditions, but some deployments (with static, additional namespaces) may need to include this.

```yaml
apiVersion: v1
kind: Secret
type: kubernetes.io/dockerconfigjson
metadata:
  name: myapp-registry
  namespace: my-other-namespace
data:
  .dockerconfigjson: repl{{ LicenseDockerCfg }}
```

## Sequence

```go
func Sequence() int64
```
Sequence is the sequence of the application deployed.
This will start at 0 for each installation, and increase with every app update, config change, license update and registry setting change.

```yaml
'{{repl Sequence }}'
```

## Cursor

```go
func Cursor() string
```
Cursor is the channel sequence of the app.
For instance, if 5 releases have been promoted to the channel that the app is running, then this would return the string `5`.

```yaml
'{{repl Cursor }}'
```

## ChannelName

```go
func ChannelName() string
```
ChannelName is the name of the deployed channel of the app.

```yaml
'{{repl ChannelName }}'
```

## VersionLabel

```go
func VersionLabel() string
```
VersionLabel is the semantic version of the app, as specified when promoting a release to a channel.

```yaml
'{{repl VersionLabel }}'
```

## ReleaseNotes

```go
func ReleaseNotes() string
```
ReleaseNotes is the release notes of the current version of the app.

```yaml
'{{repl ReleaseNotes }}'
```

## IsAirgap

```go
func IsAirgap() bool
```
IsAirgap is `true` when the app is installed via uploading an airgap package, false otherwise.

```yaml
'{{repl IsAirgap }}'
```


---


import StaticContext from "../partials/template-functions/_static-context.mdx"

# Static Context

<StaticContext/>

## Certificate Functions

### PrivateCACert

>Introduced in KOTS v1.117.0

```yaml
func PrivateCACert() string
```

PrivateCACert returns the name of a ConfigMap that contains private CA certificates provided by the end user. For Embedded Cluster installations, these certificates are provided with the `--private-ca` flag for the `install` command. For KOTS installations, the user provides the ConfigMap using the `--private-ca-configmap` flag for the `install` command.

You can use this template function to mount the specified ConfigMap so your containers can access the internet through enterprise proxies that issue their own TLS certificates in order to inspect traffic.

:::note
This function will return the name of the ConfigMap even if the ConfigMap has no entries. If no ConfigMap exists, this function returns the empty string.
:::

## Cluster Information Functions

### Distribution
```go
func Distribution() string
```
Distribution returns the Kubernetes distribution detected. The possible return values are:

* aks
* digitalOcean
* dockerDesktop
* eks
* embedded-cluster
* gke
* ibm
* k0s
* k3s
* kind
* kurl
* microk8s
* minikube
* oke
* openShift
* rke2

:::note
[IsKurl](#iskurl) can also be used to detect kURL instances.
:::

#### Detect the Distribution
```yaml
repl{{ Distribution }}
```
#### Equal To Comparison
```yaml
repl{{ eq Distribution "gke" }}
```
#### Not Equal To Comparison
```yaml
repl{{ ne Distribution "embedded-cluster" }}
```
See [Functions](https://pkg.go.dev/text/template#hdr-Functions) in the Go documentation.

### IsKurl
```go
func IsKurl() bool
```
IsKurl returns true if running within a kurl-based installation.
#### Detect kURL Installations
```yaml
repl{{ IsKurl }}
```
#### Detect Non-kURL Installations
```yaml
repl{{ not IsKurl }}
```
See [Functions](https://pkg.go.dev/text/template#hdr-Functions) in the Go documentation.

### KotsVersion

```go
func KotsVersion() string
```

KotsVersion returns the current version of KOTS.

```yaml
repl{{ KotsVersion }}
```

You can compare the KOTS version as follows:
```yaml
repl{{KotsVersion | semverCompare ">= 1.19"}}
```

This returns `true` if the KOTS version is greater than or equal to `1.19`.

For more complex comparisons, see [Semantic Version Functions](https://masterminds.github.io/sprig/semver.html) in the sprig documentation.

### KubernetesMajorVersion

> Introduced in KOTS v1.92.0

```go
func KubernetesMajorVersion() string
```

KubernetesMajorVersion returns the Kubernetes server *major* version.

```yaml
repl{{ KubernetesMajorVersion }}
```

You can compare the Kubernetes major version as follows:
```yaml
repl{{lt (KubernetesMajorVersion | ParseInt) 2 }}
```

This returns `true` if the Kubernetes major version is less than `2`.

### KubernetesMinorVersion

> Introduced in KOTS v1.92.0

```go
func KubernetesMinorVersion() string
```

KubernetesMinorVersion returns the Kubernetes server *minor* version.

```yaml
repl{{ KubernetesMinorVersion }}
```

You can compare the Kubernetes minor version as follows:
```yaml
repl{{gt (KubernetesMinorVersion | ParseInt) 19 }}
```

This returns `true` if the Kubernetes minor version is greater than `19`.

### KubernetesVersion

> Introduced in KOTS v1.92.0

```go
func KubernetesVersion() string
```

KubernetesVersion returns the Kubernetes server version.

```yaml
repl{{ KubernetesVersion }}
```

You can compare the Kubernetes version as follows:
```yaml
repl{{KubernetesVersion | semverCompare ">= 1.19"}}
```

This returns `true` if  the Kubernetes version is greater than or equal to `1.19`.

For more complex comparisons, see [Semantic Version Functions](https://masterminds.github.io/sprig/semver.html) in the sprig documentation.

### Namespace
```go
func Namespace() string
```
Namespace returns the Kubernetes namespace that the application belongs to.
```yaml
'{{repl Namespace}}'
```

### NodeCount
```go
func NodeCount() int
```
NodeCount returns the number of nodes detected within the Kubernetes cluster.
```yaml
repl{{ NodeCount }}
```

### Lookup

> Introduced in KOTS v1.103.0

```go
func Lookup(apiversion string, resource string, namespace string, name string) map[string]interface{}
```

Lookup searches resources in a running cluster and returns a resource or resource list.

Lookup uses the Helm lookup function to search resources and has the same functionality as the Helm lookup function. For more information, see [lookup](https://helm.sh/docs/chart_template_guide/functions_and_pipelines/#using-the-lookup-function) in the Helm documentation.

```yaml
repl{{ Lookup "API_VERSION" "KIND" "NAMESPACE" "NAME" }}
```

Both `NAME` and `NAMESPACE` are optional and can be passed as an empty string ("").

The following combination of parameters are possible:

<table>
  <tr>
    <th>Behavior</th>
    <th>Lookup function</th>
  </tr>
  <tr>
    <td style={{ fontSize: 14 }}><code>kubectl get pod mypod -n mynamespace</code></td>
    <td style={{ fontSize: 14 }}><code>repl&#123;&#123; Lookup "v1" "Pod" "mynamespace" "mypod" &#125;&#125;</code></td>
  </tr>
  <tr>
    <td style={{ fontSize: 14 }}><code>kubectl get pods -n mynamespace</code></td>
    <td style={{ fontSize: 14 }}><code>repl&#123;&#123; Lookup "v1" "Pod" "mynamespace" "" &#125;&#125;</code></td>
  </tr>
  <tr>
    <td style={{ fontSize: 14 }}><code>kubectl get pods --all-namespaces</code></td>
    <td style={{ fontSize: 14 }}><code>repl&#123;&#123; Lookup "v1" "Pod" "" "" &#125;&#125;</code></td>
  </tr>
  <tr>
    <td style={{ fontSize: 14 }}><code>kubectl get namespace mynamespace</code></td>
    <td style={{ fontSize: 14 }}><code>repl&#123;&#123; Lookup "v1" "Namespace" "" "mynamespace" &#125;&#125;</code></td>
  </tr>
  <tr>
    <td style={{ fontSize: 14 }}><code>kubectl get namespaces</code></td>
    <td style={{ fontSize: 14 }}><code>repl&#123;&#123; Lookup "v1" "Namespace" "" "" &#125;&#125;</code></td>
  </tr>
</table>

The following describes working with values returned by the Lookup function:

* When Lookup finds an object, it returns a dictionary with the key value pairs from the object. This dictionary can be navigated to extract specific values. For example, the following returns the annotations for the `mynamespace` object:

    ```
    repl{{ (Lookup "v1" "Namespace" "" "mynamespace").metadata.annotations }}
    ```

* When Lookup returns a list of objects, it is possible to access the object list through the `items` field. For example:

    ```
    services: |
      repl{{- range $index, $service := (Lookup "v1" "Service" "mynamespace" "").items }}
      - repl{{ $service.metadata.name }}
      repl{{- end }}
    ```

    For an array value type, omit the `|`. For example:

    ```
    services:
      repl{{- range $index, $service := (Lookup "v1" "Service" "mynamespace" "").items }}
      - repl{{ $service.metadata.name }}
      repl{{- end }}
    ```

* When no object is found, Lookup returns an empty value. This can be used to check for the existence of an object.

## Date Functions

### Now
```go
func Now() string
```
Returns the current timestamp as an RFC3339 formatted string.
```yaml
'{{repl Now }}'
```

### NowFmt
```go
func NowFmt(format string) string
```
Returns the current timestamp as a formatted string.
For information about Go time formatting guidelines, see [Constants](https://golang.org/pkg/time/#pkg-constants) in the Go documentation.
```yaml
'{{repl NowFmt "20060102" }}'
```

## Encoding Functions

### Base64Decode
```go
func Base64Decode(stringToDecode string) string
```
Returns decoded string from a Base64 stored value.
```yaml
'{{repl ConfigOption "base_64_encoded_name" | Base64Decode }}'
```

### Base64Encode
```go
func Base64Encode(stringToEncode string) string
```
Returns a Base64 encoded string.
```yaml
'{{repl ConfigOption "name" | Base64Encode }}'
```

### UrlEncode
```go
func UrlEncode(stringToEncode string) string
```
Returns the string, url encoded.
Equivalent to the `QueryEscape` function within the golang `net/url` library. For more information, see [func QueryEscape](https://godoc.org/net/url#QueryEscape) in the Go documentation.
```yaml
'{{repl ConfigOption "smtp_email" | UrlEncode }}:{{repl ConfigOption "smtp_password" | UrlEncode }}@smtp.example.com:587'
```

### UrlPathEscape

```go
func UrlPathEscape(stringToEncode string) string
```
Returns the string, url *path* encoded.
Equivalent to the `PathEscape` function within the golang `net/url` library. For more information, see [func PathEscape](https://godoc.org/net/url#PathEscape) in the Go documentation.
```yaml
'{{repl ConfigOption "smtp_email" | UrlPathEscape }}:{{repl ConfigOption "smtp_password" | UrlPathEscape }}@smtp.example.com:587'
```

## Encryption Functions

### KubeSeal
```go
func KubeSeal(certData string, namespace string, name string, value string) string
```

## Integer and Float Functions

### HumanSize
```go
func HumanSize(size interface{}) string
```
HumanSize returns a human-readable approximation of a size in bytes capped at 4 valid numbers (eg. "2.746 MB", "796 KB").
The size must be a integer or floating point number.
```yaml
'{{repl ConfigOption "min_size_bytes" | HumanSize }}'
```

## Proxy Functions

### HTTPProxy

```go
func HTTPProxy() string
```
HTTPProxy returns the address of the proxy that the Admin Console is configured to use.
```yaml
repl{{ HTTPProxy }}
```

### HTTPSProxy

```go
func HTTPSProxy() string
```
HTTPSProxy returns the address of the proxy that the Admin Console is configured to use.
```yaml
repl{{ HTTPSProxy }}
```

### NoProxy

```go
func NoProxy() string
```
NoProxy returns the comma-separated list of no-proxy addresses that the Admin Console is configured to use.
```yaml
repl{{ NoProxy }}
```

## Math Functions
### Add
```go
func Add(x interface{}, y interface{}) interface{}
```
Adds x and y.

If at least one of the operands is a floating point number, the result will be a floating point number.

If both operands are integers, the result will be an integer.
```yaml
'{{repl Add (ConfigOption "maximum_users") 1}}'
```

### Div
```go
func Div(x interface{}, y interface{}) interface{}
```
Divides x by y.

If at least one of the operands is a floating point number, the result will be a floating point number.

If both operands are integers, the result will be an integer and will be rounded down.
```yaml
'{{repl Div (ConfigOption "maximum_users") 2.0}}'
```

### Mult
```go
func Mult(x interface{}, y interface{}) interface{}
```
Multiplies x and y.

Both operands must be either an integer or a floating point number.

If at least one of the operands is a floating point number, the result will be a floating point number.

If both operands are integers, the result will be an integer.
```yaml
'{{repl Mult (NodePrivateIPAddressAll "DB" "redis" | len) 2}}'
```

If a template function returns a string, the value must be converted to an integer or a floating point number first:
```yaml
'{{repl Mult (ConfigOption "session_cookie_age" | ParseInt) 86400}}'
```

### Sub
```go
func Sub(x interface{}, y interface{}) interface{}
```
Subtracts y from x.

If at least one of the operands is a floating point number, the result will be a floating point number.

If both operands are integers, the result will be an integer.
```yaml
'{{repl Sub (ConfigOption "maximum_users") 1}}'
```

## String Functions

### ParseBool
```go
func ParseBool(str string) bool
```
ParseBool returns the boolean value represented by the string.
```yaml
'{{repl ConfigOption "str_value" | ParseBool }}'
```

### ParseFloat
```go
func ParseFloat(str string) float64
```
ParseFloat returns the float value represented by the string.
```yaml
'{{repl ConfigOption "str_value" | ParseFloat }}'
```

### ParseInt
```go
func ParseInt(str string, args ...int) int64
```
ParseInt returns the integer value represented by the string with optional base (default 10).
```yaml
'{{repl ConfigOption "str_value" | ParseInt }}'
```

### ParseUint
```go
func ParseUint(str string, args ...int) uint64
```
ParseUint returns the unsigned integer value represented by the string with optional base (default 10).
```yaml
'{{repl ConfigOption "str_value" | ParseUint }}'
```

### RandomString
```go
func RandomString(length uint64, providedCharset ...string) string
```
Returns a random string with the desired length and charset.
Provided charsets must be Perl formatted and match individual characters.
If no charset is provided, `[_A-Za-z0-9]` will be used.

#### Examples

The following example generates a 64-character random string:

```yaml
'{{repl RandomString 64}}'
```
The following example generates a 64-character random string that contains `a`s and `b`s:

```yaml
'{{repl RandomString 64 "[ab]" }}'
```
#### Generating Persistent and Ephemeral Strings

When you assign the RandomString template function to a `value` key in the Config custom resource, you can use the `hidden` and `readonly` properties to control the behavior of the RandomString function each time it is called. The RandomString template function is called each time the user deploys a change to the configuration settings for the application.

Depending on if the `hidden` and `readonly` properties are `true` or `false`, the random string generated by a RandomString template function in a `value` key is either ephemeral or persistent between configuration changes:

* **Ephemeral**: The value of the random string _changes_ when the user deploys a change to the configuration settings for the application.
* **Persistent**: The value of the random string does _not_ change when the user deploys a change to the configuration settings for the application.

For more information about these properties, see [`hidden`](custom-resource-config#hidden) and [`readonly`](custom-resource-config#readonly) in _Config_.

:::note
If you assign the RandomString template function to a `default` key in the Config custom resource rather than a `value` key, then the `hidden` and `readonly` properties do _not_ affect the behavior of the RandomString template function. For more information about the behavior of the `default` key in the Config custom resource, see [`default`](custom-resource-config#default) in _Config_.
:::

The following table describes the behavior of the RandomString template function when it is assigned to a `value` key in the Config custom resource and the `hidden` and `readonly` properties are `true` or `false`:

<table>
  <tr>
    <th width="15%">readonly</th>
    <th width="15%">hidden</th>
    <th width="15%">Outcome</th>
    <th width="55%">Use Case</th>
  </tr>
  <tr>
    <td>false</td>
    <td>true</td>
    <td>Persistent</td>
    <td>
      <p>Set <code>readonly</code> to <code>false</code> and <code>hidden</code> to <code>true</code> if:</p>
      <ul>
        <li>The random string must <em>not</em> change each time the user deploys a change to the application's configuration settings.</li>
        <li>The user does <em>not</em> need to see or change, or must be prevented from seeing or changing, the value of the random string.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>true</td>
    <td>false</td>
    <td>Ephemeral</td>
    <td>
      <p>Set <code>readonly</code> to <code>true</code> and <code>hidden</code> to <code>false</code> if:</p>
      <ul>
        <li>The random string <em>must</em> change each time the user deploys a change to the application's configuration settings.</li>
        <li>The user does <em>not</em> need to change, or must be prevented from changing, the value of the random string.</li>
        <li>The user <em>must</em> be able to see the value of the random string.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>true</td>
    <td>true</td>
    <td>Ephemeral</td>
    <td>
      <p>Set <code>readonly</code> to <code>true</code> and <code>hidden</code> to <code>true</code> if:</p>
      <ul>
        <li>The random string <em>must</em> change each time the user deploys a change to the application's configuration settings.</li>
        <li>The user does <em>not</em> need to see or change, or must be preventing from seeing or changing, the value of the random string.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>false</td>
    <td>false</td>
    <td>Persistent</td>
    <td>
      <p>Set <code>readonly</code> to <code>false</code> and <code>hidden</code> to <code>false</code> if:</p>
      <ul>
        <li>The random string must <em>not</em> change each time the user deploys a change to the application's configuration settings.</li>
        <li>The user <em>must</em> be able to see and change the value of the random string.</li>
      </ul>
      <p>For example, set both <code>readonly</code> and <code>hidden</code> to <code>false</code> to generate a random password that users must be able to see and then change to a different value that they choose.</p>
    </td>
  </tr>
</table>

### Split
```go
func Split(s string, sep string) []string
```
Split slices s into all substrings separated by sep and returns an array of the substrings between those separators.
```yaml
'{{repl Split "A,B,C" "," }}'
```

Combining `Split` and `index`:
Assuming the `github_url` param is set to `https://github.mycorp.internal:3131`, the following would set
`GITHUB_HOSTNAME` to `github.mycorp.internal`.
```yaml
'{{repl index (Split (index (Split (ConfigOption "github_url") "/") 2) ":") 0}}'
```

### ToLower
```go
func ToLower(stringToAlter string) string
```
Returns the string, in lowercase.
```yaml
'{{repl ConfigOption "company_name" | ToLower }}'
```

### ToUpper
```go
func ToUpper(stringToAlter string) string
```
Returns the string, in uppercase.
```yaml
'{{repl ConfigOption "company_name" | ToUpper }}'
```

### Trim
```go
func Trim(s string, args ...string) string
```
Trim returns a string with all leading and trailing strings contained in the optional args removed (default space).
```yaml
'{{repl Trim (ConfigOption "str_value") "." }}'
```

### TrimSpace
```go
func TrimSpace(s string) string
```
Trim returns a string with all leading and trailing spaces removed.
```yaml
'{{repl ConfigOption "str_value" | TrimSpace }}'
```

### YamlEscape
```go
func YamlEscape(input string) string
```

YamlEscape returns an escaped and quoted version of the input string, suitable for use within a YAML document.
This can be useful when dealing with user-uploaded files that may include null bytes and other nonprintable characters. For more information about printable characters, see [Character Set](https://yaml.org/spec/1.2.2/#51-character-set) in the YAML documentation.

```yaml
repl{{ ConfigOptionData "my_file_upload" | YamlEscape }}
```

---


import GitOpsLimitation from "../partials/helm/_gitops-limitation.mdx"
import GitOpsNotRecommended from "../partials/gitops/_gitops-not-recommended.mdx"
import TemplateLimitation from "../partials/helm/_helm-template-limitation.mdx"
import VersionLimitation from "../partials/helm/_helm-version-limitation.mdx"
import HooksLimitation from "../partials/helm/_hooks-limitation.mdx"
import HookWeightsLimitation from "../partials/helm/_hook-weights-limitation.mdx"
import Deprecated from "../partials/helm/_replicated-deprecated.mdx"
import KotsHelmCrDescription from "../partials/helm/_kots-helm-cr-description.mdx"
import ReplicatedHelmMigration from "../partials/helm/_replicated-helm-migration.mdx"
import Helm from "../partials/helm/_helm-definition.mdx"

# About Distributing Helm Charts with KOTS

This topic provides an overview of how Replicated KOTS deploys Helm charts, including an introduction to the KOTS HelmChart custom resource, limitations of deploying Helm charts with KOTS, and more.

## Overview

<Helm/>

KOTS can install applications that include:
* One or more Helm charts
* More than a single instance of any chart
* A combination of Helm charts and Kubernetes manifests

Replicated strongly recommends that all applications are packaged as Helm charts because many enterprise users expect to be able to install an application with the Helm CLI.

Deploying Helm charts with KOTS provides additional functionality not directly available with the Helm CLI, such as:
* The KOTS Admin Console
* Backup and restore with snapshots
* Support for air gap installations
* Support for embedded cluster installations on VMs or bare metal servers

Additionally, for applications packaged as Helm charts, you can support Helm CLI and KOTS installations from the same release without having to maintain separate sets of Helm charts and application manifests. The following diagram demonstrates how a single release containing one or more Helm charts can be installed using the Helm CLI and KOTS:

<img src="/images/helm-kots-install-options.png" width="650px" alt="One release being installed into three different customer environments"/>

[View a larger version of this image](/images/helm-kots-install-options.png)

For a tutorial that demonstrates how to add a sample Helm chart to a release and then install the release using both KOTS and the Helm CLI, see [Install a Helm Chart with KOTS and the Helm CLI](/vendor/tutorial-kots-helm-setup).

## How KOTS Deploys Helm Charts

This section describes how KOTS uses the HelmChart custom resource to deploy Helm charts.

### About the HelmChart Custom Resource

<KotsHelmCrDescription/>

The HelmChart custom resource with `apiVersion: kots.io/v1beta2` (HelmChart v2) is supported with KOTS v1.99.0 and later. For more information, see [About the HelmChart kots.io/v1beta2 Installation Method](#v2-install) below.

KOTS versions earlier than v1.99.0 can install Helm charts with `apiVersion: kots.io/v1beta1` of the HelmChart custom resource. The `kots.io/v1beta1` HelmChart custom resource is deprecated. For more information, see [Deprecated HelmChart kots.io/v1beta1 Installation Methods](#deprecated-helmchart-kotsiov1beta1-installation-methods) below.

### About the HelmChart v2 Installation Method {#v2-install}

When you include a HelmChart custom resource with `apiVersion: kots.io/v1beta2` in a release, KOTS v1.99.0 or later does a Helm install or upgrade of the associated Helm chart directly.

The `kots.io/v1beta2` HelmChart custom resource does _not_ modify the chart during installation. This results in Helm chart installations that are consistent, reliable, and easy to troubleshoot. For example, you can reproduce the exact installation outside of KOTS by downloading a copy of the application files from the cluster with `kots download`, then using those files to install with `helm install`. And, you can use `helm get values` to view the values that were used to install.

The `kots.io/v1beta2` HelmChart custom resource requires configuration. For more information, see [Configuring the HelmChart Custom Resource v2](helm-native-v2-using).

For information about the fields and syntax of the HelmChart custom resource, see [HelmChart v2](/reference/custom-resource-helmchart-v2).

### Limitations

The following limitations apply when deploying Helm charts with the `kots.io/v1beta2` HelmChart custom resource:

* Available only for Helm v3.

* Available only for KOTS v1.99.0 and later.

* The rendered manifests shown in the `rendered` directory might not reflect the final manifests that will be deployed to the cluster. This is because the manifests in the `rendered` directory are generated using `helm template`, which is not run with cluster context. So values returned by the `lookup` function and the built-in `Capabilities` object might differ.

* When updating the HelmChart custom resource in a release from `kots.io/v1beta1` to `kots.io/v1beta2`, the diff viewer shows a large diff because the underlying file structure of the rendered manifests is different.

* Editing downstream Kustomization files to make changes to the application before deploying is not supported. This is because KOTS does not use Kustomize when installing Helm charts with the `kots.io/v1beta2` HelmChart custom resource. For more information about patching applications with Kustomize, see [Patching with Kustomize](/enterprise/updating-patching-with-kustomize).

* <GitOpsLimitation/>

   <GitOpsNotRecommended/>

   For more information, see [KOTS Auto-GitOps Workflow](/enterprise/gitops-workflow).
## Support for Helm Hooks {#hooks}

KOTS supports the following hooks for Helm charts:
* `pre-install`: Executes after resources are rendered but before any resources are installed.
* `post-install`: Executes after resources are installed.
* `pre-upgrade`: Executes after resources are rendered but before any resources are upgraded.
* `post-upgrade`: Executes after resources are upgraded.
* `pre-delete`: Executes before any resources are deleted.
* `post-delete`: Executes after resources are deleted.

The following limitations apply to using hooks with Helm charts deployed by KOTS:

* <HooksLimitation/>

* <HookWeightsLimitation/>

For more information about Helm hooks, see [Chart Hooks](https://helm.sh/docs/topics/charts_hooks/) in the Helm documentation.

## Air Gap Installations

KOTS supports installation of Helm charts into air gap environments with configuration of the HelmChart custom resource [`builder`](/reference/custom-resource-helmchart-v2#builder)  key. The `builder` key specifies the Helm values to use when building the air gap bundle for the application.

For more information about how to configure the `builder` key to support air gap installations, see [Packaging Air Gap Bundles for Helm Charts](/vendor/helm-packaging-airgap-bundles).

## Resource Deployment Order

When installing an application that includes one or more Helm charts, KOTS always deploys standard Kubernetes manifests to the cluster _before_ deploying any Helm charts. For example, if your release contains a Helm chart, a CRD, and a ConfigMap, then the CRD and ConfigMap resources are deployed before the Helm chart.

For information about how to set the deployment order for Helm charts with KOTS, see [Orchestrating Resource Deployment](/vendor/orchestrating-resource-deployment).

## Deprecated HelmChart kots.io/v1beta1 Installation Methods

This section describes the deprecated Helm chart installation methods that use the HelmChart custom resource `apiVersion: kots.io/v1beta1`.

:::important
<Deprecated/>
:::

### useHelmInstall: true {#v1beta1}

:::note
This method was previously referred to as _Native Helm_.
:::

When you include version `kots.io/v1beta1` of the HelmChart custom resource with `useHelmInstall: true`, KOTS uses Kustomize to render the chart with configuration values, license field values, and rewritten image names. KOTS then packages the resulting manifests into a new Helm chart to install. For more information about Kustomize, see the [Kustomize documentation](https://kubectl.docs.kubernetes.io/).

The following diagram shows how KOTS processes Helm charts for deployment with the `kots.io/v1beta1` method:

![Flow chart of a v1beta1 Helm chart deployment to a cluster](/images/native-helm-flowchart.png)

[View a larger image](/images/native-helm-flowchart.png)

As shown in the diagram above, when given a Helm chart, KOTS:

- Uses Kustomize to merge instructions from KOTS and the end user to chart resources (see steps 2 - 4 below)
- Packages the resulting manifest files into a new Helm chart (see step 5 below)
- Deploys the new Helm chart (see step 5 below)

To deploy Helm charts with version `kots.io/v1beta1` of the HelmChart custom resource, KOTS does the following:

1. **Checks for previous installations of the chart**: If the Helm chart has already been deployed with a HelmChart custom resource that has `useHelmInstall: false`, then KOTS does not attempt the install the chart. The following error message is displayed if this check fails: `Deployment method for chart <chart_name> has changed`. For more information, see [HelmChart kots.io/v1beta1 (useHelmInstall: false)](#v1beta1-false) below.

1. **Writes base files**:  KOTS extracts Helm manifests, renders them with Replicated templating, and then adds all files from the original Helm tarball to a `base/charts/` directory.

  Under `base/charts/`, KOTS adds a Kustomization file named `kustomization.yaml` in the directories for each chart and subchart. KOTS uses these Kustomization files later in the deployment process to merge instructions from Kustomize to the chart resources. For more information about Kustomize, see the [Kustomize website](https://kustomize.io).

  The following screenshot from the Replicated Admin Console shows a `base/charts/` directory for a deployed application. The `base/charts/` directory contains a Helm chart named postgresql with one subchart:

  ![Base directory in the Admin Console](/images/native-helm-base.png)

  In the screenshot above, a Kustomization file that targets the resources from the postgresql Helm chart appears in the `base/charts/postgresql/` directory:

   ```yaml
   apiVersion: kustomize.config.k8s.io/v1beta1
   kind: Kustomization
   resources:
   - secrets.yaml
   - statefulset.yaml
   - svc-headless.yaml
   - svc.yaml
   ```
   
1. **Writes midstream files with Kustomize instructions from KOTS**: KOTS then copies the directory structure from `base/charts/` to an `overlays/midstream/charts/` directory. The following screenshot shows an example of the midstream directory for the postgresql Helm chart: 
   
  ![Midstream directory in the Admin Console UI](/images/native-helm-midstream.png)

  As shown in the screenshot above, the midstream directory also contains a Kustomization file with instructions from KOTS for all deployed resources, such as image pull secrets, image rewrites, and backup labels. For example, in the midstream Kustomization file, KOTS rewrites any private images to pull from the Replicated proxy registry.

  The following shows an example of a midstream Kustomization file for the postgresql Helm chart:

    ```yaml
    apiVersion: kustomize.config.k8s.io/v1beta1
    bases:
    - ../../../../base/charts/postgresql
    commonAnnotations:
      kots.io/app-slug: helm-test
    images:
    - name: gcr.io/replicated-qa/postgresql
      newName: proxy.replicated.com/proxy/helm-test/gcr.io/replicated-qa/postgresql
    kind: Kustomization
    patchesStrategicMerge:
    - pullsecrets.yaml
    resources:
    - secret.yaml
    transformers:
    - backup-label-transformer.yaml
    ```

    As shown in the example above, all midstream Kustomization files have a `bases` entry that references the corresponding Kustomization file from the `base/charts/` directory.

1. **Writes downstream files for end user Kustomize instructions**: KOTS then creates an `overlays/downstream/this-cluster/charts` directory and again copies the directory structure of `base/charts/` to this downstream directory:

   ![Downstream directory in the Admin Console UI](/images/native-helm-downstream.png)

   As shown in the screenshot above, each chart and subchart directory in the downstream directory also contains a Kustomization file. These downstream Kustomization files contain only a `bases` entry that references the corresponding Kustomization file from the midstream directory. For example:

    ```yaml
    apiVersion: kustomize.config.k8s.io/v1beta1
    bases:
    - ../../../../midstream/charts/postgresql
    kind: Kustomization
    ```
   
   End users can edit the downstream Kustomization files to make changes before deploying the application. Any instructions that users add to the Kustomization files in the downstream directory take priority over midstream and base Kustomization files. For more information about how users can make changes before deploying, see [Patching with Kustomize](/enterprise/updating-patching-with-kustomize).

1. **Deploys the Helm chart**: KOTS runs `kustomize build` for any Kustomization files in the `overlays/downstream/charts` directory. KOTS then packages the resulting manifests into a new chart for Helm to consume.

   Finally, KOTS runs `helm upgrade -i <release-name> <chart> --timeout 3600s -n <namespace>`. The Helm binary processes hooks and weights, applies manifests to the Kubernetes cluster, and saves a release secret similar to `sh.helm.release.v1.chart-name.v1`. Helm uses this secret to track upgrades and rollbacks of applications.

### useHelmInstall: false {#v1beta1-false}

:::note
This method was previously referred to as _Replicated Helm_.
:::

When you use version `kots.io/v1beta1` of HelmChart custom resource with `useHelmInstall: false`, KOTS renders the Helm templates and deploys them as standard Kubernetes manifests using `kubectl apply`. KOTS also has additional functionality for specific Helm hooks. For example, when KOTS encounters an upstream Helm chart with a `helm.sh/hook-delete-policy` annotation, it automatically adds the same `kots.io/hook-delete-policy` to the Job object.

The resulting deployment is comprised of standard Kubernetes manifests. Therefore, cluster operators can view the exact differences between what is currently deployed and what an update will deploy.

### Limitations {#replicated-helm-limitations}

This section lists the limitations for version `kots.io/v1beta1` of the HelmChart custom resource.
#### kots.io/v1beta1 (useHelmInstall: true) Limitations

The following limitations apply when using version `kots.io/v1beta1` of the HelmChart custom resource with `useHelmInstall: true`:

* <Deprecated/>

* Available only for Helm V3.

* <GitOpsLimitation/>

   For more information, see [KOTS Auto-GitOps Workflow](/enterprise/gitops-workflow).

* <HooksLimitation/>

* <HookWeightsLimitation/>

* <TemplateLimitation/>

* <VersionLimitation/>

  For more information, see [helmVersion](/reference/custom-resource-helmchart#helmversion) in _HelmChart_.

####  kots.io/v1beta1 (useHelmInstall: false) Limitations {#v1beta1-false-limitations}

The following limitations apply when using version `kots.io/v1beta1` of the HelmChart custom resource with `useHelmInstall: false`:

* <ReplicatedHelmMigration/>

* <TemplateLimitation/>

* <VersionLimitation/>

  For more information, see [helmVersion](/reference/custom-resource-helmchart#helmversion) in _HelmChart_.


---


import KotsHelmCrDescription from "../partials/helm/_kots-helm-cr-description.mdx"

# Configuring the HelmChart Custom Resource v2

This topic describes how to configure the Replicated HelmChart custom resource version `kots.io/v1beta2` to support Helm chart installations with Replicated KOTS.

## Workflow

To support Helm chart installations with the KOTS `kots.io/v1beta2` HelmChart custom resource, do the following:
1. Rewrite image names to use the Replicated proxy registry. See [Rewrite Image Names](#rewrite-image-names).
1. Inject a KOTS-generated image pull secret that grants proxy access to private images. See [Inject Image Pull Secrets](#inject-image-pull-secrets).
1. Add a pull secret for any Docker Hub images that could be rate limited. See [Add Pull Secret for Rate-Limited Docker Hub Images](#docker-secret).
1. Configure the `builder` key to allow your users to push images to their own local registries. See [Support Local Image Registries](#local-registries).
1. (KOTS Existing Cluster and kURL Installations Only) Add backup labels to your resources to support backup and restore with the KOTS snapshots feature. See [Add Backup Labels for Snapshots](#add-backup-labels-for-snapshots).
   :::note
   Snapshots is not supported for installations with Replicated Embedded Cluster. For more information about configuring disaster recovery for Embedded Cluster, see [Disaster Recovery for Embedded Cluster](/vendor/embedded-disaster-recovery).
   :::

## Task 1: Rewrite Image Names {#rewrite-image-names}

Configure the KOTS HelmChart custom resource `values` key so that KOTS rewrites the names for both private and public images in your Helm values during deployment. This allows images to be accessed at one of the following locations, depending on where they were pushed:
* The [Replicated proxy registry](private-images-about) (`proxy.replicated.com` or your custom domain)
* A public image registry
* Your customer's local registry
* The built-in registry used in Replicated Embedded Cluster or Replicated kURL installations in air-gapped environments

You will use the following KOTS template functions to conditionally rewrite image names depending on where the given image should be accessed: 
* [HasLocalRegistry](/reference/template-functions-config-context#haslocalregistry): Returns true if the installation environment is configured to use a local image registry. HasLocalRegistry is always true in air gap installations. HasLocalRegistry is also true in online installations if the user configured a local private registry.
* [LocalRegistryHost](/reference/template-functions-config-context#localregistryhost): Returns the host of the local registry that the user configured. Alternatively, for air gap installations with Embedded Cluster or kURL, LocalRegistryHost returns the host of the built-in registry.
* [LocalRegistryNamespace](/reference/template-functions-config-context#localregistrynamespace): Returns the namespace of the local registry that the user configured. Alternatively, for air gap installations with Embedded Cluster or kURL, LocalRegistryNamespace returns the namespace of the built-in registry.

    <details>
    <summary>What is the registry namespace?</summary>
    
    The registry namespace is the path between the registry and the image name. For example, `images.yourcompany.com/namespace/image:tag`.
    </details>

### Task 1a: Rewrite Private Image Names

For any private images used by your application, configure the HelmChart custom resource so that image names are rewritten to either the Replicated proxy registry (for online installations) or to the local registry in the user's installation environment (for air gap installations or online installations where the user configured a local registry).

To rewrite image names to the location of the image in the proxy registry, use the format `<proxy-domain>/proxy/<app-slug>/<image>`, where:
* `<proxy-domain>` is `proxy.replicated.com` or your custom domain. For more information about configuring a custom domain for the proxy registry, see [Using Custom Domains](/vendor/custom-domains-using).
* `<app-slug>` is the unique application slug in the Vendor Portal
* `<image>` is the path to the image in your registry

For example, if the private image is `quay.io/my-org/nginx:v1.0.1` and `images.yourcompany.com` is the custom proxy registry domain, then the image name should be rewritten to `images.yourcompany.com/proxy/my-app-slug/quay.io/my-org/nginx:v1.0.1`.

For more information, see the example below. 

#### Example

The following HelmChart custom resource uses the KOTS [HasLocalRegistry](/reference/template-functions-config-context#haslocalregistry), [LocalRegistryHost](/reference/template-functions-config-context#localregistryhost), and [LocalRegistryNamespace](/reference/template-functions-config-context#localregistrynamespace) template functions to conditionally rewrite an image registry and repository depending on if a local registry is used:

```yaml
# kots.io/v1beta2 HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  ...
  values:
    image:
    # If a registry is configured by the user or by Embedded Cluster/kURL, use that registry's hostname
    # Else use proxy.replicated.com or your custom proxy registry domain
      registry: '{{repl HasLocalRegistry | ternary LocalRegistryHost "images.yourcompany.com" }}'
      # If a registry is configured by the user or by Embedded Cluster/kURL, use that registry namespace
      # Else use the image's namespace at the proxy registry domain
      repository: '{{repl HasLocalRegistry | ternary LocalRegistryNamespace "proxy/my-app/quay.io/my-org" }}/nginx'
      tag: v1.0.1
```

The `spec.values.image.registry` and `spec.values.image.repository` fields in the HelmChart custom resource above correspond to `image.registry` and `image.repository` fields in the Helm chart `values.yaml` file, as shown below:

```yaml
# Helm chart values.yaml file

image:
  registry: quay.io
  repository: my-org/nginx
  tag: v1.0.1
```

During installation, KOTS renders the template functions and sets the `image.registry` and `image.repository` fields in the Helm chart `values.yaml` file based on the value of the corresponding fields in the HelmChart custom resource.

Any templates in the Helm chart that access the `image.registry` and `image.repository` fields are updated to use the appropriate value, as shown in the example below:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: 
    image: {{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}
```

### Task 1b: Rewrite Public Image Names

For any public images used by your application, configure the HelmChart custom resource so that image names are rewritten to either the location of the image in the public registry (for online installations) or the local registry (for air gap installations or online installations where the user configured a local registry.

For more information, see the example below.

#### Example

The following HelmChart custom resource uses the KOTS [HasLocalRegistry](/reference/template-functions-config-context#haslocalregistry), [LocalRegistryHost](/reference/template-functions-config-context#localregistryhost), and [LocalRegistryNamespace](/reference/template-functions-config-context#localregistrynamespace) template functions to conditionally rewrite an image registry and repository depending on if a local registry is used:

```yaml
# kots.io/v1beta2 HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  ...
  values:
    image: 
      # If a local registry is used, use that registry's hostname
      # Else, use the public registry host (ghcr.io) 
      registry: '{{repl HasLocalRegistry | ternary LocalRegistryHost "ghcr.io" }}' 
      # If a local registry is used, use the registry namespace provided
      # Else, use the path to the image in the public registry 
      repository: '{{repl HasLocalRegistry | ternary LocalRegistryNamespace "cloudnative-pg" }}/cloudnative-pg'
      tag: catalog-1.24.0
```

The `spec.values.image.registry` and `spec.values.image.repository` fields in the HelmChart custom resource correspond to `image.registry` and `image.repository` fields in the Helm chart `values.yaml` file, as shown in the example below:

```yaml
# Helm chart values.yaml file

image:
  registry: ghcr.io
  repository: cloudnative-pg/cloudnative-pg
  tag: catalog-1.24.0
```

During installation, KOTS renders the template functions and sets the `image.registry` and `image.repository` fields in your Helm chart `values.yaml` file based on the value of the corresponding fields in the HelmChart custom resource. Any templates in the Helm chart that access the `image.registry` and `image.repository` fields are updated to use the appropriate value, as shown in the example below:

```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: 
    image: {{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}
```

## Task 2: Inject Image Pull Secrets {#inject-image-pull-secrets}

Kubernetes requires a Secret of type `kubernetes.io/dockerconfigjson` to authenticate with a registry and pull a private image. When you reference a private image in a Pod definition, you also provide the name of the Secret in a `imagePullSecrets` key in the Pod definition. For more information, see [Specifying imagePullSecrets on a Pod](https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod) in the Kubernetes documentation.

During installation, KOTS creates a `kubernetes.io/dockerconfigjson` type Secret that is based on the customer license. This pull secret grants access to the private image through the Replicated proxy registry or in the Replicated registry. Additionally, if the user configured a local image registry, then the pull secret contains the credentials for the local registry. You must provide the name of this KOTS-generated pull secret in any Pod definitions that reference the private image.

You can inject the name of this pull secret into a field in the HelmChart custom resource using the Replicated ImagePullSecretName template function. During installation, KOTS sets the value of the corresponding field in your Helm chart `values.yaml` file with the rendered value of the ImagePullSecretName template function. 

#### Example

The following example shows a `spec.values.image.pullSecrets` array in the HelmChart custom resource that uses the ImagePullSecretName template function to inject the name of the KOTS-generated pull secret:

```yaml
# kots.io/v1beta2 HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  values:
    image: 
      # Note: Use proxy.replicated.com or your custom domain
      registry: '{{repl HasLocalRegistry | ternary LocalRegistryHost "proxy.replicated.com" }}'
      repository: '{{repl HasLocalRegistry | ternary LocalRegistryNamespace "proxy/my-app/ecr.us-east-1.amazonaws.com/my-org" }}/api'
      pullSecrets:
      - name: '{{repl ImagePullSecretName }}'
```

The `spec.values.image.pullSecrets` array in the HelmChart custom resource corresponds to a `image.pullSecrets` array in the Helm chart `values.yaml` file, as shown in the example below:

```yaml
# Helm chart values.yaml file

image:
  registry: ecr.us-east-1.amazonaws.com
  repository: my-org/api/nginx
  pullSecrets:
  - name: my-org-secret
```

During installation, KOTS renders the ImagePullSecretName template function and adds the rendered pull secret name to the `image.pullSecrets` array in the Helm chart `values.yaml` file.

Any templates in the Helm chart that access the `image.pullSecrets` field are updated to use the name of the KOTS-generated pull secret, as shown in the example below:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: {{ .Values.image.registry }}/{{ .Values.image.repository }}
  {{- with .Values.image.pullSecrets }}
  imagePullSecrets:
  {{- toYaml . | nindent 2 }}
  {{- end }}
```

## Task 3: Add Pull Secret for Rate-Limited Docker Hub Images {#docker-secret}

Docker Hub enforces rate limits for Anonymous and Free users. To avoid errors caused by reaching the rate limit, your users can run the `kots docker ensure-secret` command, which creates an `<app-slug>-kotsadm-dockerhub` secret for pulling Docker Hub images and applies the secret to Kubernetes manifests that have images. For more information, see [Avoiding Docker Hub Rate Limits](/enterprise/image-registry-rate-limits).

If you are deploying a Helm chart with Docker Hub images that could be rate limited, to support the use of the `kots docker ensure-secret` command, any Pod definitions in your Helm chart templates that reference the rate-limited image must be updated to access the `<app-slug>-kotsadm-dockerhub` pull secret, where `<app-slug>` is your application slug. For more information, see [Get the Application Slug](/vendor/vendor-portal-manage-app#slug).

You can do this by adding the `<app-slug>-kotsadm-dockerhub` pull secret to a field in the `values` key of the HelmChart custom resource, along with a matching field in your Helm chart `values.yaml` file. During installation, KOTS sets the value of the matching field in the `values.yaml` file with the `<app-slug>-kotsadm-dockerhub` pull secret, and any Helm chart templates that access the value are updated.

For more information about Docker Hub rate limiting, see [Understanding Docker Hub rate limiting](https://www.docker.com/increase-rate-limits) on the Docker website.

#### Example

The following Helm chart `values.yaml` file includes `image.registry`, `image.repository`, and `image.pullSecrets` for a rate-limited Docker Hub image:

```yaml
# Helm chart values.yaml file

image:
  registry: docker.io
  repository: my-org/example-docker-hub-image
  pullSecrets: []
```

The following HelmChart custom resource includes `spec.values.image.registry`, `spec.values.image.repository`, and `spec.values.image.pullSecrets`, which correspond to those in the Helm chart `values.yaml` file above.

The `spec.values.image.pullSecrets` array lists the `<app-slug>-kotsadm-dockerhub` pull secret, where the slug for the application is `example-app-slug`:

```yaml
# kots.io/v1beta2 HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  values:
    image:
      registry: docker.io
      repository: my-org/example-docker-hub-image
      pullSecrets:
      - name: example-app-slug-kotsadm-dockerhub
```

During installation, KOTS adds the `example-app-slug-kotsadm-dockerhub` secret to the `image.pullSecrets` array in the Helm chart `values.yaml` file. Any templates in the Helm chart that access `image.pullSecrets` are updated to use `example-app-slug-kotsadm-dockerhub`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example
spec:
  containers:
  - name: example
    image: {{ .Values.image.registry }}/{{ .Values.image.repository }}
  {{- with .Values.image.pullSecrets }}
  imagePullSecrets:
  {{- toYaml . | nindent 2 }}
  {{- end }}
```

## Task 4: Support the Use of Local Image Registries {#local-registries}

Local image registries are required for KOTS installations in air-gapped environments with no outbound internet connection. Also, users in online environments can optionally use a local registry. For more information about how users configure a local image registry with KOTS, see [Configuring Local Image Registries](/enterprise/image-registry-settings).

To support the use of local registries, configure the `builder` key. For more information about how to configure the `builder` key, see [`builder`](/reference/custom-resource-helmchart-v2#builder) in _HelmChart v2_.

## Task 5: Add Backup Labels for Snapshots (KOTS Existing Cluster and kURL Installations Only) {#add-backup-labels-for-snapshots}

:::note
The Replicated [snapshots](snapshots-overview) feature for backup and restsore is supported only for existing cluster installations with KOTS. Snapshots are not support for installations with Embedded Cluster. For more information about disaster recovery for installations with Embedded Cluster, see [Disaster Recovery for Embedded Cluster](/vendor/embedded-disaster-recovery.mdx).
:::

The snapshots feature requires the following labels on all resources in your Helm chart that you want to be included in the backup:
* `kots.io/backup: velero`
* `kots.io/app-slug: APP_SLUG`, where `APP_SLUG` is the slug of your Replicated application.

For more information about snapshots, see [Understanding Backup and Restore](snapshots-overview).

To support backup and restore with snapshots, add the `kots.io/backup: velero` and `kots.io/app-slug: APP_SLUG` labels to fields under the HelmChart custom resource `optionalValues` key. Add a `when` statement that evaluates to true only when the customer license has the `isSnapshotSupported` entitlement.

The fields that you create under the `optionalValues` key must map to fields in your Helm chart `values.yaml` file. For more information about working with the `optionalValues` key, see [optionalValues](/reference/custom-resource-helmchart-v2#optionalvalues) in _HelmChart v2_.

#### Example

The following example shows how to add backup labels for snapshots in the `optionalValues` key of the HelmChart custom resource:

```yaml
# kots.io/v1beta2 HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  ...
  optionalValues:
  # add backup labels only if the license supports snapshots
  - when: "repl{{ LicenseFieldValue `isSnapshotSupported` }}"
    recursiveMerge: true
    values:
      mariadb:
        commonLabels:
          kots.io/backup: velero
          kots.io/app-slug: repl{{ LicenseFieldValue "appSlug" }}
        podLabels:
          kots.io/backup: velero
          kots.io/app-slug: repl{{ LicenseFieldValue "appSlug" }}
```

## Additional Information

### About the HelmChart Custom Resource


<KotsHelmCrDescription/>

For more information about the HelmChart custom resource, including the unique requirements and limitations for the keys described in this topic, see [HelmChart v2](/reference/custom-resource-helmchart-v2).

### HelmChart v1 and v2 Differences

To support the use of local registries with version `kots.io/v1beta2` of the HelmChart custom resource, provide the necessary values in the builder field to render the Helm chart with all of the necessary images so that KOTS knows where to pull the images from to push them into the local registry.

For more information about how to configure the `builder` key, see [Packaging Air Gap Bundles for Helm Charts](/vendor/helm-packaging-airgap-bundles) and [`builder`](/reference/custom-resource-helmchart-v2#builder) in _HelmChart v2_.

The `kots.io/v1beta2` HelmChart custom resource has the following differences from `kots.io/v1beta1`:

<table>
  <tr>
    <th>HelmChart v1beta2</th>
    <th>HelmChart v1beta1</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><code>apiVersion: kots.io/v1beta2</code></td>
    <td><code>apiVersion: kots.io/v1beta1</code></td>
    <td><code>apiVersion</code> is updated to <code>kots.io/v1beta2</code></td>
  </tr>
  <tr>
    <td><code>releaseName</code></td>
    <td><code>chart.releaseName</code></td>
    <td><code>releaseName</code> is a top level field under <code>spec</code></td>
  </tr>
  <tr>
    <td>N/A</td>
    <td><code>helmVersion</code></td>
    <td><code>helmVersion</code> field is removed</td>
  </tr>
  <tr>
    <td>N/A</td>
    <td><code>useHelmInstall</code></td>
    <td><code>useHelmInstall</code> field is removed</td>
  </tr>
</table>

### Migrate Existing KOTS Installations to HelmChart v2

Existing KOTS installations can be migrated to use the KOTS HelmChart v2 method, without having to reinstall the application.

There are different steps for migrating to HelmChart v2 depending on the application deployment method used previously. For more information, see [Migrating Existing Installations to HelmChart v2](helm-v2-migrate).


---


import HelmBuilderRequirements from "../partials/helm/_helm-builder-requirements.mdx"
import BuilderAirgapIntro from "../partials/helm/_helm-cr-builder-airgap-intro.mdx"
import BuilderExample from "../partials/helm/_helm-cr-builder-example.mdx"
import AirGapBundle from "../partials/airgap/_airgap-bundle.mdx"

# Packaging Air Gap Bundles for Helm Charts

This topic describes how to package and build air gap bundles for releases that contain one or more Helm charts. This topic applies to applications deployed with Replicated KOTS.

## Overview

<AirGapBundle/>

When building the `.airgap` bundle for a release that contains one or more Helm charts, the Vendor Portal renders the Helm chart templates in the release using values supplied in the KOTS HelmChart custom resource [`builder`](/reference/custom-resource-helmchart-v2#builder) key.

## Configure the `builder` Key

You should configure the `builder` key if you need to change any default values in your Helm chart so that the `.airgap` bundle for the release includes all images needed to successfully deploy the chart. For example, you can change the default Helm values so that images for any conditionally-deployed components are always included in the air gap bundle. Additionally, you can use the `builder` key to set any `required` values in your Helm chart that must be set for the chart to render.

The values in the `builder` key map to values in the given Helm chart's `values.yaml` file. For example, `spec.builder.postgres.enabled` in the example HelmChart custom resource below would map to a `postgres.enabled` field in the `values.yaml` file for the `samplechart` chart:

```yaml
# KOTS HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  builder:
    postgres:
      enabled: true 
```

For requirements, recommendations, and examples of common use cases for the `builder` key, see the sections below.

### Requirements and Recommendations

<HelmBuilderRequirements/>

### Example: Set the Image Registry for Air Gap Installations

For air gap installations, if the [Replicated proxy registry](/vendor/private-images-about) domain `proxy.replicated.com` is used as the default image name for any images, you need to rewrite the image to the upstream image name so that it can be processed and included in the air gap bundle. You can use the `builder` key to do this by hardcoding the upstream location of the image (image registry, repository, and tag), as shown in the example below:

```yaml
apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: samplechart
spec:
  chart:
    name: samplechart
    chartVersion: 3.1.7
  builder:
    my-service:
      image:
        registry: 12345.dkr.ecr.us-west-1.amazonaws.com
        repository: my-app
        tag: "1.0.2"    
```
When building the `.airgap` bundle for the release, the Vendor Portal uses the registry, repository, and tag values supplied in the `builder` key to template the Helm chart, rather than the default values defined in the Helm `values.yaml` file. This ensures that the image is pulled from the upstream registry using the credentials supplied in the Vendor Portal, without requiring any changes to the Helm chart directly.

### Example: Include Conditional Images

Many applications have images that are included or excluded based on a given condition. For example, enterprise users might have the option to deploy an embedded database with the application or bring their own database. To support this use case for air gap installations, the images for any conditionally-deployed components must always be included in the air gap bundle.

<BuilderExample/>

## Related Topics

* [builder](/reference/custom-resource-helmchart-v2#builder)
* [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap)
* [Air Gap Installation in Existing Clusters with KOTS](/enterprise/installing-existing-cluster-airgapped)

---


# Templating Annotations

This topic describes how to use Replicated KOTS template functions to template annotations for resources and objects based on user-supplied values.

## Overview

It is common for users to need to set custom annotations for a resource or object deployed by your application. For example, you might need to allow your users to provide annotations to apply to a Service or Ingress object in public cloud environments.

For applications installed with Replicated KOTS, you can apply user-supplied annotations to resources or objects by first adding a field to the Replicated Admin Console **Config** page where users can enter one or more annotations. For information about how to add fields on the **Config** page, see [Creating and Editing Configuration Fields](/vendor/admin-console-customize-config-screen).

You can then map these user-supplied values from the **Config** page to resources and objects in your release using KOTS template functions. KOTS template functions are a set of custom template functions based on the Go text/template library that can be used to generate values specific to customer environments. The template functions in the Config context return user-supplied values on the **Config** page.

For more information about KOTS template functions in the Config text, see [Config Context](/reference/template-functions-config-context). For more information about the Go library, see [text/template](https://pkg.go.dev/text/template) in the Go documentation.

## About `kots.io/placeholder`

For applications installed with KOTS that use standard Kubernetes manifests, the `kots.io/placeholder` annotation allows you to template annotations in resources and objects without breaking the base YAML or needing to include the annotation key.

The `kots.io/placeholder` annotation uses the format `kots.io/placeholder 'bool' 'string'`. For example:

```yaml
# Example manifest file

annotations:
  kots.io/placeholder: |-
    repl{{ ConfigOption "additional_annotations" | nindent 4 }}
```

:::note
For Helm chart-based applications installed with KOTS, Replicated recommends that you map user-supplied annotations to the Helm chart `values.yaml` file using the Replicated HelmChart custom resource, rather than using `kots.io/placeholder`. This allows you to access user-supplied values in your Helm chart without needing to include KOTS template functions directly in the Helm chart templates.

For an example, see [Map User-Supplied Annotations to Helm Chart Values](#map-user-supplied-annotations-to-helm-chart-values) below.
:::

## Annotation Templating Examples

This section includes common examples of templating annotations in resources and objects to map user-supplied values.

For additional examples of how to map values to Helm chart-based applications, see [Applications](https://github.com/replicatedhq/platform-examples/tree/main/applications) in the platform-examples repository in GitHub.

### Map Multiple Annotations from a Single Configuration Field

You can map one or more annotations from a single `textarea` field on the **Config** page. The `textarea` type defines multi-line text input and supports properties such as `rows` and `cols`. For more information, see [textarea](/reference/custom-resource-config#textarea) in _Config_.

For example, the following Config custom resource adds an `ingress_annotations` field of type `textarea`: 

```yaml
# Config custom resource

apiVersion: kots.io/v1beta1
kind: Config
metadata:
  name: config
spec:
  groups:
  - name: ingress_settings
    title: Ingress Settings
    description: Configure Ingress
    items:
    - name: ingress_annotations
      type: textarea
      title: Ingress Annotations
      help_text: See your cloud provider’s documentation for the required annotations.
```

On the **Config** page, users can enter one or more key value pairs in the `ingress_annotations` field, as shown in the example below:

![Config page with custom annotations in a Ingress Annotations field](/images/config-map-annotations.png)

[View a larger version of this image](/images/config-map-annotations.png)

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-annotation
  annotations:
    kots.io/placeholder: |-
      repl{{ ConfigOption "ingress_annotations" | nindent 4 }}
```

During installation, KOTS renders the YAML with the multi-line input from the configuration field as shown below:

```yaml
# Rendered Ingress object
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-annotation
  annotations:
    kots.io/placeholder: |-
    
    key1: value1
    key2: value2
    key3: value3
```

### Map Annotations from Multiple Configuration Fields

You can specify multiple annotations using the same `kots.io/placeholder` annotation.

For example, the following Ingress object includes ConfigOption template functions that render the user-supplied values for the `ingress_annotation` and `ingress_hostname` fields: 

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-annotation
  annotations:
    kots.io/placeholder: |-
      repl{{ ConfigOption "ingress_annotation" | nindent 4 }}
      repl{{ printf "my.custom/annotation.ingress.hostname: %s" (ConfigOption "ingress_hostname") | nindent 4 }}
```

During installation, KOTS renders the YAML as shown below:

```yaml
# Rendered Ingress object

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-annotation
  annotations:
    kots.io/placeholder: |-
    
    key1: value1
    my.custom/annotation.ingress.hostname: example.hostname.com
```

### Map User-Supplied Value to a Key

You can map a user-supplied value from the **Config** page to a pre-defined annotation key.

For example, in the following Ingress object, `my.custom/annotation.ingress.hostname` is the key for the templated annotation. The annotation also uses the ConfigOption template function to map the user-supplied value from a `ingress_hostname` configuration field:

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-annotation
  annotations:
    kots.io/placeholder: |-
      repl{{ printf "my.custom/annotation.ingress.hostname: %s" (ConfigOption "ingress_hostname") | nindent 4 }}
```

During installation, KOTS renders the YAML as shown below:

```yaml
# Rendered Ingress object

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-annotation
  annotations:
    kots.io/placeholder: |-
    
    my.custom/annotation.ingress.hostname: example.hostname.com
```

### Include Conditional Statements in Templated Annotations

You can include or exclude templated annotations based on a conditional statement.

For example, the following Ingress object includes a conditional statement for `kots.io/placeholder` that renders `my.custom/annotation.class: somevalue` if the user enables a `custom_annotation` field on the **Config** page:

```yaml
apiVersion: v1
kind: Ingress
metadata:
  name: myapp
  labels:
    app: myapp
annotations:
  kots.io/placeholder: |-
    repl{{if ConfigOptionEquals "custom_annotation" "1" }}repl{{ printf "my.custom/annotation.class: somevalue" | nindent 4 }}repl{{end}}
spec:
...    
```

During installation, if the user enables the `custom_annotation` configuration field, KOTS renders the YAML as shown below:

```yaml
# Rendered Ingress object

apiVersion: v1
kind: Ingress
metadata:
  name: myapp
  labels:
    app: myapp
  annotations:
    kots.io/placeholder: |-
    my.custom/annotation.class: somevalue
spec:    
...  
```

Alternatively, if the condition evaluates to false, the annotation does not appear in the rendered YAML:

```yaml
apiVersion: v1
kind: Ingress
metadata:
  name: myapp
  labels:
    app: myapp
  annotations:
    kots.io/placeholder: |-
spec:    
...  
```

### Map User-Supplied Annotations to Helm Chart Values

For Helm chart-based applications installed with KOTS, Replicated recommends that you map user-supplied annotations to the Helm chart `values.yaml` file, rather than using `kots.io/placeholder`. This allows you to access user-supplied values in your Helm chart without needing to include KOTS template functions directly in the Helm chart templates.

To map user-supplied annotations from the **Config** page to the Helm chart `values.yaml` file, you use the `values` field of the Replicated HelmChart custom resource. For more information, see [values](/reference/custom-resource-helmchart-v2#values) in _HelmChart v2_.

For example, the following HelmChart custom resource uses a ConfigOption template function in `values.services.myservice.annotations` to map the value of a configuration field named `additional_annotations`:

```yaml
# HelmChart custom resource

apiVersion: kots.io/v1beta2
kind: HelmChart
metadata:
  name: myapp
spec:
  values:
    services:
      myservice:
        annotations: repl{{ ConfigOption "additional_annotations" | nindent 10 }}
```

The `values.services.myservice.annotations` field in the HelmChart custom resource corresponds to a `services.myservice.annotations` field in the `value.yaml` file of the application Helm chart, as shown in the example below:

```yaml
# Helm chart values.yaml

services:
  myservice:
    annotations: {}
```

During installation, the ConfigOption template function in the HelmChart custom resource renders the user-supplied values from the `additional_annotations` configuration field.

Then, KOTS replaces the value in the corresponding field in the `values.yaml` in the chart archive, as shown in the example below.

```yaml
# Rendered Helm chart values.yaml

services:
  myservice:
    annotations:
      key1: value1
```

In your Helm chart templates, you can access these values from the `values.yaml` file to apply the user-supplied annotations to the target resources or objects. For information about how to access values from a `values.yaml` file, see [Values Files](https://helm.sh/docs/chart_template_guide/values_files/) in the Helm documentation.

---


import RestoreTable from "../partials/snapshots/_restoreTable.mdx"
import NoEcSupport from "../partials/snapshots/_limitation-no-ec-support.mdx"
import RestoreTypes from "../partials/snapshots/_restore-types.mdx"
import Dr from "../partials/snapshots/_limitation-dr.mdx"
import Os from "../partials/snapshots/_limitation-os.mdx"
import InstallMethod from "../partials/snapshots/_limitation-install-method.mdx"
import CliRestores from "../partials/snapshots/_limitation-cli-restores.mdx"

# About Backup and Restore with Snapshots

This topic provides an introduction to the Replicated KOTS snapshots feature for backup and restore. It describes how vendors enable snapshots, the type of data that is backed up, and how to troubleshoot issues for enterprise users.

:::note
<NoEcSupport/>
:::

## Overview

An important part of the lifecycle of an application is backup and restore. You can enable Replicated KOTS snapshots to support backup and restore for existing cluster installations with KOTS and Replicated kURL installations.

When snapshots is enabled for your application, your customers can manage and perform backup and restore from the Admin Console or KOTS CLI.

Snapshots uses the Velero open source project as the backend to back up Kubernetes manifests and persistent volumes. Velero is a mature, fully-featured application. For more information, see the [Velero documentation](https://velero.io/docs/).

In addition to the default functionality that Velero provides, KOTS exposes hooks that let you inject scripts that can execute both before and after a backup, and before and after a restore. For more information, see [Configuring Backup and Restore Hooks for Snapshots](/vendor/snapshots-hooks).

### Limitations and Considerations

* <NoEcSupport/>

- The snapshots feature is available only for licenses with the **Allow Snapshots** option enabled. For more information, see [Creating and Managing Customers](/vendor/releases-creating-customer).

- Snapshots are useful for rollback and disaster recovery scenarios. They are not intended to be used for application migration.

- <Dr/>

- <Os/>

- <InstallMethod/>

- <CliRestores/>

- Removing data from the snapshot storage itself results in data corruption and the loss of snapshots. Instead, use the **Snapshots** tab in the Admin Console to cleanup and remove snapshots.

- Snapshots does not support Amazon Simple Storage Service (Amazon S3) buckets that have a bucket policy requiring the server-side encryption header. If you want to require server-side encryption for objects, you can enable default encryption on the bucket instead. For more information about Amazon S3, see the [Amazon S3](https://docs.aws.amazon.com/s3/?icmpid=docs_homepage_featuredsvcs) documentation.

### Velero Version Compatibility

The following table lists which versions of Velero are compatible with each version of KOTS. For more information, see the [Velero documentation](https://velero.io/docs/).

| KOTS version | Velero version |
|------|-------------|
| 1.15 to 1.20.2 | 1.2.0 |
| 1.20.3 to 1.94.0 | 1.5.1 through 1.9.x |
| 1.94.1 and later | 1.6.x through 1.12.x |

## About Backups

This section describes the types of backups that are supported with snapshots. For information about how to configure backup storage destinations for snapshots, see the [Configuring Backup Storage](/enterprise/snapshots-velero-cli-installing) section.

### Application and Admin Console (Full) Backups

Full backups (also referred to as _instance_ backups) include the KOTS Admin Console and all application data, including application volumes and manifest files.

For clusters created with Replicated kURL, full backups also back up the Docker registry, which is required for air gapped installations.

If you manage multiple applications with the Admin Console, data from all applications that support backups is included in a full backup. To be included in full backups, each application must include a manifest file with `kind: Backup` and `apiVersion: velero.io/v1`, which you can check for in the Admin Console.

Full backups are recommended because they support all types of restores. For example, you can restore both the Admin Console and application from a full backup to a new cluster in disaster recovery scenarios. Or, you can use a full backup to restore only application data for the purpose of rolling back after deploying a new version of an application.

### Application-Only (Partial) Backups

Partial backups back up the application volumes and manifest files only. Partial backups do not back up the KOTS Admin Console.

Partial backups can be useful if you need to roll back after deploying a new application version. Partial backups of the application only _cannot_ be restored to a new cluster, and are therefore not useable for disaster recovery scenarios.

### Backup Storage Destinations

For disaster recovery, backups should be configured to use a storage destination that exists outside of the cluster. This is especially true for installations in clusters created with Replicated kURL, because the default storage location on these clusters is internal.

You can use a storage provider that is compatible with Velero as the storage destination for backups created with the Replicated snapshots feature. For a list of the compatible storage providers, see [Providers](https://velero.io/docs/v1.9/supported-providers/) in the Velero documentation.

You initially configure backups on a supported storage provider backend using the KOTS CLI. If you want to change the storage destination after the initial configuration, you can use the the **Snapshots** page in the Admin Console, which has built-in support for the following storage destinations:

- Amazon Web Services (AWS)
- Google Cloud Provider (GCP)
- Microsoft Azure
- S3-Compatible
- Network File System (NFS)
- Host Path

kURL installers that include the Velero add-on also include a locally-provisioned object store. By default, kURL clusters are preconfigured in the Admin Console to store backups in the locally-provisioned object store. This object store is sufficient for only rollbacks and downgrades and is not a suitable configuration for disaster recovery. Replicated recommends that you configure a snapshots storage destination that is external to the cluster in the Admin Console for kURL clusters.

For information about how to configure backup storage destinations for snapshots, see the [Configuring Backup Storage](/enterprise/snapshots-velero-cli-installing) section.

### What Data is Backed Up?

Full backups include the Admin Console and all application data, including KOTS-specific object-stored data. For Replicated kURL installations, this also backs up the Docker registry, which is required for air gapped installations.

#### Other Object-Stored Data

For kURL clusters, you might be using object-stored data that is not specific to the kURL KOTS add-on. 

For object-stored data that is not KOTS-specific and does not use persistentVolumeClaims (PVCs), you must write custom backup and restore hooks to enable back ups for that object-stored data. For example, Rook and Ceph do not use PVCs and so require custom backup and restore hooks. For more information about writing custom hooks, see [Configuring Backup and Restore Hooks for Snapshots](snapshots-hooks).

#### Pod Volume Data

Replicated supports only the restic backup program for pod volume data.

By default, Velero requires that you opt-in to have pod volumes backed up. In the Backup resource that you configure to enable snapshots, you must annotate each specific volume that you want to back up. For more information about including and excluding pod volumes, see [Configuring Snapshots](/vendor/snapshots-configuring-backups).

## About Restores {#restores}

<RestoreTypes/>

When you restore an application with snapshots, KOTS first deletes the selected application. All existing application manifests are removed from the cluster, and all `PersistentVolumeClaims` are deleted. This action is not reversible.

Then, the restore process redeploys all of the application manifests. All Pods are given an extra `initContainer` and an extra directory named `.velero`, which are used for restore hooks. For more information about the restore process, see [Restore Reference](https://velero.io/docs/v1.9/restore-reference/) in the Velero documentation.

When you restore the Admin Console only, no changes are made to the application.

For information about how to restore using the Admin Console or the KOTS CLI, see [Restoring from Backups](/enterprise/snapshots-restoring-full).

## Using Snapshots

This section provides an overview of how vendors and enterprise users can configure and use the snapshots feature.

### How to Enable Snapshots for Your Application

To enable the snapshots backup and restore feature for your users, you must:

- Have the snapshots entitlement enabled in your Replicated vendor account. For account entitlements, contact the Replicated TAM team.
- Define a manifest for creating backups. See [Configuring Snapshots](snapshots-configuring-backups).
- When needed, configure backup and restore hooks. See [Configuring Backup and Restore Hooks for Snapshots](snapshots-hooks).
- Enable the **Allow Snapshot** option in customer licenses. See [Creating and Managing Customers](releases-creating-customer).

### Understanding Backup and Restore for Users {#how-users}

After vendors enable backup and restore, enterprise users install Velero and configure a storage destination in the Admin Console. Then users can create backups manually or schedule automatic backups.

Replicated recommends advising your users to make full backups for disaster recovery purposes. Additionally, full backups give users the flexibility to do a full restore, a partial restore (application only), or restore just the Admin Console.

From a full backup, users restore using the KOTS CLI or the Admin Console as indicated in the following table:

<RestoreTable/>

Partial backups are not recommended as they are a legacy feature and only back up the application volumes and manifests. Partial backups can be restored only from the Admin Console.

### Troubleshooting Snapshots

To support end users with backup and restore, use the following resources:

- To help troubleshoot error messages, see [Troubleshooting Snapshots](/enterprise/snapshots-troubleshooting-backup-restore). 

- Review the Limitations and Considerations section to make sure an end users system is compliant.

- Check that the installed Velero version and KOTS version are compatible.

---


import KurlDefinition from "../partials/kurl/_kurl-definition.mdx"
import Installers from "../partials/kurl/_installers.mdx"
import KurlAvailability from "../partials/kurl/_kurl-availability.mdx"

# Introduction to kURL

This topic provides an introduction to the Replicated kURL installer, including information about kURL specifications and installations.

:::note
The Replicated KOTS entitlement is required to install applications with KOTS and kURL. For more information, see [Pricing](https://www.replicated.com/pricing) on the Replicated website.
:::

<KurlAvailability/>

## Overview

<KurlDefinition/>

### kURL Installers

<Installers/>

To distribute a kURL installer alongside your application, you can promote the installer to a channel or include the installer as a manifest file within a given release. For more information about creating kURL installers, see [Creating a kURL Installer](/vendor/packaging-embedded-kubernetes).

### kURL Installations

To install with kURL, users run a kURL installation script on their VM or bare metal server to provision a cluster.

When the KOTS add-on is included in the kURL installer spec, the kURL installation script installs the KOTS CLI and KOTS Admin Console in the cluster. After the installation script completes, users can access the Admin Console at the URL provided in the ouput of the command to configure and deploy the application with KOTS.

The following shows an example of the output of the kURL installation script:

```bash
        Installation
          Complete ✔

Kotsadm: http://10.128.0.35:8800
Login with password (will not be shown again): 3Hy8WYYid

This password has been set for you by default. It is recommended that you change
this password; this can be done with the following command:
kubectl kots reset-password default
```

kURL installations are supported in online (internet-connected) and air gapped environments.

For information about how to install applications with kURL, see [Online Installation with kURL](/enterprise/installing-kurl).

## About the Open Source kURL Documentation

The open source documentation for the kURL project is available at [kurl.sh](https://kurl.sh/docs/introduction/).

The open source kURL documentation contains additional information including kURL installation options, kURL add-ons, and procedural content such as how to add and manage nodes in kURL clusters. Software vendors can use the open source kURL documentation to find detailed reference information when creating kURL installer specs or testing installation.

---


import FirewallOpeningsIntro from "../partials/install/_firewall-openings-intro.mdx"
import KurlAvailability from "../partials/kurl/_kurl-availability.mdx"

# kURL Installation Requirements

<KurlAvailability/>

This topic lists the installation requirements for Replicated kURL. Ensure that the installation environment meets these requirements before attempting to install.

## Minimum System Requirements

* 4 CPUs or equivalent per machine
* 8GB of RAM per machine
* 40GB of disk space per machine
* TCP ports 2379, 2380, 6443, 6783, and 10250 open between cluster nodes
* UDP port 8472 open between cluster nodes

  :::note
  If the Kubernetes installer specification uses the deprecated kURL [Weave add-on](https://kurl.sh/docs/add-ons/weave), UDP ports 6783 and 6784 must be open between cluster nodes. Reach out to your software vendor for more information.
  :::

* Root access is required
* (Rook Only) The Rook add-on version 1.4.3 and later requires block storage on each node in the cluster. For more information about how to enable block storage for Rook, see [Block Storage](https://kurl.sh/docs/add-ons/rook/#block-storage) in _Rook Add-On_ in the kURL documentation.

## Additional System Requirements

You must meet the additional kURL system requirements when applicable:

- **Supported Operating Systems**: For supported operating systems, see [Supported Operating Systems](https://kurl.sh/docs/install-with-kurl/system-requirements#supported-operating-systems) in the kURL documentation.

- **kURL Dependencies Directory**: kURL installs additional dependencies in the directory /var/lib/kurl and the directory requirements must be met. See [kURL Dependencies Directory](https://kurl.sh/docs/install-with-kurl/system-requirements#kurl-dependencies-directory) in the kURL documentation.

- **Networking Requirements**: Networking requirements include firewall openings, host firewalls rules, and port availability. See [Networking Requirements](https://kurl.sh/docs/install-with-kurl/system-requirements#networking-requirements) in the kURL documentation.

- **High Availability Requirements**: If you are operating a cluster with high availability, see [High Availability Requirements](https://kurl.sh/docs/install-with-kurl/system-requirements#high-availability-requirements) in the kURL documentation.

- **Cloud Disk Performance**: For a list of cloud VM instance and disk combinations that are known to provide sufficient performance for etcd and pass the write latency preflight, see [Cloud Disk Performance](https://kurl.sh/docs/install-with-kurl/system-requirements#cloud-disk-performance) in the kURL documentation.

## Firewall Openings for Online Installations with kURL {#firewall}

<FirewallOpeningsIntro/>

<table>
  <tr>
      <th width="50%">Domain</th>
      <th>Description</th>
  </tr>
  <tr>
      <td>Docker Hub</td>
      <td><p>Some dependencies of KOTS are hosted as public images in Docker Hub. The required domains for this service are `index.docker.io`, `cdn.auth0.com`, `*.docker.io`, and `*.docker.com.`</p></td>
  </tr>
  <tr>
      <td>`proxy.replicated.com` &#42;</td>
      <td><p>Private Docker images are proxied through `proxy.replicated.com`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `proxy.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L52-L57) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`replicated.app`</td>
      <td><p>Upstream application YAML and metadata is pulled from `replicated.app`. The current running version of the application (if any), as well as a license ID and application ID to authenticate, are all sent to `replicated.app`. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p>For the range of IP addresses for `replicated.app`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L60-L65) in GitHub.</p></td>
  </tr>
  <tr>
      <td>`registry.replicated.com` &#42;&#42;</td>
      <td><p>Some applications host private images in the Replicated registry at this domain. The on-prem docker client uses a license ID to authenticate to `registry.replicated.com`. This domain is owned by Replicated, Inc which is headquartered in Los Angeles, CA.</p><p> For the range of IP addresses for `registry.replicated.com`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L20-L25) in GitHub.</p></td>
  </tr>
  <tr>
     <td><p>`k8s.kurl.sh`</p><p>`s3.kurl.sh`</p></td>
     <td><p>kURL installation scripts and artifacts are served from [kurl.sh](https://kurl.sh). An application identifier is sent in a URL path, and bash scripts and binary executables are served from kurl.sh. This domain is owned by Replicated, Inc., which is headquartered in Los Angeles, CA.</p><p> For the range of IP addresses for `k8s.kurl.sh`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L34-L39) in GitHub.</p><p> The range of IP addresses for `s3.kurl.sh` are the same as IP addresses for the `kurl.sh` domain. For the range of IP address for `kurl.sh`, see [replicatedhq/ips](https://github.com/replicatedhq/ips/blob/main/ip_addresses.json#L28-L31) in GitHub.</p></td>
  </tr>
  <tr>
     <td>`amazonaws.com`</td>
     <td>`tar.gz` packages are downloaded from Amazon S3 during installations with kURL. For information about dynamically scraping the IP ranges to allowlist for accessing these packages, see [AWS IP address ranges](https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html#aws-ip-download) in the AWS documentation.</td>
  </tr>
</table>

&#42; Required only if the application uses the [Replicated proxy registry](/vendor/private-images-about).

&#42;&#42; Required only if the application uses the [Replicated registry](/vendor/private-images-replicated).

---


import KurlAbout from "../partials/install/_kurl-about.mdx"
import IntroEmbedded from "../partials/install/_intro-embedded.mdx"
import PrereqsEmbeddedCluster from "../partials/install/_prereqs-embedded-cluster.mdx"
import HaLoadBalancerPrereq from "../partials/install/_ha-load-balancer-prereq.mdx"
import LicenseFile from "../partials/install/_license-file-prereq.mdx"
import HAStep from "../partials/install/_embedded-ha-step.mdx"
import LoginPassword from "../partials/install/_embedded-login-password.mdx"
import AppNameUI from "../partials/install/_placeholder-app-name-UI.mdx"
import KurlAvailability from "../partials/kurl/_kurl-availability.mdx"

# Online Installation with kURL

<KurlAvailability/>

<IntroEmbedded/>

<KurlAbout/>

## Prerequisites

Complete the following prerequisites:

<PrereqsEmbeddedCluster/>

<LicenseFile/>

<HaLoadBalancerPrereq/>

## Install {#install-app}

To install an application with kURL:

1. Run one of the following commands to create the cluster with the kURL installer:

     * For a regular installation, run:

       ```bash
       curl -sSL https://k8s.kurl.sh/APP_NAME | sudo bash
       ```
    
     * For high availability mode:

       ```bash
       curl -sSL https://k8s.kurl.sh/APP_NAME | sudo bash -s ha
       ```
      
   Replace:

   <AppNameUI/>

1. <HAStep/> 

1. <LoginPassword/>

1. Go to the address provided in the `Kotsadm` field in the output of the installation command. For example, `Kotsadm: http://34.171.140.123:8800`.

1. On the Bypass Browser TLS warning page, review the information about how to bypass the browser TLS warning, and then click **Continue to Setup**.

1. On the HTTPS page, do one of the following:

    - To use the self-signed TLS certificate only, enter the hostname (required) if you are using the identity service. If you are not using the identity service, the hostname is optional. Click **Skip & continue**.
    - To use a custom certificate only, enter the hostname (required) if you are using the identity service. If you are not using the identity service, the hostname is optional. Then upload a private key and SSL certificate to secure communication between your browser and the Admin Console. Click **Upload & continue**.

1. Log in to the Admin Console with the password that was provided in the `Login with password (will not be shown again):` field in the output of the installation command.

1. Upload your license file.

1. On the **Preflight checks** page, the application-specific preflight checks run automatically. Preflight checks  are conformance tests that run against the target namespace and cluster to ensure that the environment meets the minimum requirements to support the application. Click **Deploy**.

    :::note
    Replicated recommends that you address any warnings or failures, rather than dismissing them. Preflight checks help ensure that your environment meets the requirements for application deployment.
    :::
    
1. (Minimal RBAC Only) If you are installing with minimal role-based access control (RBAC), KOTS recognizes if the preflight checks failed due to insufficient privileges. When this occurs, a kubectl CLI preflight command displays that lets you manually run the preflight checks. The Admin Console then automatically displays the results of the preflight checks. Click **Deploy**.

    ![kubectl CLI preflight command](/images/kubectl-preflight-command.png)

    [View a larger version of this image](/images/kubectl-preflight-command.png)

    The Admin Console dashboard opens.   

    On the Admin Console dashboard, the application status changes from Missing to Unavailable while the Deployment is being created. When the installation is complete, the status changes to Ready.

    ![Admin console dashboard showing ready status](/images/gitea-ec-ready.png)

    [View a larger version of this image](/images/gitea-ec-ready.png)

1. (Recommended) Change the Admin Console login password:
   1. Click the menu in the top right corner of the Admin Console, then click **Change password**.
   1. Enter a new password in the dialog, and click **Change Password** to save.

   Replicated strongly recommends that you change the password from the default provided during installation in a kURL cluster. For more information, see [Changing an Admin Console Password](auth-changing-passwords).

1. Add primary and secondary nodes to the cluster. You might add nodes to either meet application requirements or to support your usage of the application. See [Adding Nodes to Embedded Clusters](cluster-management-add-nodes).


---


import KurlAbout from "../partials/install/_kurl-about.mdx"
import IntroEmbedded from "../partials/install/_intro-embedded.mdx"
import IntroAirGap from "../partials/install/_intro-air-gap.mdx"
import PrereqsEmbeddedCluster from "../partials/install/_prereqs-embedded-cluster.mdx"
import HaLoadBalancerPrereq from "../partials/install/_ha-load-balancer-prereq.mdx"
import AirGapLicense from "../partials/install/_airgap-license-download.mdx"
import BuildAirGapBundle from "../partials/install/_airgap-bundle-build.mdx"
import DownloadAirGapBundle from "../partials/install/_airgap-bundle-download.mdx"
import ViewAirGapBundle from "../partials/install/_airgap-bundle-view-contents.mdx"
import LicenseFile from "../partials/install/_license-file-prereq.mdx"
import HAStep from "../partials/install/_embedded-ha-step.mdx"
import LoginPassword from "../partials/install/_embedded-login-password.mdx"
import DownloadKurlBundle from "../partials/install/_download-kurl-bundle.mdx"
import ExtractKurlBundle from "../partials/install/_extract-kurl-bundle.mdx"
import KurlAvailability from "../partials/kurl/_kurl-availability.mdx"

# Air Gap Installation with kURL

<KurlAvailability/>

<IntroEmbedded/>

<IntroAirGap/>

<KurlAbout/>

## Prerequisites

Complete the following prerequisites:

<PrereqsEmbeddedCluster/>

<HaLoadBalancerPrereq/>

## Install {#air-gap}

To install an application with kURL:

1. Download the customer license:

   <AirGapLicense/>

1. Go the channel where the target release was promoted to build and download the air gap bundle for the release:

   <BuildAirGapBundle/>

1. <DownloadAirGapBundle/>

1. <ViewAirGapBundle/>

1. Download the `.tar.gz` air gap bundle for the kURL installer, which includes the components needed to run the kURL cluster and install the application with KOTS. kURL air gap bundles can be downloaded from the channel where the given release is promoted:

    * To download the kURL air gap bundle for the Stable channel:
   
      <DownloadKurlBundle/>

    * To download the kURL bundle for channels other than Stable:

        ```bash
        replicated channel inspect CHANNEL
        ```
        Replace `CHANNEL` with the exact name of the target channel, which can include uppercase letters or special characters, such as `Unstable` or `my-custom-channel`.

        In the output of this command, copy the curl command with the air gap URL.

1. <ExtractKurlBundle/>

1. Run one of the following commands to install in air gap mode: 

    - For a regular installation, run:

      ```bash
      cat install.sh | sudo bash -s airgap
      ```

    - For high availability, run:

      ```bash
      cat install.sh | sudo bash -s airgap ha
      ```

1. <HAStep/>

1. <LoginPassword/>

1. Go to the address provided in the `Kotsadm` field in the output of the installation command. For example, `Kotsadm: http://34.171.140.123:8800`.

1. On the Bypass Browser TLS warning page, review the information about how to bypass the browser TLS warning, and then click **Continue to Setup**.

1. On the HTTPS page, do one of the following:

    - To use the self-signed TLS certificate only, enter the hostname (required) if you are using the identity service. If you are not using the identity service, the hostname is optional. Click **Skip & continue**.
    - To use a custom certificate only, enter the hostname (required) if you are using the identity service. If you are not using the identity service, the hostname is optional. Then upload a private key and SSL certificate to secure communication between your browser and the Admin Console. Click **Upload & continue**.

1. Log in to the Admin Console with the password that was provided in the `Login with password (will not be shown again):` field in the output of the installation command.

1. Upload your license file.

1. Upload the `.airgap` bundle for the release that you downloaded in an earlier step.

1. On the **Preflight checks** page, the application-specific preflight checks run automatically. Preflight checks  are conformance tests that run against the target namespace and cluster to ensure that the environment meets the minimum requirements to support the application. Click **Deploy**.

    :::note
    Replicated recommends that you address any warnings or failures, rather than dismissing them. Preflight checks help ensure that your environment meets the requirements for application deployment.
    :::
    
1. (Minimal RBAC Only) If you are installing with minimal role-based access control (RBAC), KOTS recognizes if the preflight checks failed due to insufficient privileges. When this occurs, a kubectl CLI preflight command displays that lets you manually run the preflight checks. The Admin Console then automatically displays the results of the preflight checks. Click **Deploy**.

    ![kubectl CLI preflight command](/images/kubectl-preflight-command.png)

    [View a larger version of this image](/images/kubectl-preflight-command.png)

    The Admin Console dashboard opens.   

    On the Admin Console dashboard, the application status changes from Missing to Unavailable while the Deployment is being created. When the installation is complete, the status changes to Ready. 

    ![Admin console dashboard showing ready status](/images/gitea-ec-ready.png)

    [View a larger version of this image](/images/gitea-ec-ready.png)

1. (Recommended) Change the Admin Console login password:
   1. Click the menu in the top right corner of the Admin Console, then click **Change password**.
   1. Enter a new password in the dialog, and click **Change Password** to save.

   Replicated strongly recommends that you change the password from the default provided during installation in a kURL cluster. For more information, see [Changing an Admin Console Password](auth-changing-passwords).

1. Add primary and secondary nodes to the cluster. You might add nodes to either meet application requirements or to support your usage of the application. See [Adding Nodes to Embedded Clusters](cluster-management-add-nodes).

---


import Installers from "../partials/kurl/_installers.mdx"
import KurlAvailability from "../partials/kurl/_kurl-availability.mdx"

# Creating a kURL Installer

<KurlAvailability/>

This topic describes how to create a kURL installer spec in the Replicated Vendor Portal to support installations with Replicated kURL.

For information about creating kURL installers with the Replicated CLI, see [installer create](/reference/replicated-cli-installer-create).

## Overview

<Installers/>

For more information about kURL, see [Introduction to kURL](kurl-about).

## Create an Installer

To distribute a kURL installer alongside your application, you can promote the installer to a channel or include the installer as a manifest file within a given release:

<table>
  <tr>
    <th width="30%">Method</th>
    <th width="70%">Description</th>
  </tr>
  <tr>
    <td><a href="packaging-embedded-kubernetes#channel">Promote the installer to a channel</a></td>
    <td><p>The installer is promoted to one or more channels. All releases on the channel use the kURL installer that is currently promoted to that channel. There can be only one active kURL installer on each channel at a time.</p><p>The benefit of promoting an installer to one or more channels is that you can create a single installer without needing to add a separate installer for each release. However, because all the releases on the channel will use the same installer, problems can occur if all releases are not tested with the given installer.</p></td>
  </tr>
  <tr>
    <td><a href="packaging-embedded-kubernetes#release">Include the installer in a release (Beta)</a></td>
    <td><p>The installer is included as a manifest file in a release. This makes it easier to test the installer and release together. It also makes it easier to know which installer spec customers are using based on the application version that they have installed.</p></td>
  </tr>
</table>

### Promote the Installer to a Channel {#channel}

To promote a kURL installer to a channel:

1. In the [Vendor Portal](https://vendor.replicated.com), click **kURL Installers**.

1. On the **kURL Installers** page, click **Create kURL installer**.

   <img alt="vendor portal kurl installers page" src="/images/kurl-installers-page.png" width="650px"/>

   [View a larger version of this image](/images/kurl-installers-page.png)

1. Edit the file to customize the installer. For guidance on which add-ons to choose, see [Requirements and Recommendations](#requirements-and-recommendations) below.

   You can also go to the landing page at [kurl.sh](https://kurl.sh/) to build an installer then copy the provided YAML:

   <img alt="kurl.sh landing page" src="/images/kurl-build-an-installer.png" width="650px"/>

   [View a larger version of this image](/images/kurl-build-an-installer.png)

1. Click **Save installer**. You can continue to edit your file until it is promoted.

1. Click **Promote**. In the **Promote Installer** dialog that opens, edit the fields:

    <img alt="promote installer dialog" src="/images/promote-installer.png" width="450px"/>

    [View a larger version of this image](/images/promote-installer.png)

    <table>
      <tr>
        <th width="30%">Field</th>
        <th width="70%">Description</th>
      </tr>
      <tr>
        <td>Channel</td>
        <td>Select the channel or channels where you want to promote the installer.</td>
      </tr>
      <tr>
        <td>Version label</td>
        <td>Enter a version label for the installer.</td>
      </tr>
    </table>

1. Click **Promote** again. The installer appears on the **kURL Installers** page.

   To make changes after promoting, create and promote a new installer.

### Include an Installer in a Release (Beta) {#release}

To include the kURL installer in a release:

1. In the [Vendor Portal](https://vendor.replicated.com), click **Releases**. Then, either click **Create Release** to create a new release, or click **Edit YAML** to edit an existing release.

   The YAML editor opens.

1. Create a new file in the release with `apiVersion: cluster.kurl.sh/v1beta1` and `kind: Installer`:

    ```yaml
    apiVersion: cluster.kurl.sh/v1beta1
    kind: Installer
    metadata:
      name: "latest"
    spec:
    
    ```

1. Edit the file to customize the installer. For guidance on which add-ons to choose, see [ kURL Add-on Requirements and Recommendations](#requirements-and-recommendations) below.

   You can also go to the landing page at [kurl.sh](https://kurl.sh/) to build an installer then copy the provided YAML:

   <img alt="kurl.sh landing page" src="/images/kurl-build-an-installer.png" width="650px"/>

   [View a larger version of this image](/images/kurl-build-an-installer.png)

1. Click **Save**. This saves a draft that you can continue to edit until you promote it.

1. Click **Promote**.

   To make changes after promoting, create a new release.  

## kURL Add-on Requirements and Recommendations {#requirements-and-recommendations}

KURL includes several add-ons for networking, storage, ingress, and more. The add-ons that you choose depend on the requirements for KOTS and the unique requirements for your application. For more information about each add-on, see the open source [kURL documentation](https://kurl.sh/docs/introduction/).

When creating a kURL installer, consider the following requirements and guidelines for kURL add-ons:

- You must include the KOTS add-on to support installation with KOTS and provision the KOTS Admin Console. See [KOTS add-on](https://kurl.sh/docs/add-ons/kotsadm) in the kURL documentation.

- To support the use of KOTS snapshots, Velero must be installed in the cluster. Replicated recommends that you include the Velero add-on in your kURL installer so that your customers do not have to manually install Velero.

  :::note
  During installation, the Velero add-on automatically deploys internal storage for backups. The Velero add-on requires the MinIO or Rook add-on to deploy this internal storage. If you include the Velero add-on without either the MinIO add-on or the Rook add-on, installation fails with the following error message: `Only Rook and Longhorn are supported for Velero Internal backup storage`.
  :::

- You must select storage add-ons based on the KOTS requirements and the unique requirements for your application. For more information, see [About Selecting Storage Add-ons](packaging-installer-storage).

- kURL installers that are included in releases must pin specific add-on versions and cannot pin `latest` versions or x-ranges (such as 1.2.x). Pinning specific versions ensures the most testable and reproducible installations. For example, pin `Kubernetes 1.23.0` in your manifest to ensure that version 1.23.0 of Kubernetes is installed. For more information about pinning Kubernetes versions, see [Versions](https://kurl.sh/docs/create-installer/#versions) and [Versioned Releases](https://kurl.sh/docs/install-with-kurl/#versioned-releases) in the kURL open source documentation.

  :::note
  For kURL installers that are _not_ included in a release, pinning specific versions of Kubernetes and Kubernetes add-ons in the kURL installer manifest is not required, though is highly recommended.
  :::

- After you configure a kURL installer, Replicated recommends that you customize host preflight checks to support the installation experience with kURL. Host preflight checks help ensure successful installation and the ongoing health of the cluster. For more information about customizing host preflight checks, see [Customizing Host Preflight Checks for Kubernetes Installers](preflight-host-preflights).

- For installers included in a release, Replicated recommends that you define a preflight check in the release to ensure that the target kURL installer is deployed before the release is installed. For more information about how to define preflight checks, see [Defining Preflight Checks](preflight-defining).
   
   For example, the following preflight check uses the `yamlCompare` analyzer with the `kots.io/installer: "true"` annotation to compare the target kURL installer that is included in the release against the kURL installer that is currently deployed in the customer's environment. For more information about the `yamlCompare` analyzer, see [`yamlCompare`](https://troubleshoot.sh/docs/analyze/yaml-compare/) in the open source Troubleshoot documentation.

    ```yaml
    apiVersion: troubleshoot.sh/v1beta2
    kind: Preflight
    metadata:
      name: installer-preflight-example
    spec:
      analyzers:
        - yamlCompare:
            annotations:
              kots.io/installer: "true"
            checkName: Kubernetes Installer
            outcomes:
              - fail:
                  message: The kURL installer for this version differs from what you have installed. It is recommended that you run the updated kURL installer before deploying this version.
                  uri: https://kurl.sh/my-application
              - pass:
                  message: The kURL installer for this version matches what is currently installed.
    ```

    

---


import Overview from "../partials/preflights/_preflights-sb-about.mdx"

# About Preflight Checks and Support Bundles

This topic provides an introduction to preflight checks and support bundles, which are provided by the Troubleshoot open source project.

For more information, see the [Troubleshoot](https://troubleshoot.sh/docs/collect/) documentation.

## Overview

<Overview/>

Preflight checks and support bundles consist of _collectors_, _redactors_, and _analyzers_ that are defined in a YAML specification. When preflight checks or support bundles are executed, data is collected, redacted, then analyzed to provide insights to users, as illustrated in the following diagram:

![Troubleshoot Workflow Diagram](/images/troubleshoot-workflow-diagram.png)

[View a larger version of this image](/images/troubleshoot-workflow-diagram.png)

For more information about each step in this workflow, see the sections below.

### Collect

During the collection phase, _collectors_ gather information from the cluster, the environment, the application, and other sources.

The data collected depends on the types of collectors that are included in the preflight or support bundle specification. For example, the Troubleshoot project provides collectors that can gather information about the Kubernetes version that is running in the cluster, information about database servers, logs from pods, and more.

For more information, see the [Collect](https://troubleshoot.sh/docs/collect/) section in the Troubleshoot documentation.

### Redact

During the redact phase, _redactors_ censor sensitive customer information from the data before analysis. By default, the following information is automatically redacted:

- Passwords
- API token environment variables in JSON
- AWS credentials
- Database connection strings
- URLs that include usernames and passwords

For Replicated KOTS installations, it is also possible to add custom redactors to redact additional data. For more information, see the [Redact](https://troubleshoot.sh/docs/redact/) section in the Troubleshoot documentation.

### Analyze

During the analyze phase, _analyzers_ use the redacted data to provide insights to users.

For preflight checks, analyzers define the pass, fail, and warning outcomes, and can also display custom messages to the user. For example, you can define a preflight check that fails if the cluster's Kubernetes version does not meet the minimum version that your application supports.

For support bundles, analyzers can be used to identify potential problems and share relevant troubleshooting guidance with users. Additionally, when a support bundle is uploaded to the Vendor Portal, it is extracted and automatically analyzed. The goal of analyzers in support bundles is to surface known issues or hints of what might be a problem to make troubleshooting easier.

For more information, see the [Analyze](https://troubleshoot.sh/docs/analyze/) section in the Troubleshoot documentation.

## Preflight Checks


This section provides an overview of preflight checks, including how preflights are defined and run.

### Overview

Preflight checks let you define requirements for the cluster where your application is installed. When run, preflight checks provide clear feedback to your customer about any missing requirements or incompatibilities in the cluster before they install or upgrade your application. For KOTS installations, preflight checks can also be used to block the deployment of the application if one or more requirements are not met.

Thorough preflight checks provide increased confidence that an installation or upgrade will succeed and help prevent support escalations.

### About Host Preflights {#host-preflights}

_Host preflight checks_ automatically run during [Replicated Embedded Cluster](/vendor/embedded-overview) and [Replicated kURL](/vendor/kurl-about) installations on a VM or bare metal server. The purpose of host preflight checks is to verify that the user's installation environment meets the requirements of the Embedded Cluster or kURL installer, such as checking the number of CPU cores in the system, available disk space, and memory usage. If any of the host preflight checks fail, installation is blocked and a message describing the failure is displayed.

Host preflight checks are separate from any application-specific preflight checks that are defined in the release, which run in the Admin Console before the application is deployed with KOTS. Both Embedded Cluster and kURL have default host preflight checks that are specific to the requirements of the given installer. For kURL installations, it is possible to customize the default host preflight checks.

For more information about the default Embedded Cluster host preflight checks, see [Host Preflight Checks](/vendor/embedded-using#about-host-preflight-checks) in _Using Embedded Cluster_.

For more information about kURL host preflight checks, including information about how to customize the defaults, see [Customizing Host Preflight Checks for kURL](/vendor/preflight-host-preflights).

### Defining Preflights

To add preflight checks for your application, create a Preflight YAML specification that defines the collectors and analyzers that you want to include.

For information about how to add preflight checks to your application, including examples, see [Defining Preflight Checks](preflight-defining).

### Blocking Installation with Required (Strict) Preflights

For applications installed with KOTS, it is possible to block the deployment of a release if a preflight check fails. This is helpful when it is necessary to prevent an installation or upgrade from continuing unless a given requirement is met.

You can add required preflight checks for an application by including `strict: true` for the target analyzer in the preflight specification. For more information, see [Block Installation with Required Preflights](preflight-defining#strict) in _Defining Preflight Checks_.

### Running Preflights

This section describes how users can run preflight checks for KOTS and Helm installations.

#### Replicated Installations

For Replicated installations with Embedded Cluster, KOTS, or kURL, preflight checks run automatically as part of the installation process. The results of the preflight checks are displayed either in the KOTS Admin Console or in the KOTS CLI, depending on the installation method.

Additionally, users can access preflight checks from the Admin Console after installation to view their results and optionally re-run the checks.

The following shows an example of the results of preflight checks displayed in the Admin Console during installation:

![Preflight results in Admin Console](/images/preflight-warning.png)

[View a larger version of this image](/images/preflight-warning.png)

#### Helm Installations

For installations with Helm, the preflight kubectl plugin is required to run preflight checks. The preflight plugin is a client-side utility that adds a single binary to the path. For more information, see [Getting Started](https://troubleshoot.sh/docs/) in the Troubleshoot documentation.

Users can optionally run preflight checks before they run `helm install`. The results of the preflight checks are then displayed through the CLI, as shown in the example below:

![Save output dialog](/images/helm-preflight-save-output.png)

[View a larger version of this image](/images/helm-preflight-save-output.png)

For more information, see [Running Preflight Checks for Helm Installations](preflight-running).

## Support Bundles

This section provides an overview of support bundles, including how support bundles are customized and generated.

### Overview

Support bundles collect and analyze troubleshooting data from customer environments, helping both users and support teams diagnose problems with application deployments.

Support bundles can collect a variety of important cluster-level data from customer environments, such as:
* Pod logs
* Node resources and status
* The status of replicas in a Deployment
* Cluster information
* Resources deployed to the cluster
* The history of Helm releases installed in the cluster

Support bundles can also be used for more advanced use cases, such as checking that a command successfully executes in a pod in the cluster, or that an HTTP request returns a succesful response.

Support bundles then use the data collected to provide insights to users on potential problems or suggested troubleshooting steps. The troubleshooting data collected and analyzed by support bundles not only helps users to self-resolve issues with their application deployment, but also helps reduce the amount of time required by support teams to resolve requests by ensuring they have access to all the information they need up front.

### About Host Support Bundles

For installations on VMs or bare metal servers with [Replicated Embedded Cluster](/vendor/embedded-overview) or [Replicated kURL](/vendor/kurl-about), it is possible to generate a support bundle that includes host-level information to help troubleshoot failures related to host configuration like DNS, networking, or storage problems.

For Embedded Cluster installations, a default spec can be used to generate support bundles that include cluster- and host-level information. See [Generating Host Bundles for Embedded Cluster](/vendor/support-bundle-embedded).

For kURL installations, vendors can customize a host support bundle spec for their application. See [Generating Host Bundles for kURL](/vendor/support-host-support-bundles).

### Customizing Support Bundles

To enable support bundles for your application, add a support bundle YAML specification to a release. An empty support bundle specification automatically includes several default collectors and analzyers. You can also optionally customize the support bundle specification for by adding, removing, or editing collectors and analyzers.

For more information, see [Adding and Customizing Support Bundles](support-bundle-customizing).

### Generating Support Bundles

Users generate support bundles as `tar.gz` files from the command line, using the support-bundle kubectl plugin. Your customers can share their support bundles with your team by sending you the resulting `tar.gz` file.

KOTS users can also generate and share support bundles from the KOTS Admin Console.

For more information, see [Generating Support Bundles](support-bundle-generating).

---


# About the Replicated Proxy Registry

This topic describes how the Replicated proxy registry can be used to grant proxy access to your application's private images or allow pull through access of public images.

## Overview

If your application images are available in a private image registry exposed to the internet such as Docker Hub or Amazon Elastic Container Registry (ECR), then the Replicated proxy registry can grant proxy, or _pull-through_, access to the images without exposing registry credentials to your customers. When you use the proxy registry, you do not have to modify the process that you already use to build and push images to deploy your application.

To grant proxy access, the proxy registry uses the customer licenses that you create in the Replicated vendor portal. This allows you to revoke a customer’s ability to pull private images by editing their license, rather than having to manage image access through separate identity or authentication systems. For example, when a trial license expires, the customer's ability to pull private images is automatically revoked.

The following diagram demonstrates how the proxy registry pulls images from your external registry, and how deployed instances of your application pull images from the proxy registry:

![Proxy registry workflow diagram](/images/private-registry-diagram.png)

[View a larger version of this image](/images/private-registry-diagram-large.png)

## About Enabling the Proxy Registry

The proxy registry requires read-only credentials to your private registry to access your application images. See [Connecting to an External Registry](/vendor/packaging-private-images).

After connecting your registry, the steps the enable the proxy registry vary depending on your application deployment method. For more information, see:
* [Using the Proxy Registry with KOTS Installations](/vendor/private-images-kots)
* [Using the Proxy Registry with Helm Installations](/vendor/helm-image-registry)

## About Allowing Pull-Through Access of Public Images

Using the Replicated proxy registry to grant pull-through access to public images can simplify network access requirements for your customers, as they only need to whitelist a single domain (either `proxy.replicated.com` or your custom domain) instead of multiple registry domains.

For more information about how to pull public images through the proxy registry, see [Connecting to a Public Registry through the Proxy Registry](/vendor/packaging-public-images).

---


import StepCreds from "../partials/proxy-service/_step-creds.mdx"
import StepCustomDomain from "../partials/proxy-service/_step-custom-domain.mdx"

# Using the Proxy Registry with Helm Installations

This topic describes how to use the Replicated proxy registry to proxy images for installations with the Helm CLI. For more information about the proxy registry, see [About the Replicated Proxy Registry](private-images-about).

## Overview

With the Replicated proxy registry, each customer's unique license can grant proxy access to images in an external private registry. To enable the proxy registry for Helm installations, you must create a Secret with `type: kubernetes.io/dockerconfigjson` to authenticate with the proxy registry.

During Helm installations, after customers provide their license ID, a `global.replicated.dockerconfigjson` field that contains a base64 encoded Docker configuration file is automatically injected in the Helm chart values. You can use this `global.replicated.dockerconfigjson` field to create the required pull secret.

For information about how Kubernetes uses the `kubernetes.io/dockerconfigjson` Secret type to authenticate to a private image registry, see [Pull an Image from a Private Registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/) in the Kubernetes documentation.

## Enable the Proxy Registry

This section describes how to enable the proxy registry for applications deployed with Helm, including how to use the `global.replicated.dockerconfigjson` field that is injected during application deployment to create the required pull secret.

To enable the proxy registry:

1. <StepCreds/>

1. <StepCustomDomain/>

1. In your Helm chart templates, create a Kubernetes Secret to evaluate if the `global.replicated.dockerconfigjson` value is set, and then write the rendered value into a Secret on the cluster:

   ```yaml
   # /templates/replicated-pull-secret.yaml

   {{ if .Values.global.replicated.dockerconfigjson }}
   apiVersion: v1
   kind: Secret
   metadata:
     name: replicated-pull-secret
   type: kubernetes.io/dockerconfigjson
   data:
     .dockerconfigjson: {{ .Values.global.replicated.dockerconfigjson }}
   {{ end }}
   ```

   :::note
   If you use the Replicated SDK, do not use `replicated` for the name of the image pull secret because the SDK automatically creates a Secret named `replicated`. Using the same name causes an error.
   :::

1. Ensure that you have a field in your Helm chart values file for your image repository URL, and that any references to the image in your Helm chart access the field from your values file.  

   **Example**:

   ```yaml
   # values.yaml
   ...
   dockerconfigjson: '{{ .Values.global.replicated.dockerconfigjson }}'
   images:
     myapp:
       # Add image URL in the values file
       apiImageRepository: quay.io/my-org/api
       apiImageTag: v1.0.1
   ```
   ```yaml
   # /templates/deployment.yaml

   apiVersion: apps/v1
   kind: Deployment
   metadata:
    name: example
   spec:
     template:
       spec:
         containers:
           - name: api
             # Access the apiImageRepository field from the values file
             image: {{ .Values.images.myapp.apiImageRepository }}:{{ .Values.images.myapp.apiImageTag }}
   ```

1. In your Helm chart templates, add the image pull secret that you created to any manifests that reference the private image:

   ```yaml
   # /templates/example.yaml
   ...
   {{ if .Values.global.replicated.dockerconfigjson }}
   imagePullSecrets:
     - name: replicated-pull-secret
   {{ end }}
   ```

   **Example:**

    ```yaml
    # /templates/deployment.yaml
    ...
    image: "{{ .Values.images.myapp.apiImageRepository }}:{{ .Values.images.myapp.apiImageTag }}"
    {{ if .Values.global.replicated.dockerconfigjson }}
    imagePullSecrets:
      - name: replicated-pull-secret
    {{ end }}
    name: myapp
    ports:
    - containerPort: 3000
      name: http
    ```

1. Package your Helm chart and add it to a release. Promote the release to a development channel. See [Managing Releases with Vendor Portal](releases-creating-releases).

1. Install the chart in a development environment to test your changes:

   1. Create a local `values.yaml` file to override the default external registry image URL with the URL for the image on `proxy.replicated.com`.
   
      The proxy registry URL has the following format: `proxy.replicated.com/proxy/APP_SLUG/EXTERNAL_REGISTRY_IMAGE_URL`
      
      Where:
      * `APP_SLUG` is the slug of your Replicated application.
      * `EXTERNAL_REGISTRY_IMAGE_URL` is the path to the private image on your external registry.

      **Example**
      ```yaml
      # A local values.yaml file
      ...
      images:
        myapp:
          apiImageRepository: proxy.replicated.com/proxy/my-app/quay.io/my-org/api
          apiImageTag: v1.0.1

      ```

      :::note
      If you configured a custom domain for the proxy registry, use the custom domain instead of `proxy.replicated.com`. For more information, see [Using Custom Domains](custom-domains-using).
      :::
   
   1. Log in to the Replicated registry and install the chart, passing the local `values.yaml` file you created with the `--values` flag. See [Installing with Helm](install-with-helm).


    


---


import Deprecated from "../partials/helm/_replicated-deprecated.mdx"
import StepCreds from "../partials/proxy-service/_step-creds.mdx"
import StepCustomDomain from "../partials/proxy-service/_step-custom-domain.mdx"

# Using the Proxy Registry with KOTS Installations

This topic describes how to use the Replicated proxy registry with applications deployed with Replicated KOTS.

## Overview

Replicated KOTS automatically creates the required image pull secret for accessing the Replicated proxy registry during application deployment. When possible, KOTS also automatically rewrites image names in the application manifests to the location of the image at `proxy.replicated.com` or your custom domain.  

### Image Pull Secret

During application deployment, KOTS automatically creates an `imagePullSecret` with `type: kubernetes.io/dockerconfigjson` that is based on the customer license. This secret is used to authenticate with the proxy registry and grant proxy access to private images.

For information about how Kubernetes uses the `kubernetes.io/dockerconfigjson` Secret type to authenticate to a private image registry, see [Pull an Image from a Private Registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/) in the Kubernetes documentation.

### Image Location Patching (Standard Manifests and HelmChart v1)

For applications packaged with standard Kubernetes manifests (or Helm charts deployed with the [HelmChart v1](/reference/custom-resource-helmchart) custom resource), KOTS automatically patches image names to the location of the image at at `proxy.replicated.com` or your custom domain during deployment. If KOTS receives a 401 response when attempting to load image manifests using the image reference from the PodSpec, it assumes that this is a private image that must be proxied through the proxy registry.

KOTS uses Kustomize to patch the `midstream/kustomization.yaml` file to change the image name during deployment to reference the proxy registry. For example, a PodSpec for a Deployment references a private image hosted at `quay.io/my-org/api:v1.0.1`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example
spec:
  template:
    spec:
      containers:
        - name: api
          image: quay.io/my-org/api:v1.0.1
```

When this application is deployed, KOTS detects that it cannot access
the image at quay.io. So, it creates a patch in the `midstream/kustomization.yaml`
file that changes the image name in all manifest files for the application. This causes the container runtime in the cluster to use the proxy registry to pull the images, using the license information provided to KOTS for authentication.

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
bases:
- ../../base
images:
- name: quay.io/my-org/api:v1.0.1
  newName: proxy.replicated.com/proxy/my-kots-app/quay.io/my-org/api
```

## Enable the Proxy Registry

This section describes how to enable the proxy registry for applications deployed with KOTS, including how to ensure that image names are rewritten and that the required image pull secret is provided.

To enable the proxy registry:

1. <StepCreds/>

1. <StepCustomDomain/>

1. Rewrite images names to the location of the image at `proxy.replicated.com` or your custom domain. Also, ensure that the correct image pull secret is provided for all private images. The steps required to configure image names and add the image pull secret vary depending on your application type:

    * **HelmChart v2**: For Helm charts deployed with the[ HelmChart v2](/reference/custom-resource-helmchart-v2) custom resource, configure the HelmChart v2 custom resource to dynamically update image names in your Helm chart and to inject the image pull secret that is automatically created by KOTS. For instructions, see [Configuring the HelmChart Custom Resource v2](/vendor/helm-native-v2-using).

    * **Standard Manifests or HelmChart v1**: For standard manifest-based applications or Helm charts deployed with the [HelmChart v1](/reference/custom-resource-helmchart) custom resource, no additional configuration is required. KOTS automatically rewrites image names and injects image pull secrets during deployment for these application types.

        :::note
        <Deprecated/>
        :::

    * **Kubernetes Operators**: For applications packaged with Kubernetes Operators, KOTS cannot modify pods that are created at runtime by the Operator. To support the use of private images in all environments, the Operator code should use KOTS functionality to determine the image name and image pull secrets for all pods when they are created. For instructions, see [Referencing Images](/vendor/operator-referencing-images) in the _Packaging Kubernetes Operators_ section.

1. If you are deploying Pods to namespaces other than the application namespace, add the namespace to the `additionalNamespaces` attribute of the KOTS Application custom resource. This ensures that KOTS can provision the `imagePullSecret` in the namespace to allow the Pod to pull the image. For instructions, see [Defining Additional Namespaces](operator-defining-additional-namespaces).

---


# Connecting to a Public Registry through the Proxy Registry

This topic describes how to pull images from public registries using the Replicated proxy registry.

For more information about the Replicated proxy registry, see [About the Replicated Proxy Registry](private-images-about).

## Pull Public Images Through the Replicated Proxy Registry

You can use the Replicated proxy registry to pull both public and private images. Using the Replicated proxy registry for public images can simplify network access requirements for your customers, as they only need to whitelist a single domain (either `proxy.replicated.com` or your custom domain) instead of multiple registry domains.

For public images, you need to first configure registry credentials.

To pull public images through the Replicated proxy registry, use the following `docker` command:

```bash
docker pull REPLICATED_PROXY_DOMAIN/proxy/APPSLUG/UPSTREAM_REGISTRY_HOSTNAME/IMAGE:TAG
```
Where:
* `APPSLUG` is your Replicated app slug found on the [app settings page](https://vendor.replicated.com/settings).
* `REPLICATED_PROXY_DOMAIN` is `proxy.replicated.com` or your custom domain. For information about how to set a custom domain for the proxy registry, see [Using Custom Domains](/vendor/custom-domains-using). 
* `UPSTREAM_REGISTRY_HOSTNAME` is the hostname for the public registry where the image is located. If the image is located in a namespace within the registry, include the namespace after the hostname. For example, `quay.io/namespace`.
* `IMAGE` is the image name.
* `TAG` is the image tag.

## Examples

This section includes examples of pulling public images through the Replicated proxy registry.

### Pull Images from DockerHub

The following examples show how to pull public images from DockerHub:

```bash
# DockerHub is the default when no hostname is specified
docker pull proxy.replicated.com/proxy/APPSLUG/busybox
docker pull proxy.replicated.com/proxy/APPSLUG/nginx:1.16.0
```
```bash
# You can also optionally specify docker.io
docker pull proxy.replicated.com/proxy/APPSLUG/docker.io/replicated/replicated-sdk:1.0.0
```

### Pull Images from Other Registries

The following example shows how to pull images from the Amazon ECR Public Gallery:

```bash
docker pull proxy.replicated.com/proxy/APPSLUG/public.ecr.aws/nginx/nginx:latest
```

### Pull Images Using a Custom Domain for the Proxy Registry

The following example shows how to pull a public image when a custom domain is configured for the proxy registry:

```bash
docker pull my.customdomain.io/proxy/APPSLUG/public.ecr.aws/nginx/nginx:latest
```
For information about how to set a custom domain for the proxy registry, see [Using Custom Domains](/vendor/custom-domains-using). 

## Related Topic

[Connecting to an External Registry](packaging-private-images)


---


import Verify from "../partials/replicated-cli/_verify-install.mdx"
import Sudo from "../partials/replicated-cli/_sudo-install.mdx"
import Login from "../partials/replicated-cli/_login.mdx"
import Logout from "../partials/replicated-cli/_logout.mdx"
import AuthToken from "../partials/replicated-cli/_authorize-with-token-note.mdx"

# Installing the Replicated CLI

This topic describes how to install and run the Replicated CLI.

You can use the Replicated CLI to manage your applications with Replicated programmatically, rather than using the Replicated Vendor Portal.

## Prerequisites

Complete the following prerequisites before installing the Replicated CLI:

- Create a vendor account. See [Creating a Vendor Account](/vendor/vendor-portal-creating-account).
- To run on Linux or Mac, install [curl](https://curl.haxx.se/).
- To run through a Docker container, install [docker](https://www.docker.com).

## Install and Run

You can install and run the Replicated CLI in the following environments: 

* Directly on MacOS
* Directly on Linux
* Through Docker (Useful for Windows, GitHub Actions, or computers without sufficient access)

### MacOS

To install and run the latest Replicated CLI on MacOS:

1. Run one of the following commands:

    - With Brew:

      ```shell
      brew install replicatedhq/replicated/cli
      ```

    - Without Brew:

      ```shell
      curl -s https://api.github.com/repos/replicatedhq/replicated/releases/latest \
      | grep "browser_download_url.*darwin_all.tar.gz" \
      | cut -d : -f 2,3 \
      | tr -d \" \
      | wget -O replicated.tar.gz -qi -
      tar xf replicated.tar.gz replicated && rm replicated.tar.gz
      mv replicated /usr/local/bin/replicated
      ```

      <Sudo/>

1. <Verify/> 

1. <Login/>

   <AuthToken/>

1. <Logout/>

### Linux

To install and run the latest Replicated CLI on Linux:

1. Run the following command:

    ```shell
    curl -s https://api.github.com/repos/replicatedhq/replicated/releases/latest \
    | grep "browser_download_url.*linux_amd64.tar.gz" \
    | cut -d : -f 2,3 \
    | tr -d \" \
    | wget -O replicated.tar.gz -qi -
    tar xf replicated.tar.gz replicated && rm replicated.tar.gz
    mv replicated /usr/local/bin/replicated
    ```

    <Sudo/>

1. <Verify/>

1. <Login/>

   <AuthToken/>

1. <Logout/>

### Docker / Windows

Installing in Docker environments requires that you set the `REPLICATED_API_TOKEN` environment variable to authorize the Replicated CLI with an API token. For more information, see [(Optional) Set Environment Variables](#env-var) below.

To install and run the latest Replicated CLI in Docker environments:

1. Generate a service account or user API token in the vendor portal. To create new releases, the token must have `Read/Write` access. See [Generating API Tokens](/vendor/replicated-api-tokens).

1. Get the latest Replicated CLI installation files from the [replicatedhq/replicated repository](https://github.com/replicatedhq/replicated/releases) on GitHub.

    Download and install the files. For simplicity, the usage in the next step is represented assuming that the CLI is downloaded and installed to the desktop.

1. Authorize the Replicated CLI:

     - Through a Docker container:

        ```shell
        docker run \
          -e REPLICATED_API_TOKEN=$TOKEN \
          replicated/vendor-cli --help
        ```
        Replace `TOKEN` with your API token.

     - On Windows:

        ```dos
        docker.exe run \
          -e REPLICATED_API_TOKEN=%TOKEN% \
          replicated/vendor-cli --help
        ```

        Replace `TOKEN` with your API token.

  For more information about the `docker run` command, see [docker run](https://docs.docker.com/engine/reference/commandline/run/) in the Docker documentation.  

## (Optional) Set Environment Variables {#env-var}

The Replicated CLI supports setting the following environment variables:

* **`REPLICATED_API_TOKEN`**: A service account or user API token generated from a vendor portal team or individual account. The `REPLICATED_API_TOKEN` environment variable has the following use cases:

  * To use Replicated CLI commands as part of automation (such as from continuous integration and continuous delivery pipelines), authenticate by providing the `REPLICATED_API_TOKEN` environment variable.

  * To authorize the Replicated CLI when installing and running the CLI in Docker containers.
  
  * Optionally set the `REPLICATED_API_TOKEN` environment variable instead of using the `replicated login` command to authorize the Replicated CLI in MacOS or Linux environments.

* **`REPLICATED_APP`**: The slug of the target application.

  When using the Replicated CLI to manage applications through your vendor account (including channels, releases, customers, or other objects associated with an application), you can set the `REPLICATED_APP` environment variable to avoid passing the application slug with each command.

### `REPLICATED_API_TOKEN`

To set the `REPLICATED_API_TOKEN` environment variable:

1. Generate a service account or user API token in the vendor portal. To create new releases, the token must have `Read/Write` access. See [Generating API Tokens](/vendor/replicated-api-tokens).

1. Set the environment variable, replacing `TOKEN` with the token you generated in the previous step:

    * **MacOs or Linux**:

      ```
      export REPLICATED_API_TOKEN=TOKEN
      ```

    * **Docker**:

      ```
      docker run \
       -e REPLICATED_API_TOKEN=$TOKEN \
       replicated/vendor-cli --help
      ```

    * **Windows**:

      ```
      docker.exe run \
        -e REPLICATED_API_TOKEN=%TOKEN% \
        replicated/vendor-cli --help
      ```

### `REPLICATED_APP`

To set the `REPLICATED_APP` environment variable:

1. In the [vendor portal](https://vendor.replicated.com), go to the **Application Settings** page and copy the slug for the target application. For more information, see [Get the Application Slug](/vendor/vendor-portal-manage-app#slug) in _Managing Application_.

1. Set the environment variable, replacing `APP_SLUG` with the slug for the target application that you retreived in the previous step:

    * **MacOs or Linux**:

      ```
      export REPLICATED_APP=APP_SLUG
      ```

    * **Docker**:

      ```
      docker run \
         -e REPLICATED_APP=$APP_SLUG
         replicated/vendor-cli --help
      ```

    * **Windows**:

      ```
      docker.exe run \
        -e REPLICATED_APP=%APP_SLUG% \
        replicated/vendor-cli --help
      ```


---


# Replicated SDK API

The Replicated SDK provides an API that you can use to embed Replicated functionality in your Helm chart application.

For example, if your application includes a UI where users manage their application instance, then you can use the `/api/v1/app/updates` endpoint to include messages in the UI that encourage users to upgrade when new versions are available. You could also revoke access to the application during runtime when a license expires using the `/api/v1/license/fields` endpoint.

For more information about how to get started with the Replicated SDK, see [About the Replicated SDK](/vendor/replicated-sdk-overview).

For information about how to develop against the Replicated SDK API with mock data, see [Developing Against the Replicated SDK](/vendor/replicated-sdk-development).

## app

### GET /app/info

List details about an application instance, including the app name, location of the Helm chart in the Replicated OCI registry, and details about the current application release that the instance is running. 

```bash
GET http://replicated:3000/api/v1/app/info
```

Response:

```json
{
  "instanceID": "8dcdb181-5cc4-458c-ad95-c0a1563cb0cb",
  "appSlug": "my-app",
  "appName": "My App",
  "appStatus": "ready",
  "helmChartURL": "oci://registry.replicated.com/my-app/beta/my-helm-chart",
  "currentRelease": {
    "versionLabel": "0.1.72",
    "channelID": "2CBDxNwDH1xyYiIXRTjiB7REjKX",
    "channelName": "Beta",
    "createdAt": "2023-05-28T16:31:21Z",
    "releaseNotes": "",
    "helmReleaseName": "my-helm-chart",
    "helmReleaseRevision": 5,
    "helmReleaseNamespace": "my-helm-chart"
  },
  "channelID": "2CBDxNwDH1xyYiIXRTjiB7REjKX",
  "channelName": "Beta",
  "channelSequence": 4,
  "releaseSequence": 30
}
```

### GET /app/status

List details about an application status, including the list of individual resource states and the overall application state. 

```bash
GET http://replicated:3000/api/v1/app/status
```

Response:

```json
{
  "appStatus": {
    "appSlug": "my-app",
    "resourceStates": [
      {
        "kind": "deployment",
        "name": "api",
        "namespace": "default",
        "state": "ready"
      }
    ],
    "updatedAt": "2024-12-19T23:01:52.207162284Z",
    "state": "ready",
    "sequence": 268
  }
}
```

### GET /app/updates

List details about the releases that are available to an application instance for upgrade, including the version label, created timestamp, and release notes.

```bash
GET http://replicated:3000/api/v1/app/updates
```

Response:

```json
[
  {
    "versionLabel": "0.1.15",
    "createdAt": "2023-05-12T15:48:45.000Z",
    "releaseNotes": "Awesome new features!"
  }
]
```

### GET /app/history

List details about the releases that an application instance has installed previously.

```bash
GET http://replicated:3000/api/v1/app/history
```

Response:

```json
{
  "releases": [
    {
      "versionLabel": "0.1.70",
      "channelID": "2CBDxNwDH1xyYiIXRTjiB7REjKX",
      "channelName": "Stable",
      "createdAt": "2023-05-12T17:43:51Z",
      "releaseNotes": "",
      "helmReleaseName": "echo-server",
      "helmReleaseRevision": 2,
      "helmReleaseNamespace": "echo-server-helm"
    }
  ]
}
```

### POST /app/custom-metrics

Send custom application metrics. For more information and examples see [Configuring Custom Metrics](/vendor/custom-metrics).

### PATCH /app/custom-metrics

Send partial custom application metrics for upserting. 

```bash
PATCH http://replicated:3000/api/v1/app/custom-metrics
```
Request: 

```json
{
  "data": {
    "numProjects": 20,
  }
}
```

Response: Status `200` OK

### DELETE /app/custom-metrics/\{metric_name\}

Delete an application custom metric. 

```bash
DELETE http://replicated:3000/api/v1/app/custom-metrics/numProjects
```

Response: Status `204` No Content 

### POST /app/instance-tags

Programmatically set new instance tags or overwrite existing tags. Instance tags are key-value pairs, where the key and the value are strings.

Setting a tag with the `name` key will set the instance's name in the vendor portal.

The `force` parameter defaults to `false`. If `force` is `false`, conflicting pre-existing tags will not be overwritten and the existing tags take precedence. If the `force` parameter is set to `true`, any conflicting pre-existing tags will be overwritten.

To delete a particular tag, set the key's value to an empty string `""`. 

```bash
POST http://replicated:3000/api/v1/app/instance-tags
```
Request: 

```json
{
  "data": {
    "force": false,
    "tags": {
      "name": "my-instance-name",
      "preExistingKey": "will-not-be-overwritten",
      "cpuCores": "10",
      "supportTier": "basic"
    }
  }
}
```

Response: Status `200` OK

## license

### GET /license/info

List details about the license that was used to install, including the license ID, type, the customer name, and the channel the customer is assigned.

```bash
GET http://replicated:3000/api/v1/license/info
```

Response:

```json
{
  "licenseID": "YiIXRTjiB7R...",
  "appSlug": "my-app",
  "channelID": "2CBDxNwDH1xyYiIXRTjiB7REjKX",
  "channelName": "Stable",
  "customerName": "Example Customer",
  "customerEmail": "username@example.com",
  "licenseType": "dev",
  "licenseSequence": 1,
  "isAirgapSupported": false,
  "isGitOpsSupported": false,
  "isIdentityServiceSupported": false,
  "isGeoaxisSupported": false,
  "isSnapshotSupported": false,
  "isSupportBundleUploadSupported": false,
  "isSemverRequired": true,
  "endpoint": "https://replicated.app",
  "entitlements": {
    "expires_at": {
      "title": "Expiration",
      "description": "License Expiration",
      "value": "",
      "valueType": "String"
    },
    "numSeats": {
      "title": "Number of Seats",
      "value": 10,
      "valueType": "Integer"
    }
  }
}
```

### GET /license/fields

List details about all the fields in the license that was used to install, including the field names, descriptions, values, and signatures.

```bash
GET http://replicated:3000/api/v1/license/fields
```

Response:

```json
{
  "expires_at": {
    "name": "expires_at",
    "title": "Expiration",
    "description": "License Expiration",
    "value": "2023-05-30T00:00:00Z",
    "valueType": "String",
    "signature": {
      "v1": "Vs+W7+sF0RA6UrFEJcyHAbC5YCIT67hdsDdqtJTRBd4ZitTe4pr1D/SZg2k0NRIozrBP1mXuTgjQgeI8PyQJc/ctQwZDikIEKFW0sVv0PFPQV7Uf9fy7wRgadfUxkagcCS8O6Tpcm4WqlhEcgiJGvPBki3hZLnMO9Ol9yOepZ7UtrUMVsBUKwcTJWCytpFpvvOLfSNoHxMnPuSgpXumbHZjvdXrJoJagoRDXPiXXKGh02DOr58ncLofYqPzze+iXWbE8tqdFBZc72lLayT1am3MN0n3ejCNWNeX9+CiBJkqMqLLkjN4eugUmU/gBiDtJgFUB2gq8ejVVcohqos69WA=="
    }
  },
  "numSeats": {
    "name": "numSeats",
    "title": "Number of Seats",
    "value": 10,
    "valueType": "Integer",
    "signature": {
      "v1": "UmsYlVr4+Vg5TWsJV6goagWUM4imdj8EUUcdau7wIzfcU0MuZnv3UNVlwVE/tCuROCMcbei6ygjm4j5quBdkAGUyq86BCtohg/SqRsgVoNV6BN0S+tnqJ7w4/nqRVBc2Gsn7wTYNXiszLMkmfeNOrigLgsrtaGJmZ4IsczwI1V5Tr+AMAgrACL/UyLg78Y6EitKFW4qvJ9g5Q8B3uVmT+h9xTBxJFuKTQS6qFcDx9XCu+bKqoSmJDZ8lwgwpJDAiBzIhxiAd66lypHX9cpOg5A7cKEW+FLdaBKQdNRcPHQK2O9QwFj/NKEeCJEufuD3OeV8MSbN2PCehMzbj7tXSww=="
    }
  }
}
```

### GET /license/fields/\{field_name\}

List details about one of the fields in the license that was used to install, including the field name, description, value, and signature.

```bash
GET http://replicated:3000/api/v1/license/fields/\{field_name\}
```

Example request:

```bash
curl replicated:3000/api/v1/license/fields/expires_at
```

Response:

```json
{
  "name": "expires_at",
  "title": "Expiration",
  "description": "License Expiration",
  "value": "2023-05-30T00:00:00Z",
  "valueType": "String",
  "signature": {
    "v1": "c6rsImpilJhW0eK+Kk37jeRQvBpvWgJeXK2MD0YBlIAZEs1zXpmvwLdfcoTsZMOj0lZbxkPN5dPhEPIVcQgrzfzwU5HIwQbwc2jwDrLBQS4hGOKdxOWXnBUNbztsHXMqlAYQsmAhspRLDhBiEoYpFV/8oaaAuNBrmRu/IVAW6ahB4KtP/ytruVdBup3gn1U/uPAl5lhzuBifaW+NDFfJxAXJrhdTxMBxzfdKa6dGmlGu7Ou/xqDU1bNF3AuWoP3C78GzSBQrD1ZPnu/d+nuEjtakKSX3EK6VUisNucm8/TFlEVKUuX7hex7uZ9Of+UgS1GutQXOhXzfMZ7u+0zHXvQ=="
  }
}
```

## Integration

### GET /api/v1/integration/status

Get status of Development Mode. When this mode is enabled, the `app` API will use mock data. This value cannot be set programmatically. It is controlled by the installed license.

```json
{
  "isEnabled": true
}
```

### GET /api/v1/integration/mock-data

Get mock data that is used when Development Mode is enabled.

```json
{
  "appStatus": "ready",
  "helmChartURL": "oci://registry.replicated.com/dev-app/dev-channel/dev-parent-chart",
  "currentRelease": {
    "versionLabel": "0.1.3",
    "releaseNotes": "release notes 0.1.3",
    "createdAt": "2023-05-23T20:58:07Z",
    "deployedAt": "2023-05-23T21:58:07Z",
    "helmReleaseName": "dev-parent-chart",
    "helmReleaseRevision": 3,
    "helmReleaseNamespace": "default"
  },
  "deployedReleases": [
    {
      "versionLabel": "0.1.1",
      "releaseNotes": "release notes 0.1.1",
      "createdAt": "2023-05-21T20:58:07Z",
      "deployedAt": "2023-05-21T21:58:07Z",
      "helmReleaseName": "dev-parent-chart",
      "helmReleaseRevision": 1,
      "helmReleaseNamespace": "default"
    },
    {
      "versionLabel": "0.1.2",
      "releaseNotes": "release notes 0.1.2",
      "createdAt": "2023-05-22T20:58:07Z",
      "deployedAt": "2023-05-22T21:58:07Z",
      "helmReleaseName": "dev-parent-chart",
      "helmReleaseRevision": 2,
      "helmReleaseNamespace": "default"
    },
    {
      "versionLabel": "0.1.3",
      "releaseNotes": "release notes 0.1.3",
      "createdAt": "2023-05-23T20:58:07Z",
      "deployedAt": "2023-05-23T21:58:07Z",
      "helmReleaseName": "dev-parent-chart",
      "helmReleaseRevision": 3,
      "helmReleaseNamespace": "default"
    }
  ],
  "availableReleases": [
    {
      "versionLabel": "0.1.4",
      "releaseNotes": "release notes 0.1.4",
      "createdAt": "2023-05-24T20:58:07Z",
      "deployedAt": "2023-05-24T21:58:07Z",
      "helmReleaseName": "",
      "helmReleaseRevision": 0,
      "helmReleaseNamespace": ""
    },
    {
      "versionLabel": "0.1.5",
      "releaseNotes": "release notes 0.1.5",
      "createdAt": "2023-06-01T20:58:07Z",
      "deployedAt": "2023-06-01T21:58:07Z",
      "helmReleaseName": "",
      "helmReleaseRevision": 0,
      "helmReleaseNamespace": ""
    }
  ]
}
```

### POST /api/v1/integration/mock-data

Programmatically set mock data that is used when Development Mode is enabled. The payload will overwrite the existing mock data. Any data that is not included in the payload will be removed. For example, to remove release data, simply include empty arrays:

```bash
POST http://replicated:3000/api/v1/integration/mock-data
```

Request:

```json
{
  "appStatus": "ready",
  "helmChartURL": "oci://registry.replicated.com/dev-app/dev-channel/dev-parent-chart",
  "currentRelease": {
    "versionLabel": "0.1.3",
    "releaseNotes": "release notes 0.1.3",
    "createdAt": "2023-05-23T20:58:07Z",
    "deployedAt": "2023-05-23T21:58:07Z",
    "helmReleaseName": "dev-parent-chart",
    "helmReleaseRevision": 3,
    "helmReleaseNamespace": "default"
  },
  "deployedReleases": [],
  "availableReleases": []
}
```

Response: Status `201` Created

## Examples

This section provides example use cases for the Replicated SDK API.

### Support Update Checks in Your Application

The `api/v1/app/updates` endpoint returns details about new releases that are available to an instance for upgrade. You could use the `api/v1/app/updates` endpoint to allow your users to easily check for available updates from your application.

Additionally, to make it easier for users to upgrade to new versions of your application, you could provide customer-specific upgrade instructions in your application by injecting values returned by the `/api/v1/license/info` and `/api/vi/app/info` endpoints. 

The following examples show how you could include a page in your application that lists available updates and also provides customer-specific upgrade instructions:  

![a user interface showing a list of available releases](/images/slackernews-update-page.png)
[View a larger version of this image](/images/slackernews-update-page.png)

![user-specific application upgrade instructions displayed in a dialog](/images/slackernews-update-instructions.png)
[View a larger version of this image](/images/slackernews-update-instructions.png)

To use the SDK API to check for available application updates and provide customer-specific upgrade instructions:

1. From your application, call the `api/v1/app/updates` endpoint to return available updates for the application instance. Use the response to display available upgrades for the customer.

   ```bash 
   curl replicated:3000/api/v1/app/updates
   ```

   **Example response**:

    ```json
    [
      {
        "versionLabel": "0.1.15",
        "createdAt": "2023-05-12T15:48:45.000Z",
        "releaseNotes": "Awesome new features!"
      }
    ]
    ```

1. For each available release, add logic that displays the required upgrade commands with customer-specific values. To upgrade, users must first run `helm registry login` to authenticate to the Replicated registry. Then, they can run `helm upgrade`:

    1. Inject customer-specific values into the `helm registry login` command:

      ```bash
      helm registry login REGISTRY_DOMAIN --username EMAIL --password LICENSE_ID
      ```

      The `helm registry login` command requires the following components: 

        * `REGISTRY_DOMAIN`: The domain for the registry where your Helm chart is pushed. The registry domain is either `replicated.registry.com` or a custom domain that you added.
        
        * `EMAIL`: The customer email address is available from the `/api/v1/license/info` endpoint in the `customerEmail` field.
        
        * `LICENSE_ID` The customer license ID is available from the `/api/v1/license/info` endpoint in the `licenseID` field.

    1. Inject customer-specific values into the `helm upgrade` command:

        ```bash
        helm upgrade -n NAMESPACE RELEASE_NAME HELM_CHART_URL
        ```

       The following describes where the values in the `helm upgrade` command are available:

        * `NAMESPACE`: The release namespace is available from the `/api/v1/app/info` endpoint in the `currentRelease.helmReleaseNamespace`
        
        * `RELEASE_NAME`: The release name is available from the `/api/v1/app/info` endpoint in the `currentRelease.helmReleaseName` field.
        
        * `HELM_CHART_URL`: The URL of the Helm chart at the OCI registry is available from the `/api/v1/app/info` endpoint in the `helmChartURL` field.

### Revoke Access at Runtime When a License Expires        

You can use the Replicated SDK API `/api/v1/license/fields/{field_name}` endpoint to revoke a customer's access to your application during runtime when their license expires.

To revoke access to your application when a license expires:

1. In the vendor portal, click **Customers**. Select the target customer and click the **Manage customer** tab. Alternatively, click **+ Create customer** to create a new customer.

1. Under **Expiration policy**:

   1. Enable **Customer's license has an expiration date**.

   1. For **When does this customer expire?**, use the calendar to set an expiration date for the license.

  <img alt="expiration policy field in the manage customer page" src="/images/customer-expiration-policy.png" width="500px"/>

  [View a larger version of this image](/images/customer-expiration-policy.png)

1. Install the Replicated SDK as a standalone component in your cluster. This is called _integration mode_. Installing in integration mode allows you to develop locally against the SDK API without needing to create releases for your application in the vendor portal. See [Developing Against the SDK API](/vendor/replicated-sdk-development).

1. In your application, use the `/api/v1/license/fields/expires_at` endpoint to get the `expires_at` field that you defined in the previous step.

    **Example:**

    ```bash
    curl replicated:3000/api/v1/license/fields/expires_at
    ```

    ```json
    {
      "name": "expires_at",
      "title": "Expiration",
      "description": "License Expiration",
      "value": "2023-05-30T00:00:00Z",
      "valueType": "String",
      "signature": {
        "v1": "c6rsImpilJhW0eK+Kk37jeRQvBpvWgJeXK2M..."
      }
    }
    ```

1. Add logic to your application to revoke access if the current date and time is more recent than the expiration date of the license.

1. (Recommended) Use signature verification in your application to ensure the integrity of the license field. See [Verifying License Field Signatures with the Replicated SDK API](/vendor/licenses-verify-fields-sdk-api).


---


import DependencyYaml from "../partials/replicated-sdk/_dependency-yaml.mdx"
import KotsVerReq from "../partials/replicated-sdk/_kots-version-req.mdx"
import RegistryLogout from "../partials/replicated-sdk/_registry-logout.mdx"
import IntegrationMode from "../partials/replicated-sdk/_integration-mode-install.mdx"

# Installing the Replicated SDK

This topic describes the methods for distributing and installing the Replicated SDK.

It includes information about how to install the SDK alongside Helm charts or Kubernetes manifest-based applications using the Helm CLI or a Replicated installer (Replicated KOTS, kURL, Embedded Cluster). It also includes information about installing the SDK as a standalone component in integration mode.

For information about installing the SDK in air gap mode, see [Installing the SDK in Air Gap Environments](replicated-sdk-airgap).

## Requirement

<KotsVerReq/>

## Install the SDK as a Subchart

When included as a dependency of your application Helm chart, the SDK is installed as a subchart alongside the application.

To install the SDK as a subchart:

1. In your application Helm chart `Chart.yaml` file, add the YAML below to declare the SDK as a dependency. If your application is installed as multiple charts, declare the SDK as a dependency of the chart that customers install first. Do not declare the SDK in more than one chart.

     <DependencyYaml/>

1. Update the `charts/` directory:

    ```
    helm dependency update
    ```
    :::note
    <RegistryLogout/> 
    :::
    
1. Package the Helm chart into a `.tgz` archive:

   ```
   helm package .
   ```  

1. Add the chart archive to a new release. For more information, see [Managing Releases with the CLI](/vendor/releases-creating-cli) or [Managing Releases with the Vendor Portal](/vendor/releases-creating-releases).

1. (Optional) Add a KOTS HelmChart custom resource to the release to support installation with Embedded Cluster, KOTS, or kURL. For more information, see [Configuring the HelmChart Custom Resource v2](/vendor/helm-native-v2-using).

1. Save and promote the release to an internal-only channel used for testing, such as the default Unstable channel.

1. Install the release using Helm or a Replicated installer. For more information, see:
     * [Online Installation with Embedded Cluster](/enterprise/installing-embedded)
     * [Installing with Helm](/vendor/install-with-helm)
     * [Online Installation in Existing Clusters with KOTS](/enterprise/installing-existing-cluster)
     * [Online Installation with kURL](/enterprise/installing-kurl)

1. Confirm that the SDK was installed by seeing that the `replicated` Deployment was created:

    ```
    kubectl get deploy --namespace NAMESPACE
    ```
    Where `NAMESPACE` is the namespace in the cluster where the application and the SDK are installed. 

    **Example output**:

    ```
    NAME         READY   UP-TO-DATE   AVAILABLE   AGE
    my-app       1/1     1            1           35s
    replicated   1/1     1            1           35s
    ```

## Install the SDK Alongside a Kubernetes Manifest-Based Application {#manifest-app}

For applications that use Kubernetes manifest files instead of Helm charts, the SDK Helm chart can be added to a release and then installed by KOTS alongside the application.

<KotsVerReq/>

To add the SDK Helm chart to a release for a Kubernetes manifest-based application:

1. Install the Helm CLI using Homebrew:

    ```
    brew install helm
    ```
    For more information, including alternative installation options, see [Install Helm](https://helm.sh/docs/intro/install/) in the Helm documentation.

1. Download the `.tgz` chart archive for the SDK Helm chart:

    ```
    helm pull oci://registry.replicated.com/library/replicated --version SDK_VERSION
    ```
    Where `SDK_VERSION` is the version of the SDK to install. For a list of available SDK versions, see the [replicated-sdk repository](https://github.com/replicatedhq/replicated-sdk/tags) in GitHub.

    The output of this command is a `.tgz` file with the naming convention `CHART_NAME-CHART_VERSION.tgz`. For example, `replicated-1.3.0.tgz`.

    For more information and additional options, see [Helm Pull](https://helm.sh/docs/helm/helm_pull/) in the Helm documentation.

1. Add the SDK `.tgz` chart archive to a new release. For more information, see [Managing Releases with the CLI](/vendor/releases-creating-cli) or [Managing Releases with the Vendor Portal](/vendor/releases-creating-releases).

    The following shows an example of the SDK Helm chart added to a draft release for a standard manifest-based application:

    ![SDK Helm chart in a draft release](/images/sdk-kots-release.png)
    
    [View a larger version of this image](/images/sdk-kots-release.png)

1. If one was not created automatically, add a KOTS HelmChart custom resource to the release. HelmChart custom resources have `apiVersion: kots.io/v1beta2` and `kind: HelmChart`. 

     **Example:**
  
    ```yaml
    apiVersion: kots.io/v1beta2
    kind: HelmChart
    metadata:
      name: replicated
    spec:
    # chart identifies a matching chart from a .tgz
      chart:
        # for name, enter replicated
        name: replicated
        # for chartversion, enter the version of the
        # SDK Helm chart in the release
        chartVersion: 1.3.0
    ```

     As shown in the example above, the HelmChart custom resource requires the name and version of the SDK Helm chart that you added to the release:
     * **`chart.name`**: The name of the SDK Helm chart is `replicated`. You can find the chart name in the `name` field of the SDK Helm chart `Chart.yaml` file.
     * **`chart.chartVersion`**: The chart version varies depending on the version of the SDK that you pulled and added to the release. You can find the chart version in the `version` field of SDK Helm chart `Chart.yaml` file.

     For more information about configuring the HelmChart custom resource to support KOTS installations, see [About Distributing Helm Charts with KOTS](/vendor/helm-native-about) and [HelmChart v2](/reference/custom-resource-helmchart-v2).

1. Save and promote the release to an internal-only channel used for testing, such as the default Unstable channel.

1. Install the release using a Replicated installer. For more information, see:
     * [Online Installation with Embedded Cluster](/enterprise/installing-embedded)
     * [Online Installation in Existing Clusters with KOTS](/enterprise/installing-existing-cluster)
     * [Online Installation with kURL](/enterprise/installing-kurl)

1. Confirm that the SDK was installed by seeing that the `replicated` Deployment was created:

    ```
    kubectl get deploy --namespace NAMESPACE
    ```
    Where `NAMESPACE` is the namespace in the cluster where the application, the Admin Console, and the SDK are installed. 

    **Example output**:

    ```
    NAME         READY   UP-TO-DATE   AVAILABLE   AGE
    kotsadm      1/1     1            1           112s
    my-app       1/1     1            1           28s
    replicated   1/1     1            1           27s
    ```

## Install the SDK in Integration Mode

<IntegrationMode/>

## Troubleshoot

### 401 Unauthorized Error When Updating Helm Dependencies {#401}

#### Symptom

You see an error message similar to the following after adding the Replicated SDK as a dependency in your Helm chart then running `helm dependency update`:

```
Error: could not download oci://registry.replicated.com/library/replicated-sdk: failed to authorize: failed to fetch oauth token: unexpected status from GET request to https://registry.replicated.com/v2/token?scope=repository%3Alibrary%2Freplicated-sdk%3Apull&service=registry.replicated.com: 401 Unauthorized
```

#### Cause

When you run `helm dependency update`, Helm attempts to pull the Replicated SDK chart from the Replicated registry. An error can occur if you are already logged in to the Replicated registry with a license that has expired, such as when testing application releases.

#### Solution

To solve this issue:

1. Run the following command to remove login credentials for the Replicated registry:

    ```
    helm registry logout registry.replicated.com
    ```

1. Re-run `helm dependency update` for your Helm chart.    

---


import SDKOverview from "../partials/replicated-sdk/_overview.mdx"
import SdkValues from "../partials/replicated-sdk/_sdk-values.mdx"

# About the Replicated SDK

This topic provides an introduction to using the Replicated SDK with your application.

## Overview

<SDKOverview/>

For more information about the Replicated SDK API, see [Replicated SDK API](/reference/replicated-sdk-apis). For information about developing against the SDK API locally, see [Developing Against the SDK API](replicated-sdk-development).

## Limitations

The Replicated SDK has the following limitations:

* Some popular enterprise continuous delivery tools, such as ArgoCD and Pulumi, deploy Helm charts by running `helm template` then `kubectl apply` on the generated manifests, rather than running `helm install` or `helm upgrade`.  The following limitations apply to applications installed by running `helm template` then `kubectl apply`:

  * The `/api/v1/app/history` SDK API endpoint always returns an empty array because there is no Helm history in the cluster. See [GET /app/history](/reference/replicated-sdk-apis#get-apphistory) in _Replicated SDK API_.

  * The SDK does not automatically generate status informers to report status data for installed instances of the application. To get instance status data, you must enable custom status informers by overriding the `replicated.statusInformers` Helm value. See [Enable Application Status Insights](/vendor/insights-app-status#enable-application-status-insights) in _Enabling and Understanding Application Status_.
 
## SDK Resiliency

At startup and when serving requests, the SDK retrieves and caches the latest information from the upstream Replicated APIs, including customer license information.

If the upstream APIs are not available at startup, the SDK does not accept connections or serve requests until it is able to communicate with the upstream APIs. If communication fails, the SDK retries every 10 seconds and the SDK pod is at `0/1` ready.

When serving requests, if the upstream APIs become unavailable, the SDK serves from the memory cache and sets the `X-Replicated-Served-From-Cache` header to `true`.  Additionally, rapid successive requests to same SDK endpoint with the same request properties will be rate-limited returning the last cached payload and status code without reaching out to the upstream APIs. A `X-Replicated-Rate-Limited` header will set to `true`.

## Replicated SDK Helm Values

<SdkValues/>

---


# Customizing the Replicated SDK

This topic describes various ways to customize the Replicated SDK, including customizing RBAC, setting environment variables, adding tolerations, and more.

## Customize RBAC for the SDK

This section describes role-based access control (RBAC) for the Replicated SDK, including the default RBAC, minimum RBAC requirements, and how to install the SDK with custom RBAC.

### Default RBAC

The SDK creates default Role, RoleBinding, and ServiceAccount objects during installation. The default Role allows the SDK to get, list, and watch all resources in the namespace, to create Secrets, and to update the `replicated` and `replicated-instance-report` Secrets:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    {{- include "replicated.labels" . | nindent 4 }}
  name: replicated-role
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - 'get'
  - 'list'
  - 'watch'
- apiGroups:
  - ''
  resources:
  - 'secrets'
  verbs:
  - 'create'
- apiGroups:
  - ''
  resources:
  - 'secrets'
  verbs:
  - 'update'
  resourceNames:
  - replicated
  - replicated-instance-report
  - replicated-custom-app-metrics-report
```

### Minimum RBAC Requirements

The SDK requires the following minimum RBAC permissions:
* Create Secrets.
* Get and update Secrets named `replicated`, `replicated-instance-report`, and `replicated-custom-app-metrics-report`.
* The SDK requires the following minimum RBAC permissions for status informers:
  * If you defined custom status informers, then the SDK must have permissions to get, list, and watch all the resources listed in the `replicated.statusInformers` array in your Helm chart `values.yaml` file.
  * If you did _not_ define custom status informers, then the SDK must have permissions to get, list, and watch the following resources:
    * Deployments
    * Daemonsets
    * Ingresses
    * PersistentVolumeClaims
    * Statefulsets
    * Services   
  * For any Ingress resources used as status informers, the SDK requires `get` permissions for the Service resources listed in the `backend.Service.Name` field of the Ingress resource.
  * For any Daemonset and Statefulset resources used as status informers, the SDK requires `list` permissions for pods in the namespace.
  * For any Service resources used as status informers, the SDK requires `get` permissions for Endpoint resources with the same name as the service.  

  The Replicated Vendor Portal uses status informers to provide application status data. For more information, see [Helm Installations](/vendor/insights-app-status#helm-installations) in _Enabling and Understanding Application Status_.
### Install the SDK with Custom RBAC

#### Custom ServiceAccount

To use the SDK with custom RBAC permissions, provide the name for a custom ServiceAccount object during installation. When a service account is provided, the SDK uses the RBAC permissions granted to the service account and does not create the default Role, RoleBinding, or ServiceAccount objects.

To install the SDK with custom RBAC:

1. Create custom Role, RoleBinding, and ServiceAccount objects. The Role must meet the minimum requirements described in [Minimum RBAC Requirements](#minimum-rbac-requirements) above.
1. During installation, provide the name of the service account that you created by including `--set replicated.serviceAccountName=CUSTOM_SERVICEACCOUNT_NAME`.

  **Example**:

  ```
  helm install wordpress oci://registry.replicated.com/my-app/beta/wordpress --set replicated.serviceAccountName=mycustomserviceaccount
  ```

 For more information about installing with Helm, see [Installing with Helm](/vendor/install-with-helm).  

#### Custom ClusterRole

To use the SDK with an existing ClusterRole, provide the name for a custom ClusterRole object during installation. When a cluster role is provided, the SDK uses the RBAC permissions granted to the cluster role and does not create the default RoleBinding. Instead, the SDK creates a ClusterRoleBinding as well as a ServiceAccount object.

To install the SDK with a custom ClusterRole:

1. Create a custom ClusterRole object. The ClusterRole must meet at least the minimum requirements described in [Minimum RBAC Requirements](#minimum-rbac-requirements) above. However, it can also provide additional permissions that can be used by the SDK, such as listing cluster Nodes.
1. During installation, provide the name of the cluster role that you created by including `--set replicated.clusterRole=CUSTOM_CLUSTERROLE_NAME`.

  **Example**:

  ```
  helm install wordpress oci://registry.replicated.com/my-app/beta/wordpress --set replicated.clusterRole=mycustomclusterrole
  ```

 For more information about installing with Helm, see [Installing with Helm](/vendor/install-with-helm).

## Set Environment Variables {#env-var}

The Replicated SDK provides a `replicated.extraEnv` value that allows users to set additional environment variables for the deployment that are not exposed as Helm values.

This ensures that users can set the environment variables that they require without the SDK Helm chart needing to be modified to expose the values. For example, if the SDK is running behind an HTTP proxy server, then the user could set `HTTP_PROXY` or `HTTPS_PROXY` environment variables to provide the hostname or IP address of their proxy server.

To add environment variables to the Replicated SDK deployment, include the `replicated.extraEnv` array in your Helm chart `values.yaml` file. The `replicated.extraEnv` array accepts a list of environment variables in the following format:

```yaml
# Helm chart values.yaml

replicated:
  extraEnv:
  - name: ENV_VAR_NAME
    value: ENV_VAR_VALUE
```

:::note
If the `HTTP_PROXY`, `HTTPS_PROXY`, and `NO_PROXY` variables are configured with the [kots install](/reference/kots-cli-install) command, these variables will also be set automatically in the Replicated SDK.
:::

**Example**:

```yaml
# Helm chart values.yaml

replicated:
  extraEnv:
  - name: MY_ENV_VAR
    value: my-value
  - name: MY_ENV_VAR_2
    value: my-value-2  
```

## Custom Certificate Authority

When installing the Replicated SDK behind a proxy server that terminates TLS and injects a custom certificate, you must provide the CA to the SDK. This can be done by storing the CA in a ConfigMap or a Secret prior to installation and providing appropriate values during installation.

### Using a ConfigMap

To use a CA stored in a ConfigMap:

1. Create a ConfigMap and the CA as the data value. Note that name of the ConfigMap and data key can be anything.
   ```bash
   kubectl -n <NAMESPACE> create configmap private-ca --from-file=ca.crt=./ca.crt
   ```
1. Add the name of the config map to the values file:
   ```yaml
   replicated:
     privateCAConfigmap: private-ca
   ```

:::note
If the `--private-ca-configmap` flag is used with the [kots install](/reference/kots-cli-install) command, this value will be populated in the Replicated SDK automatically.
:::

### Using a Secret

To use a CA stored in a Secret:

1. Create a Secret and the CA as a data value. Note that the name of the Secret and the key can be anything.
   ```bash
   kubectl -n <NAMESPACE> create secret generic private-ca --from-file=ca.crt=./ca.crt
   ```
1. Add the name of the secret and the key to the values file:
   ```yaml
   replicated:
     privateCASecret:
       name: private-ca
       key: ca.crt
   ```

## Add Tolerations

The Replicated SDK provides a `replicated.tolerations` value that allows users to add custom tolerations to the deployment. For more information about tolerations, see [Taints and Tolerations](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) in the Kubernetes documentation.

To add tolerations to the Replicated SDK deployment, include the `replicated.tolerations` array in your Helm chart `values.yaml` file. The `replicated.tolerations` array accepts a list of tolerations in the following format:

```yaml
# Helm chart values.yaml

replicated:
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "value"
    effect: "NoSchedule"
```

## Add Affinity

The Replicated SDK provides a `replicated.affinity` value that allows users to add custom affinity to the deployment. For more information about affinity, see [Affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in the Kubernetes documentation.

To add affinity to the Replicated SDK deployment, include the `replicated.affinity` map in your Helm chart `values.yaml` file. The `replicated.affinity` map accepts a standard Kubernets affinity object in the following format:

```yaml
# Helm chart values.yaml

replicated:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: production/node-pool
            operator: In
            values:
            - private-node-pool
```
## Add Custom Labels

With the Replicated SDK version 1.1.0 and later, you can pass custom labels to the Replicated SDK Helm Chart by setting the `replicated.commonLabels` and `replicated.podLabels` Helm values in your Helm chart.

### Requirement

The `replicated.commonLabels` and `replicated.podLabels` values are available with the Replicated SDK version 1.1.0 and later.

### commonLabels

The `replicated.commonLabels` value allows you to add one or more labels to all resources created by the SDK chart.

For example:

```yaml
# Helm chart values.yaml

replicated:
  commonLabels:
    environment: production
    team: platform
```

### podLabels

The `replicated.podLabels` value allows you to add pod-specific labels to the pod template.

For example:

```yaml
# Helm chart values.yaml

replicated:
  podLabels:
    monitoring: enabled
    custom.company.io/pod-label: value
```

---


# Using Custom Domains

This topic describes how to use the Replicated Vendor Portal to add and manage custom domains to alias the Replicated registry, the Replicated proxy registry, the Replicated app service, and the Download Portal.

For information about adding and managing custom domains with the Vendor API v3, see the [customHostnames](https://replicated-vendor-api.readme.io/reference/createcustomhostname) section in the Vendor API v3 documentation.

For more information about custom domains, see [About Custom Domains](custom-domains).

## Add a Custom Domain in the Vendor Portal {#add-domain}

To add and verify a custom domain:

1. In the [Vendor Portal](https://vendor.replicated.com), go to **Custom Domains**. 

1. In the **Add custom domain** dropdown, select the target Replicated endpoint.

    The **Configure a custom domain** wizard opens.

    <img src="/images/custom-domains-download-configure.png" alt="custom domain wizard" width="500"/>

    [View a larger version of this image](/images/custom-domains-download-configure.png)

1. For **Domain**, enter the custom domain. Click **Save & continue**.

1. For **Create CNAME**, copy the text string and use it to create a CNAME record in your DNS account. Click **Continue**.

1. For **Verify ownership**, ownership will be validated automatically using an HTTP token when possible.

    If ownership cannot be validated automatically, copy the text string provided and use it to create a TXT record in your DNS account. Click **Validate & continue**. Your changes can take up to 24 hours to propagate.

1. For **TLS cert creation verification**, TLS verification will be performed automatically using an HTTP token when possible.

    If TLS verification cannot be performed automatically, copy the text string provided and use it to create a TXT record in your DNS account. Click **Validate & continue**. Your changes can take up to 24 hours to propagate.

    :::note
    If you set up a [CAA record](https://letsencrypt.org/docs/caa/) for this hostname, you must include all Certificate Authorities (CAs) that Cloudflare partners with. The following CAA records are required to ensure proper certificate issuance and renewal:

    ```dns
    @ IN CAA 0 issue "letsencrypt.org"
    @ IN CAA 0 issue "pki.goog; cansignhttpexchanges=yes"
    @ IN CAA 0 issue "ssl.com"
    @ IN CAA 0 issue "amazon.com"
    @ IN CAA 0 issue "cloudflare.com"
    @ IN CAA 0 issue "google.com"
    ```

    Failing to include any of these CAs might prevent certificate issuance or renewal, which can result in downtime for your customers. For additional security, you can add an IODEF record to receive notifications about certificate requests:

    ```dns
    @ IN CAA 0 iodef "mailto:your-security-team@example.com"
    ```
    :::

1. For **Use Domain**, to set the new domain as the default, click **Yes, set as default**. Otherwise, click **Not now**.

    :::note
    Replicated recommends that you do _not_ set a domain as the default until you are ready for it to be used by customers.
    :::

    After the verification checks for ownership and TLS certificate creation are complete, the Vendor Portal marks the domain as **Configured**. 

1. (Optional) After a domain is marked as **Configured**, you can remove any TXT records that you created in your DNS account.

## Use Custom Domains

After you add one or more custom domains in the Vendor Portal, you can configure your application to use the domains. 

### Configure Embedded Cluster to Use Custom Domains {#ec}

You can configure Replicated Embedded Cluster to use your custom domains for the Replicated proxy registry and Replicated app service. For more information about Embedded Cluster, see [Embedded Cluster Overview](/vendor/embedded-overview).

To configure Embedded Cluster to use your custom domains for the proxy registry and app service:

1. In the [Embedded Cluster Config](/reference/embedded-config) spec for your application, add `domains.proxyRegistryDomain` and `domains.replicatedAppDomain`. Set each field to your custom domain for the given service.

    **Example:**

    ```yaml
    apiVersion: embeddedcluster.replicated.com/v1beta1
    kind: Config
    spec:
      domains:
        # Your proxy registry custom domain
        proxyRegistryDomain: proxy.yourcompany.com
        # Your app service custom domain
        replicatedAppDomain: updates.yourcompany.com   
    ```
    For more information, see [domains](/reference/embedded-config#domains) in _Embedded Cluster Config_.

1. Add the Embedded Cluster Config to a new release. Promote the release to a channel that your team uses for testing, and install with Embedded Cluster in a development environment to test your changes.

### Set a Default Domain

Setting a default domain is useful for ensuring that the same domain is used across channels for all your customers.

When you set a custom domain as the default, it is used by default for all new releases promoted to any channel, as long as the channel does not have a different domain assigned in its channel settings.

Only releases that are promoted to a channel _after_ you set a default domain use the new default domain. Any existing releases that were promoted before you set the default continue to use the same domain that they used previously.

:::note
In Embedded Cluster installations, the KOTS Admin Console will use the domains specified in the `domains.proxyRegistryDomain` and `domains.replicatedAppDomain` fields of the Embedded Cluster Config when making requests to the proxy registry and app service, regardless of the default domain or the domain assigned to the given release channel. For more information about using custom domains in Embedded Cluster installations, see [Configure Embedded Cluster to Use Custom Domains](#ec) above.
:::

To set a custom domain as the default:

1. In the Vendor Portal, go to **Custom Domains**.

1. Next to the target domain, click **Set as default**.

1. In the confirmation dialog that opens, click **Yes, set as default**.

### Assign a Domain to a Channel {#channel-domain}

You can assign a domain to an individual channel by editing the channel settings. When you specify a domain in the channel settings, new releases promoted to the channel use the selected domain even if there is a different domain set as the default on the **Custom Domains** page.

Assigning a domain to a release channel is useful when you need to override either the default Replicated domain or a default custom domain for a specific channel. For example:
* You need to use a different domain for releases promoted to your Beta and Stable channels.
* You need to test a domain in a development environment before you set the domain as the default for all channels.

:::note
In Embedded Cluster installations, the KOTS Admin Console will use the domains specified in the `domains.proxyRegistryDomain` and `domains.replicatedAppDomain` fields of the Embedded Cluster Config when making requests to the proxy registry and app service, regardless of the default domain or the domain assigned to the given release channel. For more information about using custom domains in Embedded Cluster installations, see [Configure Embedded Cluster to Use Custom Domains](#ec) above.
:::

To assign a custom domain to a channel:

1. In the Vendor Portal, go to **Channels** and click the settings icon for the target channel.

1. Under **Custom domains**, in the drop-down for the target Replicated endpoint, select the domain to use for the channel. For more information about channel settings, see [Settings](releases-about#settings) in _About Channels and Releases_.

    <img alt="channel settings dialog" src="/images/channel-settings.png" width="500px"/>

    [View a larger version of this image](/images/channel-settings.png)

## Reuse a Custom Domain for Another Application

If you have configured a custom domain for one application, you can reuse the custom domain for another application in the same team without going through the ownership and TLS certificate verification process again.

To reuse a custom domain for another application:

1. In the Vendor Portal, select the application from the dropdown list.

1. Click **Custom Domains**.

1. In the section for the target endpoint, click **Add your first custom domain** for your first domain, or click **Add new domain** for additional domains.

    The **Configure a custom domain** wizard opens.

1. In the text box, enter the custom domain name that you want to reuse. Click **Save & continue**.
  
    The last page of the wizard opens because the custom domain was verified previously.

1. Do one of the following:

    - Click **Set as default**. In the confirmation dialog that opens, click **Yes, set as default**.
   
    - Click **Not now**. You can come back later to set the domain as the default. The Vendor Portal shows shows that the domain has a Configured status because it was configured for a previous application, though it is not yet assigned as the default for this application.


## Remove a Custom Domain

You can remove a custom domain at any time, but you should plan the transition so that you do not break any existing installations or documentation.

Removing a custom domain for the Replicated registry, proxy registry, or Replicated app service will break existing installations that use the custom domain. Existing installations need to be upgraded to a version that does not use the custom domain before it can be removed safely.

If you remove a custom domain for the download portal, it is no longer accessible using the custom URL. You will need to point customers to an updated URL.

To remove a custom domain:

1. Log in to the [Vendor Portal](https://vendor.replicated.com) and click **Custom Domains**.

1. Verify that the domain is not set as the default nor in use on any channels. You can edit the domains in use on a channel in the channel settings. For more information, see [Settings](releases-about#settings) in _About Channels and Releases_.

    :::important
    When you remove a registry or Replicated app service custom domain, any installations that reference that custom domain will break. Ensure that the custom domain is no longer in use before you remove it from the Vendor Portal.
    :::

1. Click **Remove** next to the unused domain in the list, and then click **Yes, remove domain**.  


---


# About Custom Domains

This topic provides an overview and the limitations of using custom domains to alias the Replicated proxy registry, the Replicated app service, the Replicated Download Portal, and the Replicated registry.

For information about adding and managing custom domains, see [Using Custom Domains](custom-domains-using).

## Overview

You can use custom domains to alias Replicated endpoints by creating Canonical Name (CNAME) records for your domains.

Replicated domains are external to your domain and can require additional security reviews by your customer. Using custom domains as aliases can bring the domains inside an existing security review and reduce your exposure.

You can configure custom domains for the following services:

- **Proxy registry:** Images can be proxied from external private registries using the Replicated proxy registry. By default, the proxy registry uses the domain `proxy.replicated.com`. Replicated recommends using a CNAME such as `proxy.{your app name}.com`. 

- **Replicated app service:** Upstream application YAML and metadata, including a license ID, are pulled from the app service. By default, this service uses the domain `replicated.app`. Replicated recommends using a CNAME such as `updates.{your app name}.com`. 

- **Download Portal:** The Download Portal can be used to share customer license files, air gap bundles, and so on. By default, the Download Portal uses the domain `get.replicated.com`. Replicated recommends using a CNAME such as `portal.{your app name}.com` or `enterprise.{your app name}.com`. 

- **Replicated registry:** Images and Helm charts can be pulled from the Replicated registry. By default, this registry uses the domain `registry.replicated.com`. Replicated recommends using a CNAME such as `registry.{your app name}.com`.

## Limitations

Using custom domains has the following limitations:

- A single custom domain cannot be used for multiple endpoints. For example, a single domain can map to `registry.replicated.com` for any number of applications, but cannot map to both `registry.replicated.com` and `proxy.replicated.com`, even if the applications are different.

- Custom domains cannot be used to alias `api.replicated.com` (legacy customer-facing APIs) or kURL.

- Multiple custom domains can be configured, but only one custom domain can be the default for each Replicated endpoint. All configured custom domains work whether or not they are the default.

- Each custom domain can only be used by one team.

- For [Replicated Embedded Cluster](/vendor/embedded-overview) installations, any Helm [`extensions`](/reference/embedded-config) that you add in the Embedded Cluster Config do not use custom domains. During deployment, Embedded Cluster pulls both the repo for the given chart and any images in the chart as written. Embedded Cluster does not rewrite image names to use custom domains.

---


# Configuring Custom Metrics (Beta)

This topic describes how to configure an application to send custom metrics to the Replicated Vendor Portal.

## Overview

In addition to the built-in insights displayed in the Vendor Portal by default (such as uptime and time to install), you can also configure custom metrics to measure instances of your application running customer environments. Custom metrics can be collected for application instances running in online or air gap environments.

Custom metrics can be used to generate insights on customer usage and adoption of new features, which can help your team to make more informed prioritization decisions. For example:
* Decreased or plateaued usage for a customer can indicate a potential churn risk
* Increased usage for a customer can indicate the opportunity to invest in growth, co-marketing, and upsell efforts
* Low feature usage and adoption overall can indicate the need to invest in usability, discoverability, documentation, education, or in-product onboarding
* High usage volume for a customer can indicate that the customer might need help in scaling their instance infrastructure to keep up with projected usage

## How the Vendor Portal Collects Custom Metrics

The Vendor Portal collects custom metrics through the Replicated SDK that is installed in the cluster alongside the application.

The SDK exposes an in-cluster API where you can configure your application to POST metric payloads. When an application instance sends data to the API, the SDK sends the data (including any custom and built-in metrics) to the Replicated app service. The app service is located at `replicated.app` or at your custom domain.

If any values in the metric payload are different from the current values for the instance, then a new event is generated and displayed in the Vendor Portal. For more information about how the Vendor Portal generates events, see [How the Vendor Portal Generates Events and Insights](/vendor/instance-insights-event-data#about-events) in _About Instance and Event Data_.

The following diagram demonstrates how a custom `activeUsers` metric is sent to the in-cluster API and ultimately displayed in the Vendor Portal, as described above:

<img alt="Custom metrics flowing from customer environment to Vendor Portal" src="/images/custom-metrics-flow.png" width="800px"/>

[View a larger version of this image](/images/custom-metrics-flow.png)

## Requirements

To support the collection of custom metrics in online and air gap environments, the Replicated SDK version 1.0.0-beta.12 or later must be running in the cluster alongside the application instance.

The `PATCH` and `DELETE` methods described below are available in the Replicated SDK version 1.0.0-beta.23 or later.

For more information about the Replicated SDK, see [About the Replicated SDK](/vendor/replicated-sdk-overview).

If you have any customers running earlier versions of the SDK, Replicated recommends that you add logic to your application to gracefully handle a 404 from the in-cluster APIs.

## Limitations

Custom metrics have the following limitations:

* The label that is used to display metrics in the Vendor Portal cannot be customized. Metrics are sent to the Vendor Portal with the same name that is sent in the `POST` or `PATCH` payload. The Vendor Portal then converts camel case to title case: for example, `activeUsers` is displayed as **Active Users**.

* The in-cluster APIs accept only JSON scalar values for metrics. Any requests containing nested objects or arrays are rejected.

* When using the `POST` method any existing keys that are not included in the payload will be deleted. To create new metrics or update existing ones without sending the entire dataset, simply use the `PATCH` method.

## Configure Custom Metrics

You can configure your application to `POST` or `PATCH` a set of metrics as key value pairs to the API that is running in the cluster alongside the application instance.  

To remove an existing custom metric use the `DELETE` endpoint with the custom metric name.

The Replicated SDK provides an in-cluster API custom metrics endpoint at `http://replicated:3000/api/v1/app/custom-metrics`.

**Example:**

```bash
POST http://replicated:3000/api/v1/app/custom-metrics
```

```json
{
  "data": {
    "num_projects": 5,
    "weekly_active_users": 10
  }
}
```

```bash
PATCH http://replicated:3000/api/v1/app/custom-metrics
```

```json
{
  "data": {
    "num_projects": 54,
    "num_error": 2
  }
}
```

```bash
DELETE http://replicated:3000/api/v1/app/custom-metrics/num_projects
```

### POST vs PATCH

The `POST` method will always replace the existing data with the most recent payload received. Any existing keys not included in the most recent payload will still be accessible in the instance events API, but they will no longer appear in the instance summary.

The `PATCH` method will accept partial updates or add new custom metrics if a key:value pair that does not currently exist is passed.

In most cases, simply using the `PATCH` method is recommended.

For example, if a component of your application sends the following via the `POST` method:

```json
{
  "numProjects": 5,
  "activeUsers": 10,
}
```

Then, the component later sends the following also via the `POST` method:

```json
{
  "activeUsers": 10,
  "usingCustomReports": false
}
```

The instance detail will show `Active Users: 10` and `Using Custom Reports: false`, which represents the most recent payload received. The previously-sent `numProjects` value is discarded from the instance summary and is available in the instance events payload.  In order to preseve `numProjects`from the initial payload and upsert `usingCustomReports` and `activeUsers` use the `PATCH` method instead of `POST` on subsequent calls to the endpoint.

For example, if a component of your application initially sends the following via the `POST` method:

```json
{
  "numProjects": 5,
  "activeUsers": 10,
}
``` 

Then, the component later sends the following also via the `PATCH` method:
```json
{
  "usingCustomReports": false
}
```

The instance detail will show `Num Projects: 5`, `Active Users: 10`, `Using Custom Reports: false`, which represents the merged and upserted payload.

### NodeJS Example

The following example shows a NodeJS application that sends metrics on a weekly interval to the in-cluster API exposed by the SDK:

```javascript
async function sendMetrics(db) {

    const projectsQuery = "SELECT COUNT(*) as num_projects from projects";
    const numProjects = (await db.getConnection().queryOne(projectsQuery)).num_projects;

    const usersQuery = 
        "SELECT COUNT(*) as active_users from users where DATEDIFF('day', last_active, CURRENT_TIMESTAMP) < 7";
    const activeUsers = (await db.getConnection().queryOne(usersQuery)).active_users;

    const metrics = { data: { numProjects, activeUsers }};
    
    const res = await fetch('https://replicated:3000/api/v1/app/custom-metrics', {
        method: 'POST',
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(metrics),
    });
    if (res.status !== 200) {
        throw new Error(`Failed to send metrics: ${res.statusText}`);
    }
}

async function startMetricsLoop(db) {

    const ONE_DAY_IN_MS = 1000 * 60 * 60 * 24

    // send metrics once on startup
    await sendMetrics(db)
      .catch((e) => { console.log("error sending metrics: ", e) });        

    // schedule weekly metrics payload

    setInterval( () => {
        sendMetrics(db, licenseId)
          .catch((e) => { console.log("error sending metrics: ", e) });        
    }, ONE_DAY_IN_MS);
}

startMetricsLoop(getDatabase());
```

## View Custom Metrics

You can view the custom metrics that you configure for each active instance of your application on the **Instance Details** page in the Vendor Portal.

The following shows an example of an instance with custom metrics:

<img alt="Custom Metrics section of Instance details page" src="/images/instance-custom-metrics.png" width="700px"/>

[View a larger version of this image](/images/instance-custom-metrics.png)

As shown in the image above, the **Custom Metrics** section of the **Instance Details** page includes the following information:
* The timestamp when the custom metric data was last updated.
* Each custom metric that you configured, along with the most recent value for the metric.
* A time-series graph depicting the historical data trends for the selected metric.

Custom metrics are also included in the **Instance activity** stream of the **Instance Details** page. For more information, see [Instance Activity](/vendor/instance-insights-details#instance-activity) in _Instance Details_.

## Export Custom Metrics

You can use the Vendor API v3 `/app/{app_id}/events` endpoint to programatically access historical timeseries data containing instance level events, including any custom metrics that you have defined. For more information about the endpoint, see [Export Customer and Instance Data](/vendor/instance-data-export).


---


import StatusesTable from "../partials/status-informers/_statusesTable.mdx"
import AggregateStatus from "../partials/status-informers/_aggregateStatus.mdx"
import AggregateStatusIntro from "../partials/status-informers/_aggregate-status-intro.mdx"
import SupportedResources from "../partials/instance-insights/_supported-resources-status.mdx"

# Enabling and Understanding Application Status

This topic describes how to configure your application so that you can view the status of application instances in the Replicated Vendor Portal. It also describes the meaning of the different application statuses.

## Overview

The Vendor Portal displays data on the status of instances of your application that are running in customer environments, including the current state (such as Ready or Degraded), the instance uptime, and the average amount of time it takes your application to reach a Ready state during installation. For more information about viewing instance data, see [Instance Details](instance-insights-details).

To compute and display these insights, the Vendor Portal interprets and aggregates the state of one or more of the supported Kubernetes resources that are deployed to the cluster as part of your application.

<SupportedResources/>

For more information about how instance data is sent to the Vendor Portal, see [About Instance and Event Data](instance-insights-event-data).

## Enable Application Status Insights

To display insights on application status, the Vendor Portal requires that your application has one or more _status informers_. Status informers indicate the Kubernetes resources deployed as part of your application that are monitored for changes in state.

To enable status informers for your application, do one of the following, depending on the installation method:
* [Helm Installations](#helm-installations)
* [KOTS Installations](#kots-installations)

### Helm Installations 

To get instance status data for applications installed with Helm, the Replicated SDK must be installed alongside the application. For information about how to distribute and install the SDK with your application, see [Installing the Replicated SDK](/vendor/replicated-sdk-installing).

After you include the SDK as a dependency, the requirements for enabling status informers vary depending on how your application is installed:

* For applications installed by running `helm install` or `helm upgrade`, the Replicated SDK automatically detects and reports the status of the resources that are part of the Helm release. No additional configuration is required to get instance status data.

* For applications installed by running `helm template` then `kubectl apply`, the SDK cannot automatically detect and report the status of resources. You must configure custom status informers by overriding the `statusInformers` value in the Replicated SDK chart. For example:

  ```yaml
  # Helm chart values.yaml file 

  replicated:
    statusInformers:
      - deployment/nginx
      - statefulset/mysql
  ```

  :::note
  Applications installed by running `helm install` or `helm upgrade` can also use custom status informers. When the `replicated.statusInformers` field is set, the SDK detects and reports the status of only the resources included in the `replicated.statusInformers` field.
  :::

### KOTS Installations

For applications installed with Replicated KOTS, configure one or more status informers in the KOTS Application custom resource. For more information, see [Adding Resource Status Informers](admin-console-display-app-status).

When Helm-based applications that include the Replicated SDK and are deployed by KOTS, the SDK inherits the status informers configured in the KOTS Application custom resource. In this case, the SDK does _not_ automatically report the status of the resources that are part of the Helm release. This prevents discrepancies in the instance data in the vendor platform.

## View Resource Status Insights {#resource-status}

For applications that include the Replicated SDK, the Vendor Portal also displays granular resource status insights in addition to the aggregate application status. For example, you can hover over the **App status** field on the **Instance details** page to view the statuses of the indiviudal resources deployed by the application, as shown below:

<img src="/images/resource-status-hover-current-state.png" alt="resource status pop up" width="400px"/>

[View a larger version of this image](/images/resource-status-hover-current-state.png)

Viewing these resource status details is helpful for understanding which resources are contributing to the aggregate application status. For example, when an application has an Unavailable status, that means that one or more resources are Unavailable. By viewing the resource status insights on the **Instance details** page, you can quickly understand which resource or resources are Unavailable for the purpose of troubleshooting.

Granular resource status details are automatically available when the Replicated SDK is installed alongside the application. For information about how to distribute and install the SDK with your application, see [Installing the Replicated SDK](/vendor/replicated-sdk-installing).

## Understanding Application Status

This section provides information about how Replicated interprets and aggregates the status of Kubernetes resources for your application to report an application status.

### About Resource Statuses {#resource-statuses}

Possible resource statuses are Ready, Updating, Degraded, Unavailable, and Missing.

The following table lists the supported Kubernetes resources and the conditions that contribute to each status:

<StatusesTable/>

### Aggregate Application Status

<AggregateStatusIntro/>

<AggregateStatus/>

---


import AirGapTelemetry from "../partials/instance-insights/_airgap-telemetry.mdx"

# About Instance and Event Data

This topic provides an overview of the customer and instance insights that you can view in the Replicated Vendor Portal. It includes information about how the Vendor Portal accesses data as well as requirements and limitations.  

## How the Vendor Portal Collects Instance Data {#about-reporting}

This section describes how the Vendor Portal collects instance data from online and air gap environments.

### Online Instances

For instances running in online (internet-connected) environments, either Replicated KOTS or the Replicated SDK periodically sends a small amount of data to the Vendor Portal, depending on which is installed in the cluster alongside the application. If both KOTS and the SDK are installed in the cluster (such as when a Helm chart that includes the SDK is installed by KOTS), then both send instance data.

The data sent to the Vendor Portal includes properties such as the current version and status of the instance. For a full overview of what data might be included, see the [Replicated Data Transmission Policy](https://docs.replicated.com/vendor/policies-data-transmission).

The following diagram shows the flow of different types of data from customer environments to the Vendor Portal:

![Telemetry sent from instances to vendor platform](/images/telemetry-diagram.png)

[View a larger version of this image](/images/telemetry-diagram.png)

As shown in the diagram above, application instance data, application status data, and details about the KOTS and the SDK instances running in the cluster are all sent to the Vendor Portal through the Replicated app service:
* When both KOTS and the SDK are installed in the cluster, they both send application instance data, including information about the cluster where the instance is running.
* KOTS and the SDK both send information about themselves, including the version of KOTS or the SDK running in the cluster.
* Any custom metrics configured by the software vendor are sent to the Vendor Portal through the Replicated SDK API. For more information, see [Configuring Custom Metrics](/vendor/custom-metrics).
* Application status data, such as if the instance is ready or degraded, is sent by KOTS. If KOTS is not installed in the cluster, then the SDK sends the application status data. For more information, see [Enabling and Understanding Application Status](/vendor/insights-app-status).

### Air Gap Instances

<AirGapTelemetry/>

For more information, see [Collecting Telemetry for Air Gap Instances](/vendor/telemetry-air-gap).

## Frequency of Data Sent to the Vendor Portal

This section describes how frequently data is sent to the Vendor Portal for online and air gap instances.

### From the Replicated SDK (Online Instances Only)

When installed alongside the application in an online environment, the SDK automatically sends instance data to the Vendor Portal when any of the following occur:

* The SDK sends data every four hours.

* The instance checks for updates. An update check occurs when the instance makes a request to the `/api/v1/app/updates` SDK API endpoint. See [app](/reference/replicated-sdk-apis#app) in _Replicated SDK API (Alpha)_.

* The instance completes a Helm update to a new application version. After the update completes, the SDK sends data when it restarts.

* The status of an instance changes. For example, an instance can change from a Ready to Degraded status. For more information, see [Enabling and Understanding Application Status](insights-app-status).

### From KOTS (Online Instances Only)

When installed alongisde the application in an online environment, KOTS automatically sends instance data to the Vendor Portal when any of the following occur:

* The instance checks for updates. By default, KOTS checks for updates every four hours. Additionally, an update check can occur when a user clicks the **Check for updates** button in the Replicated Admin Console. 

  :::note
  KOTS users can modify or disable automatic update checks from the Admin Console. For more information, see [Configuring Automatic Updates](/enterprise/updating-apps).
  :::

* The status of an instance changes. For example, an instance can change from a Ready to Degraded status. For more information, see [Enabling and Understanding Application Status](insights-app-status).

* (KOTS v1.92 and later only) The instance deploys a new application version.

### From Air Gap Instances

For air gap instances, the frequency of data sent to the Vendor Portal depends on how frequently support bundles are collected in the customer environment and uploaded to the Vendor Portal.

For more information, see [Collecting Telemetry for Air Gap Instances](/vendor/telemetry-air-gap).

## How the Vendor Portal Generates Events and Insights {#about-events}

When the Vendor Portal receives instance data, it evaluates each data field to determine if there was a change in its value. For each field that changes in value, the Vendor Portal creates an _event_ to record the change. For example, a change from Ready to Degraded in the application status generates an event.

In addition to creating events for changes in data sent by the instance, the Vendor Portal also generates events for changes in values of computed metrics. The Vendor Portal updates the values of computed metrics each time it receives instance data. For example, the Vendor Portal computes a _Versions behind_ metric that tracks the number of versions behind the latest available version for the instance. When the instance checks for updates and a new update is available, the value of this metric changes and the Vendor Portal generates an event.

The Vendor Portal uses events to display insights for each active instance in a **Instance details** dashboard. For more information about using the Vendor Portal **Instance details** page to monitor active instances of your application, see [Instance Details](instance-insights-details).

## Requirements

The following requirements apply to collecting instance telemetry:

* Replicated KOTS or the Replicated SDK must be installed in the cluster where the application instance is running. 

* For KOTS installations and for Helm CLI installations that use `helm template` then `kubectl apply`, additional configuration is required to get application status data. For more information, see [Enabling and Understanding Application Status](/vendor/insights-app-status).

* To view resource status details for an instance on the **Instance details** page, the Replicated SDK must be installed in the cluster alongside the application. For more information, see [View Resource Status Insights](insights-app-status#resource-status) in _Enabling and Understanding Application Status_.

* There are additional requirements for collecting telemetry from air gap instances. For more information, see [Collecting Telemetry for Air Gap Instances](/vendor/telemetry-air-gap).

## Limitations

The Vendor Portal has the following limitations for reporting instance data and generating events:

* **Active instances**: Instance data is available for _active_ instances. An instance is considered inactive when its most recent check-in was more than 24 hours ago. An instance can become inactive if it is decommissioned, stops checking for updates, or otherwise stops reporting.

   The Vendor Portal continues to display data for an inactive instance from its most-recently seen state. This means that data for an inactive instance might continue to show a Ready status after the instance becomes inactive. Replicated recommends that you use the timestamp in the **Last Check-in** field to understand if an instance might have become inactive, causing its data to be out-of-date.
* **Air gap instances**: There are additional limitations for air gap telemetry. For more information, see [Collecting Telemetry for Air Gap Instances](/vendor/telemetry-air-gap).  
* **Instance data freshness**: The rate at which data is updated in the Vendor Portal varies depending on how often the Vendor Portal receives instance data.
* **Event timestamps**: The timestamp of events displayed on the **Instances details** page is the timestamp when the Replicated Vendor API received the data from the instance. The timestamp of events does not necessarily reflect the timestamp of when the event occurred.
* **Caching for kURL cluster data**: For clusters created with Replicated kURL (embedded clusters), KOTS stores the counts of total nodes and ready nodes in a cache for five minutes. If KOTS sends instance data to the Vendor Portal within the five minute window, then the reported data for total nodes and ready nodes reflects the data in the cache. This means that events displayed on the **Instances details** page for the total nodes and ready nodes can show values that differ from the current values of these fields.


---


import ChangeChannel from "../partials/customers/_change-channel.mdx"

# About Customers and Licensing

This topic provides an overview of customers and licenses in the Replicated Platform.

## Overview

The licensing features of the Replicated Platform allow vendors to securely grant access to software, making license agreements available to the application in end customer environments at startup and runtime.

The Replicated Vendor Portal also allows vendors to create and manage customer records. Each customer record includes several fields that uniquely identify the customer and the application, specify the customer's assigned release channel, and define the customer's entitlements. 

Vendors can use these licensing features to enforce entitlements such as license expiration dates, and to track and report on software usage for the purpose of surfacing insights to both internal teams and customers.

The following diagram provides an overview of licensing with the Replicated Platform:

![App instance communicates with the Replicated licensing server](/images/licensing-overview.png)

[View a larger version of this image](/images/licensing-overview.png)

As shown in the diagram above, the Replicated license and update server manages and distributes customer license information. The license server retrieves this license information from customer records managed by vendors in the Vendor Portal. 

During installation or upgrade, the customer's license ID is used to authenticate with the license server. The license ID also provides authentication for the Replicated proxy registry, securely granting proxy access to images in the vendor's external registry.

The license server is identified with a CNAME record where it can be accessed from end customer environments. When running alongside an application in a customer environment, the Replicated SDK retrieves up-to-date customer license information from the license server during runtime. The in-cluster SDK API `/license/` endpoints can be used to get customer license information on-demand, allowing vendors to programmatically enforce and report on license agreements.

Vendors can also integrate internal Customer Relationship Management (CRM) tools such as Salesforce with the Replicated Platform so that any changes to a customer's entitlements are automatically reflected in the Vendor Portal. This ensures that updates to license agreements are reflected in the customer environment in real time.

## About Customers

Each customer that you create in the Replicated Vendor Portal has a unique license ID. Your customers use their license when they install or update your application.

You assign customers to channels in the Vendor Portal to control their access to your application releases. Customers can install or upgrade to releases that are promoted to the channel they are assigned. For example, assigning a customer to your Beta channel allows that customer to install or upgrade to only releases promoted to the Beta channel.

Each customer license includes several fields that uniquely identify the customer and the application, specify the customer's assigned release channel, and define the customer's entitlements, such as if the license has an expiration date or what application functionality the customer can access. Replicated securely delivers these entitlements to the application and makes them available at installation or at runtime.

For more information about how to create and manage customers, see [Creating and Managing Customers](releases-creating-customer).

### Customer Channel Assignment {#channel-assignment}

<ChangeChannel/>

For example, if the latest release promoted to the Beta channel is version 1.25.0 and version 1.10.0 is marked as required, when you edit an existing customer to assign them to the Beta channel, then the KOTS Admin Console always fetches 1.25.0, even though 1.10.0 is marked as required. The required release 1.10.0 is ignored and is not available to the customer for upgrade.

For more information about how to mark a release as required, see [Properties](releases-about#properties) in _About Channels and Releases_. For more information about how to synchronize licenses in the Admin Console, see [Updating Licenses in the Admin Console](/enterprise/updating-licenses).

### Customer Types

Each customer is assigned one of the following types:

* **Development**: The Development type can be used internally by the development
team for testing and integration.
* **Trial**: The Trial type can be used for customers who are on 2-4 week trials
of your software.
* **Paid**: The Paid type identifies the customer as a paying customer for which
additional information can be provided.
* **Community**: The Community type is designed for a free or low cost version of your application. For more details about this type, see [Community Licenses](licenses-about-types).
* (Beta) **Single Tenant Vendor Managed**: The Single Tenant Vendor Managed type is for customers for whom your team is operating the application in infrastructure you fully control and operate. Single Tenant Vendor Managed licenses are free to use, but come with limited support. The Single Tenant Vendor Managed type is a Beta feature. Reach out to your Replicated account representative to get access.

Except Community licenses, the license type is used solely for reporting purposes and a customer's access to your application is not affected by the type that you assign.

You can change the type of a license at any time in the Vendor Portal. For example, if a customer upgraded from a trial to a paid account, then you could change their license type from Trial to Paid for reporting purposes. 

### About Managing Customers

Each customer record in the Vendor Portal has built-in fields and also supports custom fields:
* The built-in fields include values such as the customer name, customer email, and the license expiration date. You can optionally set initial values for the built-in fields so that each new customer created in the Vendor Portal starts with the same set of values.
* You can also create custom fields to define entitlements for your application. For example, you can create a custom field to set the number of active users permitted.

For more information, see [Managing Customer License Fields](/vendor/licenses-adding-custom-fields).

You can make changes to a customer record in the Vendor Portal at any time. The license ID, which is the unique identifier for the customer, never changes. For more information about managing customers in the Vendor Portal, see [Creating and Managing Customers](releases-creating-customer).

### About the Customers Page

The following shows an example of the **Customers** page:

![Customers page](/images/customers-page.png)

[View a larger version of this image](/images/customers-page.png)

From the **Customers** page, you can do the following:

* Create new customers.

* Download CSVs with customer and instance data.

* Search and filter customers.

* Click the **Manage customer** button to edit details such as the customer name and email, the custom license fields assigned to the customer, and the license expiration policy. For more information, see [Creating and Managing Customers](releases-creating-customer).

* Download the license file for each customer.

* Click the **Customer reporting** button to view data about the active application instances associated with each customer. For more information, see [Customer Reporting](customer-reporting).

* View instance details for each customer, including the version of the application that this instance is running, the Kubernetes distribution of the cluster, the last check-in time, and more:

  <img width="800px" src="/images/customer-reporting-details.png" />
  
  [View a larger version of this image](/images/customer-reporting-details.png)

* Archive customers. For more information, see [Creating and Managing Customers](releases-creating-customer).

* Click on a customer on the **Customers** page to access the following customer-specific pages:
  * [Reporting](#about-the-customer-reporting-page)
  * [Manage customer](#about-the-manage-customer-page)
  * [Support bundles](#about-the-customer-support-bundles-page)

### About the Customer Reporting Page

The **Reporting** page for a customer displays data about the active application instances associated with each customer. The following shows an example of the **Reporting** page for a customer that has two active application instances:

![Customer reporting page in the Vendor Portal](/images/customer-reporting-page.png)
[View a larger version of this image](/images/customer-reporting-page.png)

For more information about interpreting the data on the **Reporting** page, see [Customer Reporting](customer-reporting).

### About the Manage Customer Page

The **Manage customer** page for a customer displays details about the customer license, including the customer name and email, the license expiration policy, custom license fields, and more.

The following shows an example of the **Manage customer** page:

![Manage customer page in the Vendor Portal](/images/customer-details.png)
[View a larger version of this image](/images/customer-details.png)

From the **Manage customer** page, you can view and edit the customer's license fields or archive the customer. For more information, see [Creating and Managing Customers](releases-creating-customer).

### About the Customer Support Bundles Page

The **Support bundles** page for a customer displays details about the support bundles collected from the customer. Customers with the **Support Bundle Upload Enabled** entitlement can provide support bundles through the KOTS Admin Console, or you can upload support bundles manually in the Vendor Portal by going to **Troubleshoot > Upload a support bundle**. For more information about uploading and analyzing support bundles, see [Inspecting Support Bundles](support-inspecting-support-bundles).

The following shows an example of the **Support bundles** page:

![Support bundles page in the Vendor Portal](/images/customer-support-bundles.png)
[View a larger version of this image](/images/customer-support-bundles.png)

As shown in the screenshot above, the **Support bundles** page lists details about the collected support bundles, such as the date the support bundle was collected and the debugging insights found. You can click on a support bundle to view it in the **Support bundle analysis** page. You can also click **Delete** to delete the support bundle, or click **Customer Reporting** to view the **Reporting** page for the customer.

## About Licensing with Replicated

### About Syncing Licenses

When you edit customer licenses for an application installed with a Replicated installer (Embedded Cluster, KOTS, kURL), your customers can use the KOTS Admin Console to get the latest license details from the Vendor Portal, then deploy a new version that includes the license changes. Deploying a new version with the license changes ensures that any license fields that you have templated in your release using [KOTS template functions](/reference/template-functions-about) are rendered with the latest license details.

For online instances, KOTS pulls license details from the Vendor Portal when:
* A customer clicks **Sync license** in the Admin Console.
* An automatic or manual update check is performed by KOTS.
* An update is performed with Replicated Embedded Cluster. See [Performing Updates with Embedded Cluster](/enterprise/updating-embedded).
* An application status changes. See [Current State](instance-insights-details#current-state) in _Instance Details_.

For more information, see [Updating Licenses in the Admin Console](/enterprise/updating-licenses).

### About Syncing Licenses in Air-Gapped Environments

To update licenses in air gap installations, customers need to upload the updated license file to the Admin Console.

After you update the license fields in the Vendor Portal, you can notify customers by either sending them a new license file or instructing them to log into their Download Portal to downlaod the new license.

For more information, see [Updating Licenses in the Admin Console](/enterprise/updating-licenses).

### Retrieving License Details with the SDK API

The [Replicated SDK](replicated-sdk-overview) includes an in-cluster API that can be used to retrieve up-to-date customer license information from the Vendor Portal during runtime through the [`license`](/reference/replicated-sdk-apis#license) endpoints. This means that you can add logic to your application to get the latest license information without the customer needing to perform a license update. The SDK API polls the Vendor Portal for updated data every four hours.

In KOTS installations that include the SDK, users need to update their licenses from the Admin Console as described in [About Syncing Licenses](#about-syncing-licenses) above. However, any logic in your application that uses the SDK API will update the user's license information without the customer needing to deploy a license update in the Admin Console.

For information about how to use the SDK API to query license entitlements at runtime, see [Querying Entitlements with the Replicated SDK API](/vendor/licenses-reference-sdk).

### License Expiration Handling {#expiration}

The built-in `expires_at` license field defines the expiration date for a customer license. When you set an expiration date in the Vendor Portal, the `expires_at` field is encoded in ISO 8601 format (`2026-01-23T00:00:00Z`) and is set to midnight UTC at the beginning of the calendar day (`00:00:00`) on the date selected.

Replicated enforces the following logic when a license expires:
* By default, instances with expired licenses continue to run.
   To change the behavior of your application when a license expires, you can can add custom logic in your application that queries the `expires_at` field using the Replicated SDK in-cluster API. For more information, see [Querying Entitlements with the Replicated SDK API](/vendor/licenses-reference-sdk).
* Expired licenses cannot log in to the Replicated registry to pull a Helm chart for installation or upgrade.
* Expired licenses cannot pull application images through the Replicated proxy registry or from the Replicated registry.
* In Replicated KOTS installations, KOTS prevents instances with expired licenses from receiving updates.

### Replacing Licenses for Existing Installations

Community licenses are the only license type that can be replaced with a new license without needing to reinstall the application. For more information, see [Community Licenses](licenses-about-types).

Unless the existing customer is using a community license, it is not possible to replace one license with another license without reinstalling the application. When you need to make changes to a customer's entitlements, Replicated recommends that you edit the customer's license details in the Vendor Portal, rather than issuing a new license.


---


# Managing Customer License Fields

This topic describes how to manage customer license fields in the Replicated Vendor Portal, including how to add custom fields and set initial values for the built-in fields.

## Set Initial Values for Built-In License Fields (Beta)

You can set initial values to populate the **Create Customer** form in the Vendor Portal when a new customer is created. This ensures that each new customer created from the Vendor Portal UI starts with the same set of built-in license field values.

:::note
Initial values are not applied to new customers created through the Vendor API v3. For more information, see [Create a customer](https://replicated-vendor-api.readme.io/reference/createcustomer-1) in the Vendor API v3 documentation.
:::

These _initial_ values differ from _default_ values in that setting initial values does not update the license field values for any existing customers.

To set initial values for built-in license fields:

1. In the Vendor Portal, go to **License Fields**.

1. Under **Built-in license options**, click **Edit** next to each license field where you want to set an initial value. 

     ![Edit Initial Value](/images/edit-initial-value.png)

     [View a larger version of this image](/images/edit-initial-value.png)

## Manage Custom License Fields

You can create custom license fields in the Vendor Portal. For example, you can create a custom license field to set the number of active users permitted. Or, you can create a field that sets the number of nodes a customer is permitted on their cluster.

The custom license fields that you create are displayed in the Vendor Portal for all new and existing customers. If the custom field is not hidden, it is also displayed to customers under the **Licenses** tab in the Replicated Admin Console.

### Limitation

The maximum size for a license field value is 64KB.

### Create Custom License Fields

To create a custom license field:

1. Log in to the Vendor Portal and select the application.

1. On the **License Fields** page, click **Create license field**.

   <img width="500" alt="create a new License Field dialog" src="/images/license-add-custom-field.png"/>

   [View a larger version of this image](/images/license-add-custom-field.png)

1. Complete the following fields:

   | Field                  | Description           |
   |-----------------------|------------------------|
   | Field | The name used to reference the field. This value cannot be changed. |
   | Title| The display name for the field. This is how the field appears in the Vendor Portal and the Admin Console. You can change the title in the Vendor Portal. |
   | Type| The field type. Supported formats include integer, string, text (multi-line string), and boolean values. This value cannot be changed. |
   | Default | The default value for the field for both existing and new customers. It is a best practice to provide a default value when possible. The maximum size for a license field value is 64KB. |
   | Required | If checked, this prevents the creation of customers unless this field is explicitly defined with a value. |
   | Hidden | If checked, the field is not visible to your customer in the Replicated Admin Console. The field is still visible to you in the Vendor Portal. **Note**: The Hidden field is displayed only for vendors with access to the Replicated installers (KOTS, kURL, Embedded Cluster). |

### Update Custom License Fields

To update a custom license field:

1. Log in to the Vendor Portal and select the application.
1. On the **License Fields** page, click **Edit Field** on the right side of the target row. Changing the default value for a field updates the value for each existing customer record that has not overridden the default value.

   :::important
   Enabling **Is this field is required?** updates the license field to be required on all new and existing customers. If you enable **Is this field is required?**, you must either set a default value for the field or manually update each existing customer to provide a value for the field.
   :::
   
### Set Customer-Specific Values for Custom License Fields

To set a customer-specific value for a custom license field:

1. Log in to the Vendor Portal and select the application.
1. Click **Customers**.
1. For the target customer, click the **Manage customer** button.
1. Under **Custom fields**, enter values for the target custom license fields for the customer.

   :::note
   The maximum size for a license field value is 64KB.
   :::

   <img width="600" alt="Custom license fields section in the manage customer page" src="/images/customer-license-custom-fields.png"/>

   [View a larger version of this image](/images/customer-license-custom-fields.png)

### Delete Custom License Fields

Deleted license fields and their values do not appear in the customer's license in any location, including your view in the Vendor Portal, the downloaded YAML version of the license, and the Admin Console **License** screen.

By default, deleting a custom license field also deletes all of the values associated with the field in each customer record.

Only administrators can delete license fields.

:::important
Replicated recommends that you take care when deleting license fields.

Outages can occur for existing deployments if your application or the Admin Console **Config** page expect a license file to provide a required value.
:::

To delete a custom license field:

1. Log in to the Vendor Portal and select the application.
1. On the **License Fields** page, click **Edit Field** on the right side of the target row.
1. Click **Delete** on the bottom left of the dialog.
1. (Optional) Enable **Preserve License Values** to save values for the license field that were not set by the default in each customer record. Preserved license values are not visible to you or the customer.

   :::note
   If you enable **Preserve License Values**, you can create a new field with the same name and `type` as the deleted field to reinstate the preserved values.
   :::

1. Follow the instructions in the dialog and click **Delete**.

---


import InstallerOnlyAnnotation from "../partials/helm/_installer-only-annotation.mdx"

# Managing Install Types for a License

This topic describes how to manage which installation types and options are enabled for a license.

## Overview

You can control which installation methods are available to each of your customers by enabling or disabling **Install types** fields in the customer's license. 

The following shows an example of the **Install types** field in a license:

![Install types license fields](/images/license-install-types.png)

[View a larger version of this image](/images/license-install-types.png)

The installation types that are enabled or disabled for a license determine the following:
* The Replicated installers ([Replicated KOTS](../intro-kots), [Replicated Embedded Cluster](/vendor/embedded-overview), [Replicated kURL](/vendor/kurl-about)) that the customer's license entitles them to use 
* The installation assets and/or instructions provided in the Replicated Download Portal for the customer
* The customer's KOTS Admin Console experience

Setting the supported installation types on a per-customer basis gives you greater control over the installation method used by each customer. It also allows you to provide a more curated Download Portal experience, in that customers will only see the installation assets and instructions that are relevant to them.

## Understanding Install Types {#install-types}

In the customer license, under **Install types**, the **Available install types** field allows you to enable and disable different installation methods for the customer.

You can enable one or more installation types for a license.

The following describes each installation type available, as well as the requirements for enabling each type:

<table>
     <tr>
          <th width="30%">Install Type</th>
          <th width="35%">Description</th>
          <th>Requirements</th>
     </tr>
     <tr>
          <th>Existing Cluster (Helm CLI)</th>
          <td><p>Allows the customer to install with Helm in an existing cluster. The customer does not have access to the Replicated installers (Embedded Cluster, KOTS, and kURL).</p><p>When the <strong>Helm CLI Air Gap Instructions (Helm CLI only)</strong> install option is also enabled, the Download Portal displays instructions on how to pull Helm installable images into a local repository. See <a href="#install-options">Understanding Additional Install Options</a> below.</p></td>
          <td>
            <p>The latest release promoted to the channel where the customer is assigned must contain one or more Helm charts. It can also include Replicated custom resources, such as the Embedded Cluster Config custom resource, the KOTS HelmChart, Config, and Application custom resources, or the Troubleshoot Preflight and SupportBundle custom resources.</p>
            <InstallerOnlyAnnotation/>
          </td>
     </tr>
     <tr>
       <th>Existing Cluster (KOTS install)</th>
       <td>Allows the customer to install with Replicated KOTS in an existing cluster.</td>
       <td>
          <ul>
            <li>Your Vendor Portal team must have the KOTS entitlement</li>
            <li>The latest release promoted to the channel where the customer is assigned must contain KOTS custom resources, such as the KOTS HelmChart, Config, and Application custom resources. For more information, see [About Custom Resources](/reference/custom-resource-about).</li>
          </ul> 
       </td>   
     </tr>
     <tr>
        <th>kURL Embedded Cluster (first generation product)</th>
        <td>
          <p>Allows the customer to install with Replicated kURL on a VM or bare metal server.</p>
          <p><strong>Note:</strong> For new installations, enable Replicated Embedded Cluster (current generation product) instead of Replicated kURL (first generation product).</p>
        </td>
          <td>
            <ul>
              <li>Your Vendor Portal team must have the kURL entitlement</li>
              <li>A kURL installer spec must be promoted to the channel where the customer is assigned. For more information, see <a href="/vendor/packaging-embedded-kubernetes">Creating a kURL Installer</a>.</li>
            </ul>
          </td>     
     </tr>
     <tr>
       <th>Embedded Cluster (current generation product)</th>
       <td>Allows the customer to install with Replicated Embedded Cluster on a VM or bare metal server.</td>
       <td>
         <ul>
           <li>Your Vendor Portal team must have the Embedded Cluster entitlement</li>
           <li>The latest release promoted to the channel where the customer is assigned must contain an Embedded Cluster Config custom resource. For more information, see <a href="/reference/embedded-config">Embedded Cluster Config</a>.</li>
         </ul> 
       </td>   
     </tr>
</table>

## Understanding Additional Install Options {#install-options}

After enabling installation types in the **Available install types** field, you can also enable the following options in the **Additional install options** field:

<table>
     <tr>
       <th width="30%">Install Type</th>
       <th>Description</th>
       <th>Requirements</th>
     </tr>
     <tr>
       <th>Helm CLI Air Gap Instructions (Helm CLI only)</th>
       <td><p>When enabled, a customer will see instructions on the Download Portal on how to pull Helm installable images into their local repository.</p><p><strong>Helm CLI Air Gap Instructions</strong> is enabled by default when you select the <strong>Existing Cluster (Helm CLI)</strong> install type. For more information see [Installing with Helm in Air Gap Environments](/vendor/helm-install-airgap)</p></td>
       <td>The <strong>Existing Cluster (Helm CLI)</strong> install type must be enabled</td>
     </tr>
     <tr>
       <th>Air Gap Installation Option (Replicated Installers only)</th>
       <td><p>When enabled, new installations with this license have an option in their Download Portal to install from an air gap package or do a traditional online installation.</p></td>
       <td>
          <p>At least one of the following Replicated install types must be enabled:</p>
          <ul>
           <li>Existing Cluster (KOTS install)</li>
           <li>kURL Embedded Cluster (first generation product)</li>
           <li>Embedded Cluster (current generation product)</li>
         </ul>
       </td>
     </tr>
</table>

## About Migrating Existing Licenses to Use Install Types

By default, when an existing customer license is migrated to include the Beta **Install types** field, the Vendor Portal automatically enables certain install types so that the customer does not experience any interruptions or errors in their deployment.

The Vendor Portal uses the following logic to enable install types for migrated licenses:

If the existing license has the **KOTS Install Enabled** field enabled, then the Vendor Portal enables the following install types in the migrated license by default:
* Existing Cluster (Helm CLI) 
* Existing Cluster (KOTS install)
* kURL Embedded Cluster (first generation product)
* Embedded Cluster (current generation product)

Additionally, if the existing **KOTS Install Enabled** license also has the **Airgap Download Enabled** option enabled, then the Vendor Portal enables both of the air gap install options in the migrated license (**Helm CLI Air Gap Instructions (Helm CLI only)** and **Air Gap Installation Option (Replicated Installers only)**).

Otherwise, if the **KOTS Install Enabled** field is disabled for the existing license, then the Vendor Portal enables only the **Existing Cluster (Helm CLI)** install type by default. All other install types will be disabled by default.


---


# Querying Entitlements with the Replicated SDK API

This topic describes how to query license entitlements at runtime using the Replicated SDK in-cluster API. The information in this topic applies to applications installed with Replicated KOTS or Helm.

## Overview

The Replicated SDK retrieves up-to-date customer license information from the Vendor Portal during runtime. This means that any changes to customer licenses are reflected in real time in the customer environment. For example, you can revoke access to your application when a license expires, expose additional product functionality dynamically based on entitlements, and more. For more information about distributing the SDK with your application, see [About the Replicated SDK](replicated-sdk-overview).

After the Replicated SDK is initialized and running in a customer environment, you can use the following SDK API endpoints to get information about the license:
* `/api/v1/license/info`: List license details, including the license ID, the channel the customer is assigned, and the license type.
* `/api/v1/license/fields`: List all the fields in the license.  
* `/api/v1/license/fields/{field_name}`: List details about a specific license field, including the field name, description, type, and the value.

For more information about these endpoints, see [license](/reference/replicated-sdk-apis#license) in _Replicated SDK API_.

## Prerequisite

Add the Replicated SDK to your application:
* For Helm-based applications, see [Install the SDK as a Subchart](/vendor/replicated-sdk-installing#install-the-sdk-as-a-subchart) in _Installing the Replicated SDK_
* For applications that use standard Kubernetes manifests, see [Install the SDK Alongside a Standard Manifest-Based Application](/vendor/replicated-sdk-installing#manifest-app) in _Installing the Replicated SDK_

## Query License Entitlements at Runtime {#runtime}

To use the SDK API to query entitlements at runtime:

1. Create or edit a customer to use for testing:

   1. In the Vendor Portal, click **Customers**. Select a customer and click the **Manage customer** tab. Alternatively, click **+ Create customer** to create a new customer. For more information, see [Creating and Managing Customers](/vendor/releases-creating-customer).

   1. Edit the built-in fields and add custom fields for the customer. For example, you can set a license expiration date in the **Expiration policy** field. Or, you can create a custom field that limits the number of nodes a user is permitted in their cluster. For more information, see [Managing Customer License Fields](/vendor/licenses-adding-custom-fields).

1. (Recommended) Develop against the SDK API `license` endpoints locally:

   1. Install the Replicated SDK as a standalone component in your cluster. This is called _integration mode_. Installing in integration mode allows you to develop locally against the SDK API without needing to create releases for your application in the Vendor Portal. See [Developing Against the SDK API](/vendor/replicated-sdk-development).

   1. In your application, add logic to control application behavior based on the customer license information returned by the SDK API service running in your cluster. See [license](/reference/replicated-sdk-apis#license) in _Replicated SDK API (Beta)_.

      **Example:**

      ```bash
      curl replicated:3000/api/v1/license/fields/expires_at
      ```

      ```json
      {
        "name": "expires_at",
        "title": "Expiration",
        "description": "License Expiration",
        "value": "2023-05-30T00:00:00Z",
        "valueType": "String",
        "signature": {
          "v1": "c6rsImpilJhW0eK+Kk37jeRQvBpvWgJeXK2M..."
        }
      }
      ```

1. When you are ready to test your changes outside of integration mode, do the following:

   1. Package your Helm chart and its dependencies (including the Replicated SDK) into a `.tgz` chart archive. See [Packaging a Helm Chart for a Release](helm-install-release).

   1. Add the `.tgz` archive to a release and promote to a development channel, such as Unstable. See [Managing Releases with the Vendor Portal](/vendor/releases-creating-releases).

   1. Install in a development environment using the license ID for the test customer that you created. See [Installing with Helm](install-with-helm).

   1. (Optional) As needed, verify the license information returned by the SDK API in your development environment using port forwarding to access the SDK service locally:

      1. Use port forwarding to access the `replicated` service from the local development environment on port 3000:

         ```bash
         kubectl port-forward service/replicated 3000
         ```

         The output looks similar to the following:

         ```bash
         Forwarding from 127.0.0.1:3000 -> 3000
         ```

         For more information about `kubectl port-forward`, see [port-forward](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#port-forward) in the kubectl reference documentation.

      1. With the port forward running, in another terminal, use the SDK API to return information about the license.

         **Example:**

         ```
         curl localhost:3000/api/v1/license/fields/expires_at
         ```

1. Repeat these steps to add and test new license fields.

1. (Recommended) Use signature verification in your application to ensure the integrity of the license field. See [Verifying License Field Signatures with the Replicated SDK API](/vendor/licenses-verify-fields-sdk-api).

---


import ChangeChannel from "../partials/customers/_change-channel.mdx"
import RequiredReleasesLimitations from "../partials/releases/_required-releases-limitations.mdx"
import RequiredReleasesDescription from "../partials/releases/_required-releases-description.mdx"
import VersionLabelReqsHelm from "../partials/releases/_version-label-reqs-helm.mdx"

# About Channels and Releases

This topic describes channels and releases, including information about the **Releases** and **Channels** pages in the Replicated Vendor Portal.

## Overview

A _release_ represents a single version of your application. Each release is promoted to one or more _channels_. Channels provide a way to progress releases through the software development lifecycle: from internal testing, to sharing with early-adopters, and finally to making the release generally available.

Channels also control which customers are able to install a release. You assign each customer to a channel to define the releases that the customer can access. For example, a customer assigned to the Stable channel can only install releases that are promoted to the Stable channel, and cannot see any releases promoted to other channels. For more information about assigning customers to channels, see [Channel Assignment](licenses-about#channel-assignment) in _About Customers_.

Using channels and releases helps you distribute versions of your application to the right customer segments, without needing to manage different release workflows.

You can manage channels and releases with the Vendor Portal, the Replicated CLI, or the Vendor API v3. For more information about creating and managing releases or channels, see [Managing Releases with the Vendor Portal](releases-creating-releases) or [Creating and Editing Channels](releases-creating-channels). 

## About Channels

This section provides additional information about channels, including details about the default channels in the Vendor Portal and channel settings.

### Unstable, Beta, and Stable Channels

Replicated includes the following channels by default:

* **Unstable**: The Unstable channel is designed for internal testing and development. You can create and assign an internal test customer to the Unstable channel to install in a development environment. Replicated recommends that you do not license any of your external users against the Unstable channel.
* **Beta**: The Beta channel is designed for release candidates and early-adopting customers. Replicated recommends that you promote a release to the Beta channel after it has passed automated testing in the Unstable channel. You can also choose to license early-adopting customers against this channel.
* **Stable**: The Stable channel is designed for releases that are generally available. Replicated recommends that you assign most of your customers to the Stable channel. Customers licensed against the Stable channel only receive application updates when you promote a new release to the Stable channel.

You can archive or edit any of the default channels, and create new channels. For more information, see [Creating and Editing Channels](releases-creating-channels).

### Settings

Each channel has settings. You can customize the settings for a channel to control some of the behavior of releases promoted to the channel.

The following shows the **Channel Settings** dialog, accessed by clicking the settings icon on a channel:

<img src="/images/channel-settings.png" alt="Channel Settings dialog in the Vendor Portal" width="500"/>

[View a larger version of this image](/images/channel-settings.png)

The following describes each of the channel settings:

* **Channel name**: The name of the channel. You can change the channel name at any time. Each channel also has a unique ID listed below the channel name.
* **Description**: Optionally, add a description of the channel.
* **Set this channel to default**: When enabled, sets the channel as the default channel. The default channel cannot be archived.
* **Custom domains**: Select the customer-facing domains that releases promoted to this channel use for the Replicated registry, Replicated proxy registry, Replicated app service, or Replicated Download Portal endpoints. If a default custom domain exists for any of these endpoints, choosing a different domain in the channel settings overrides the default. If no custom domains are configured for an endpoint, the drop-down for the endpoint is disabled.

  For more information about configuring custom domains and assigning default domains, see [Using Custom Domains](custom-domains-using).
* The following channel settings apply only to applications that support KOTS:
    * **Automatically create airgap builds for newly promoted releases in this channel**: When enabled, the Vendor Portal automatically builds an air gap bundle when a new release is promoted to the channel. When disabled, you can generate an air gap bundle manually for a release on the **Release History** page for the channel.
    * **Enable semantic versioning**: When enabled, the Vendor Portal verifies that the version label for any releases promoted to the channel uses a valid semantic version. For more information, see [Semantic Versioning](releases-about#semantic-versioning) in _About Releases_.
    * **Enable new airgap bundle format**: When enabled, air gap bundles built for releases promoted to the channel use a format that supports image digests. This air gap bundle format also ensures that identical image layers are not duplicated, resulting in a smaller air gap bundle size. For more information, see [Using Image Digests in Air Gap Installations](private-images-tags-digests#digests-air-gap) in _Using Image Tags and Digests_.

      :::note
      The new air gap bundle format is supported for applications installed with KOTS v1.82.0 or later. 
      :::  
   
## About Releases

This section provides additional information about releases, including details about release promotion, properties, sequencing, and versioning.

### Release Files

A release contains your application files as well as the manifests required to install the application with the Replicated installers ([Replicated Embedded Cluster](/vendor/embedded-overview) and [Replicated KOTS](../intro-kots)). 

The application files in releases can be Helm charts and/or Kubernetes manifests. Replicated strongly recommends that all applications are packaged as Helm charts because many enterprise customers will expect to be able to install with Helm.

### Promotion

Each release is promoted to one or more channels. While you are developing and testing releases, Replicated recommends promoting to a channel that does not have any real customers assigned, such as the default Unstable channel. When the release is ready to be shared externally with customers, you can then promote to a channel that has the target customers assigned, such as the Beta or Stable channel.

A release cannot be edited after it is promoted to a channel. This means that you can test a release on an internal development channel, and know with confidence that the same release will be available to your customers when you promote it to a channel where real customers are assigned.

### Properties

Each release has properties. You define release properties when you promote a release to a channel. You can edit release properties at any time from the channel **Release History** page in the Vendor Portal. For more information, see [Edit Release Properties](releases-creating-releases#edit-release-properties) in _Managing Releases with the Vendor Portal_.

The following shows an example of the release properties dialog:

<img src="/images/release-properties.png" width="500px" alt="release properties dialog for a release with version label 0.1.22"/>

[View a larger version of this image](/images/release-properties.png)

As shown in the screenshot above, the release has the following properties:

* **Version label**: The version label for the release. Version labels have the following requirements:

  * If semantic versioning is enabled for the channel, you must use a valid semantic version. For more information, see [Semantic Versioning](#semantic-versioning).

  <VersionLabelReqsHelm/>

* **Requirements**: Select **Prevent this release from being skipped during upgrades** to mark the release as required.

  <RequiredReleasesDescription/> 

  <RequiredReleasesLimitations/>

* **Release notes (supports markdown)**: Detailed release notes for the release. The release notes support markdown and are shown to your customer.

### Sequencing

By default, Replicated uses release sequence numbers to organize and order releases, and uses instance sequence numbers in an instance's internal version history.

#### Release Sequences

In the Vendor Portal, each release is automatically assigned a unique, monotonically-increasing sequence number. You can use this number as a fallback to identify a promoted or draft release, if you do not set the `Version label` field during promotion. For more information, see [Managing Releases with the Vendor Portal](releases-creating-releases).

The following graphic shows release sequence numbers in the Vendor Portal:

<img alt="Release sequence numbers" src="/images/release-sequences.png" width="750px"/>

[View a larger version of this image](/images/release-sequences.png)

#### Instance Sequences 

When a new version is available for upgrade, including when KOTS checks for upstream updates as well as when the user syncs their license or makes a config change, the KOTS Admin Console assigns a unique instance sequence number to that version. The instance sequence in the Admin Console starts at 0 and increments for each identifier that is returned when a new version is available.

This instance sequence is unrelated to the release sequence dispalyed in the Vendor Portal, and it is likely that the instance sequence will differ from the release sequence. Instance sequences are only tracked by KOTS instances, and the Vendor Portal has no knowledge of these numbers.

The following graphic shows instance sequence numbers on the Admin Console dashboard:

<img alt="Instance sequence numbers" src="/images/instance-sequences.png" width="550px"/>

[View a larger version of this image](/images/instance-sequences.png)

#### Channel Sequences

When a release is promoted to a channel, a channel sequence number is assigned. This unique sequence number increments by one and tracks the order in which releases were promoted to a channel. You can view the channel sequence on the **Release History** page in the Vendor Portal, as shown in the image below:

<img alt="Channel sequence on Release History page" src="/images/release-history-channel-sequence.png" width="750px"/>

[View a larger version of this image](/images/release-history-channel-sequence.png)

The channel sequence is also used in certain URLs. For example, a release with a *release sequence* of `170` can have a *channel sequence* of `125`. The air gap download URL for that release can contain `125` in the URL, even though the release sequence is `170`.

Ordering is more complex if some or all of the releases in a channel have a semantic version label and semantic versioning is enabled for the channel. For more information, see [Semantic Versioning Sequence](#semantic-versioning-sequence).

#### Semantic Versioning Sequence

For channels with semantic versioning enabled, the Admin Console sequences instance releases by their semantic versions instead of their promotion dates.

If releases without a valid semantic version are already promoted to a channel, the Admin Console sorts the releases that do have semantic versions starting with the earliest version and proceeding to the latest. The releases with non-semantic versioning stay in the order of their promotion dates. For example, assume that you promote these releases in the following order to a channel: 

- 1.0.0
- abc
- 0.1.0
- xyz
- 2.0.0 

Then, you enable semantic versioning on that channel. The Admin Console sequences the version history for the channel as follows: 

- 0.1.0
- 1.0.0
- abc
- xyz
- 2.0.0

### Semantic Versioning

Semantic versioning is available with the Replicated KOTS v1.58.0 and later. Note the following:

- For applications created in the Vendor Portal on or after February 23, 2022, semantic versioning is enabled by default on the Stable and Beta channels. Semantic versioning is disabled on the Unstable channel by default.

- For existing applications created before February 23, 2022, semantic versioning is disabled by default on all channels.

Semantic versioning is recommended because it makes versioning more predictable for users and lets you enforce versioning so that no one uses an incorrect version.

To use semantic versioning:

1. Enable semantic versioning on a channel, if it is not enabled by default. Click the **Edit channel settings** icon, and turn on the **Enable semantic versioning** toggle.
1. Assign a semantic version number when you promote a release.

Releases promoted to a channel with semantic versioning enabled are verified to ensure that the release version label is a valid semantic version. For more information about valid semantic versions, see [Semantic Versioning 2.0.0](https://semver.org).

If you enable semantic versioning for a channel and then promote releases to it, Replicated recommends that you do not later disable semantic versioning for that channel.

You can enable semantic versioning on a channel that already has releases promoted to it without semantic versioning. Any subsequently promoted releases must use semantic versioning. In this case, the channel will have releases with and without semantic version numbers. For information about how Replicated organizes these release sequences, see [Semantic Versioning Sequences](#semantic-versioning-sequence).

### Demotion

A channel release can be demoted from a channel. When a channel release is demoted, the release is no longer available for download, but is not withdrawn from environments where it was already downloaded or installed.

The demoted release's channel sequence and version are not reused. For customers, the release will appear to have been skipped. Un-demoting a release will restore its place in the channel sequence making it again available for download and installation.

For information about how to demote a release, see [Demote a Release](/vendor/releases-creating-releases#demote-a-release) in _Managing Releases with the Vendor Portal_. 

## Vendor Portal Pages

This section provides information about the channels and releases pages in the Vendor Portal.

### Channels Page

The **Channels** page in the Vendor Portal includes information about each channel. From the **Channels** page, you can edit and archive your channels. You can also edit the properties of the releases promoted to each channel, and view and edit the customers assigned to each channel.

The following shows an example of a channel in the Vendor Portal **Channels** page:

<img src="/images/channel-card.png" alt="Channel card in the Vendor Portal" width="400"/>

[View a larger version of this image](/images/channel-card.png)

As shown in the image above, you can do the following from the **Channels** page:

* Edit the channel settings by clicking on the settings icon, or archive the channel by clicking on the trash can icon. For information about channel settings, see [Settings](#settings).

* In the **Adoption rate** section, view data on the adoption rate of releases promoted to the channel among customers assigned to the channel.

* In the **Customers** section, view the number of active and inactive customers assigned to the channel. Click **Details** to go to the **Customers** page, where you can view details about the customers assigned to the channel.

* In the **Latest release** section, view the properties of the latest release, and get information about any warnings or errors in the YAML files for the latest release.

   Click **Release history** to access the history of all releases promoted to the channel. From the **Release History** page, you can view the version labels and files in each release that has been promoted to the selected channel.
   
   You can also build and download air gap bundles to be used in air gap installations with Replicated installers (Embedded Cluster, KOTS, kURL), edit the release properties for each release promoted to the channel from the **Release History** page, and demote a release from the channel.

   The following shows an example of the **Release History** page: 

   <img src="/images/channels-release-history.png" alt="Release history page in the Vendor Portal" width="750"/>

   [View a larger version of this image](/images/channel-card.png)

* For applications that support KOTS, you can also do the following from the **Channel** page:

   * In the **kURL installer** section, view the current kURL installer promoted to the channel. Click **Installer history** to view the history of kURL installers promoted to the channel. For more information about creating kURL installers, see [Creating a kURL Installer](packaging-embedded-kubernetes).

   * In the **Install** section, view and copy the installation commands for the latest release on the channel.

### Draft Release Page

For applications that support installation with KOTS, the **Draft** page provides a YAML editor to add, edit, and delete your application files and Replicated custom resources. You click **Releases > Create Release** in the Vendor Portal to open the **Draft** page.

The following shows an example of the **Draft** page in the Vendor Portal:

  <img alt="Draft release page"src="/images/guides/kots/default-yaml.png" width="700px"/>

  [View a larger version of this image](/images/guides/kots/default-yaml.png)

You can do the following tasks on the **Draft** page:

- In the file directory, manage the file directory structure. Replicated custom resource files are grouped together above the white line of the file directory. Application files are grouped together underneath the white line in the file directory.

  Delete files using the trash icon that displays when you hover over a file. Create a new file or folder using the corresponding icons at the bottom of the file directory pane. You can also drag and drop files in and out of the folders.

    ![Manage File Directory](/images/new-file-and-trash.png)

- Edit the YAML files by selecting a file in the directory and making changes in the YAML editor.

- In the **Help** or **Config help** pane, view the linter for any errors. If there are no errors, you get an **Everything looks good!** message. If an error displays, you can click the **Learn how to configure** link. For more information, see [Linter Rules](/reference/linter).

- Select the Config custom resource to preview how your application's Config page will look to your customers. The **Config preview** pane only appears when you select that file. For more information, see [About the Configuration Screen](config-screen-about).

- Select the Application custom resource to preview how your application icon will look in the Admin Console. The **Application icon preview** only appears when you select that file. For more information, see [Customizing the Application Icon](admin-console-customize-app-icon).


---


# Creating and Editing Channels

This topic describes how to create and edit channels using the Replicated Vendor Portal. For more information about channels, see [About Channels and Releases](releases-about).

For information about creating channels with the Replicated CLI, see [channel create](/reference/replicated-cli-channel-create).

For information about creating and managing channels with the Vendor API v3, see the [channels](https://replicated-vendor-api.readme.io/reference/createchannel) section in the Vendor API v3 documentation.

## Create a Channel

To create a channel:

1. From the Replicated [Vendor Portal](https://vendor.replicated.com), select **Channels** from the left menu.
1. Click **Create Channel**.

   The Create a new channel dialog opens. For example:

   <img src="/images/channels-create.png" alt="Create channel dialog" width="400px"/>

1. Enter a name and description for the channel.
1. (Recommended) Enable semantic versioning on the channel if it is not enabled by default by turning on **Enable semantic versioning**. For more information about semantic versioning and defaults, see [Semantic Versioning](releases-about#semantic-versioning).

1. (Recommended) Enable an air gap bundle format that supports image digests and deduplication of image layers, by turning on **Enable new air gap bundle format**. For more information, see [Using Image Tags and Digests](private-images-tags-digests).

1. Click **Create Channel**.

## Edit a Channel

To edit the settings of an existing channel:

1. In the Vendor Portal, select **Channels** from the left menu.
1. Click the gear icon on the top right of the channel that you want to modify.

   The Channel settings dialog opens. For example:

   <img src="/images/channel-settings.png" alt="Channel Settings dialog in the Vendor Portal" width="500"/>

1. Edit the fields and click **Save**.

   For more information about channel settings, see [Settings](releases-about#settings) in _About Channels and Releases_.

## Archive a Channel

You can archive an existing channel to prevent any new releases from being promoted to the channel.

:::note
You cannot archive a channel if:
* There are customers assigned to the channel.
* The channel is set as the default channel.

Assign customers to a different channel and set a different channel as the default before archiving.
:::

To archive a channel with the Vendor Portal or the Replicated CLI:

* **Vendor portal**: In the Vendor Portal, go to the **Channels** page and click the trash can icon in the top right corner of the card for the channel that you want to archive.
* **Replicated CLI**:
  1. Run the following command to find the ID for the channel that you want to archive:
     ```
     replicated channel ls
     ```
     The output of this command includes the ID and name for each channel, as well as information about the latest release version on the channels.

  1. Run the following command to archive the channel:
     ```
     replicated channel rm CHANNEL_ID
     ```
     Replace `CHANNEL_ID` with the channel ID that you retrieved in the previous step.

     For more information, see [channel rm](/reference/replicated-cli-channel-rm) in the Replicated CLI documentation.


---


# Managing Releases with the CLI

This topic describes how to use the Replicated CLI to create and promote releases.

For information about creating and managing releases with the Vendor Portal, see [Managing Releases with the Vendor Portal](/vendor/releases-creating-releases).

For information about creating and managing releases with the Vendor API v3, see the [releases](https://replicated-vendor-api.readme.io/reference/createrelease) section in the Vendor API v3 documentation.

## Prerequisites

Before you create a release using the Replicated CLI, complete the following prerequisites:

* Install the Replicated CLI and then log in to authorize the CLI. See [Installing the Replicated CLI](/reference/replicated-cli-installing).
  
* Create a new application using the `replicated app create APP_NAME` command. You only need to do this procedure one time for each application that you want to deploy. See [`app create`](/reference/replicated-cli-app-create) in _Reference_.

* Set the `REPLICATED_APP` environment variable to the slug of the target application. See [Set Environment Variables](/reference/replicated-cli-installing#env-var) in _Installing the Replicated CLI_.

  **Example**:

  ```bash
  export REPLICATED_APP=my-app-slug
  ```

## Create a Release From a Local Directory {#dir}

You can use the Replicated CLI to create a release from a local directory that contains the release files.

To create and promote a release:

1. (Helm Charts Only) If your release contains any Helm charts:

   1. Package each Helm chart as a `.tgz` file. See [Packaging a Helm Chart for a Release](/vendor/helm-install-release).

   1. Move the `.tgz` file or files to the local directory that contains the release files:

       ```bash
       mv CHART_TGZ PATH_TO_RELEASE_DIR
       ```
       Where:
       * `CHART_TGZ` is the `.tgz` Helm chart archive.
       * `PATH_TO_RELEASE_DIR` is path to the directory that contains the release files.

       **Example**

       ```bash
       mv wordpress-1.3.5.tgz manifests
       ```

   1. In the same directory that contains the release files, add a HelmChart custom resource for each Helm chart in the release. See [Configuring the HelmChart Custom Resource](helm-native-v2-using).

1. Lint the application manifest files and ensure that there are no errors in the YAML:
 
    ```bash
    replicated release lint --yaml-dir=PATH_TO_RELEASE_DIR
    ```

    Where `PATH_TO_RELEASE_DIR` is the path to the directory with the release files.

    For more information, see [release lint](/reference/replicated-cli-release-lint) and [Linter Rules](/reference/linter).   

1. Do one of the following:

   * **Create and promote the release with one command**:

      ```bash
      replicated release create --yaml-dir PATH_TO_RELEASE_DIR --lint --promote CHANNEL
      ```
      Where:
      * `PATH_TO_RELEASE_DIR` is the path to the directory with the release files.
      * `CHANNEL` is the channel ID or the case sensitive name of the channel.

   * **Create and edit the release before promoting**:

      1. Create the release:

          ```bash
          replicated release create --yaml-dir PATH_TO_RELEASE_DIR
          ```
          Where `PATH_TO_RELEASE_DIR` is the path to the directory with the release files.

          For more information, see [release create](/reference/replicated-cli-release-create).

      1. Edit and update the release as desired:

          ```
          replicated release update SEQUENCE --yaml-dir PATH_TO_RELEASE_DIR
          ```
          Where:
            
          -  `SEQUENCE` is the release sequence number. This identifies the existing release to be updated.
          -  `PATH_TO_RELEASE_DIR` is the path to the directory with the release files.

          For more information, see [release update](/reference/replicated-cli-release-update).

      1. Promote the release when you are ready to test it. Releases cannot be edited after they are promoted. To make changes after promotion, create a new release.

          ```
          replicated release promote SEQUENCE CHANNEL
          ```

          Where:
            
          -  `SEQUENCE` is the release sequence number.
          -  `CHANNEL` is the channel ID or the case sensitive name of the channel.

          For more information, see [release promote](/reference/replicated-cli-release-promote).

1. Verify that the release was promoted to the target channel:

    ```
    replicated release ls
    ```

---


# Accessing a Customer's Download Portal

This topic describes how to access installation instructions and download installation assets (such as customer license files and air gap bundles) from the Replicated Download Portal.

For information about downloading air gap bundles and licenses with the Vendor API v3, see the following pages in the Vendor API v3 documentation:
* [Download a customer license file as YAML](https://replicated-vendor-api.readme.io/reference/downloadlicense)
* [Trigger airgap build for a channel's release](https://replicated-vendor-api.readme.io/reference/channelreleaseairgapbuild)
* [Get airgap bundle download URL for the active release on the channel](https://replicated-vendor-api.readme.io/reference/channelreleaseairgapbundleurl)

## Overview

The Replicated Download Portal can be used to share license files, air gap bundles, and other assets with customers. From the Download Portal, customers can also view instructions for how to install a release with Replicated Embedded Cluster or the Helm CLI.

A unique Download Portal link is available for each customer. The Download Portal uses information from the customer's license to make the relevant assets and installation instructions available.

## Limitation

Sessions in the Download Portal are valid for 72 hours. After the session expires, your customer must log in again. The Download Portal session length is not configurable.

## Access the Download Portal

To access the Download Portal for a customer:

1. In the [Vendor Portal](https://vendor.replicated.com), on the **Customers** page, click on the name of the customer.

1. On the **Manage customer** tab, under **Install types**, enable the installation types that are supported for the customer. This determines the installation instructions and assets that are available for the customer in the Download Portal. For more information about install types, see [Managing Install Types for a License](/vendor/licenses-install-types).

   <img alt="install types" src="/images/license-install-types.png" width="700px"/>

   [View a larger version of this image](/images/license-install-types.png)

1. (Optional) Under **Advanced install options**, enable the air gap installation options that are supported for the customer:
   * Enable **Helm CLI Air Gap Instructions (Helm CLI only)** to display Helm air gap installation instructions in the Download Portal for the customer.
   * Enable **Air Gap Installation Option (Replicated Installers only)** to allow the customer to install from an air gap bundle using one of the Replicated installers (KOTS, kURL, Embedded Cluster).

   <img alt="advanced install options" src="/images/airgap-download-enabled.png" width="500px"/>

   [View a larger version of this image](/images/airgap-download-enabled.png)

1. Save your changes.

1. On the **Reporting** tab, in the **Download portal** section, click **Manage customer password**.

   <img alt="download portal section" src="/images/download-portal-link.png" width="450px"/>

   [View a larger version of this image](/images/download-portal-link.png)

1. In the pop-up window, enter a password or click **Generate**.

   <img alt="download portal password pop-up" src="/images/download-portal-password-popup.png" width="450px"/>

   [View a larger version of this image](/images/download-portal-password-popup.png)

1. Click **Copy** to copy the password to your clipboard.

   After the password is saved, it cannot be retrieved again. If you lose the password, you can generate a new one.

1. Click **Save** to set the password.   

1. Click **Visit download portal** to log in to the Download Portal
and preview your customer's experience.

   :::note
   By default, the Download Portal uses the domain `get.replicated.com`. You can optionally use a custom domain for the Download Portal. For more information, see [Using Custom Domains](/vendor/custom-domains-using).
   :::

1. In the Download Portal, on the left side of the screen, select the installation type. The options displayed vary depending on the **Install types** and **Advanced install options** that you enabled in the customer's license.

     The following is an example of the Download Portal with options for Helm and Embedded Cluster installations:

     ![download portal showing embedded cluster installation instructions](/images/download-portal-helm-ec.png)

     [View a larger version of this image](/images/download-portal-helm-ec.png)

1. To share installation instructions and assets with a customer, send the customer their unique link and password for the Download Portal.


---


import TeamTokenNote from "../partials/vendor-api/_team-token-note.mdx"

# Generating API Tokens

This topic describes the available types of API tokens and how to generate them for use with the Replicated CLI and Replicated Vendor API v3.

## About API Tokens

The Vendor API v3 is the API that manages applications in the Replicated Vendor Portal. The Replicated CLI is an implementation of the Vendor API v3.

Using the Replicated CLI and Vendor API V3 requires an API token for authorization. Tokens are primarily used for automated customer, channel, and release management. You create tokens in the Vendor Portal.

The following types of tokens are available:

- [Service Accounts](#service-accounts)
- [User API Tokens](#user-api-tokens)

<TeamTokenNote/>

### Service Accounts

Service accounts are assigned a token and associated with an RBAC policy. Users with the proper permissions can create, retrieve, or revoke service account tokens. Admin users can assign any RBAC policy to a service account. Non-admin users can only assign their own RBAC policy when they create a service account.

Service accounts are useful for operations that are not tied to a particular user, such as CI/CD or integrations.

Updates to a service account's RBAC policy are automatically applied to its associated token. When a service account is removed, its tokens are also invalidated.

### User API Tokens

User API tokens are private to the user creating the token. User tokens assume the user's account when used, including any RBAC permissions.

Updates to a user's RBAC role are applied to all of the tokens belonging to that user.

Revoking a user token immediately invalidates that token. When a user account is deleted, its user tokens are also deleted.

## Generate Tokens

To use the Replicated CLI or the Vendor API v3, you need a User API token or a Service Account token. Existing team API tokens also continue to work.

### Generate a Service Account

To generate a service account:

1. Log in to the Vendor Portal, and select [**Team > Service Accounts**](https://vendor.replicated.com/team/serviceaccounts).
1. Select **New Service Account**. If one or more service accounts already exist, you can add another by selecting **New Service Account**.

1. Edit the fields in the **New Service Account** dialog:

     <img alt="New Service Accounts Dialog" src="/images/service-accounts.png" width="400px"/>

     [View a larger version of this image](/images/service-accounts.png)

     1. For **Nickname**, enter a name the token. Names for service accounts must be unique within a given team.

     1. For **RBAC**, select the RBAC policy from the dropdown list. The token must have `Admin` access to create new releases.

       This list includes the Vendor Portal default policies `Admin` and `Read Only`. Any custom policies also display in this list. For more information, see [Configuring RBAC Policies](team-management-rbac-configuring).

       Users with a non-admin RBAC role cannot select any other RBAC role when creating a token. They are restricted to creating a token with their same level of access to avoid permission elevation.

     1. (Optional) For custom RBAC policies, select the **Limit to read-only version of above policy** check box to if you want use a policy that has Read/Write permissions but limit this service account to read-only. This option lets you maintain one version of a custom RBAC policy and use it two ways: as read/write and as read-only.

1. Select **Create Service Account**.

1. Copy the service account token and save it in a secure location. The token will not be available to view again.

   :::note
   To remove a service account, select **Remove** for the service account that you want to delete.
   :::

### Generate a User API Token

To generate a user API token:

1. Log in to the Vendor Portal and go to the [Account Settings](https://vendor.replicated.com/account-settings) page.
1. Under **User API Tokens**, select **Create a user API token**. If one or more tokens already exist, you can add another by selecting **New user API token**.

   <img alt="User API Token Page" src="/images/user-token-list.png" width="600px"/>

   [View a larger version of this image](/images/user-token-list.png)

1. In the **New user API token** dialog, enter a name for the token in the **Nickname** field. Names for user API tokens must be unique per user. 

   <img alt="Create New User Token Dialog" src="/images/user-token-create.png" width="400px"/>

   [View a larger version of this image](/images/user-token-create.png)

1. Select the required permissions or use the default **Read and Write** permissions. Then select **Create token**.

   :::note
   The token must have `Read and Write` access to create new releases.
   :::

1. Copy the user API token that displays and save it in a secure location. The token will not be available to view again.

   :::note
   To revoke a token, select **Revoke token** for the token that you want to delete.
   :::


---


import CollabRbacResourcesImportant from "../partials/collab-repo/_collab-rbac-resources-important.mdx"

# Configuring RBAC Policies

This topic describes how to use role-based access policies (RBAC) to grant or deny team members permissions to use Replicated services in the Replicated Vendor Portal.

## About RBAC Policies

By default, every team has two policies created automatically: **Admin** and **Read Only**. If you have an Enterprise plan, you will also have the **Sales** and **Support** policies created automatically. These default policies are not configurable. For more information, see [Default RBAC Policies](#default-rbac) below.

You can configure custom RBAC policies if you are on the Enterprise pricing plan. Creating custom RBAC policies lets you limit which areas of the Vendor Portal are accessible to team members, and control read and read/write privileges to groups based on their role. For example, you can limit access for the sales team to one application and to specific channels. Or, you can grant only certain users permission to promote releases to your production channels. 

You can also create custom RBAC policies in the Vendor Portal to manage user access and permissions in the Replicated collab repository in GitHub. For more information, see [Managing Access to the Collab Repository](team-management-github-username).

## Default RBAC Policies {#default-rbac}

This section describes the default RBAC policies that are included for Vendor Portal teams, depending on the team's Replicated pricing plan.

### Admin 

The Admin policy grants read/write permissions to all resources on the team. 

:::note
This policy is automatically created for all plans.
:::

```json
{
  "v1": {
    "name": "Admin",
    "resources": {
      "allowed": [
        "**/*"
      ],
      "denied": []
    }
  }
}
```

### Read Only

The Read Only policy grants read permission to all resources on the team except for API tokens.

:::note
This policy is automatically created for all plans.
:::

```json
{
  "v1": {
    "name": "Read Only",
    "resources": {
      "allowed": [
        "**/list",
        "**/read"
      ],
      "denied": [
        "**/*"
      ]
    }
  }
}
```

### Support Engineer

The Support Engineer policy grants read access to release, channels, and application data, and read-write access to customer and license details. It also grants permission to open Replicated support issues and upload support bundles. 

:::note
This policy is automatically created for teams with the Enterprise plan only.
:::

```json
{
  "v1": {
    "name": "Support Engineer",
    "resources": {
      "allowed": [
        "**/read",
        "**/list",
        "kots/app/*/license/**",
        "team/support-issues/read",
        "team/support-issues/write"
      ],
      "denied": [
        "**/*"
      ]
    }
  }
}
```

### Sales

The Sales policy grants read-write access to customers and license details and read-only access to resources necessary to manage licenses (applications, channels, and license fields). No additional access is granted.

:::note
This policy is automatically created for teams with the Enterprise plan only.
:::

```json
{
  "v1": {
    "name": "Sales",
    "resources": {
      "allowed": [
        "kots/app/*/read",
        "kots/app/*/channel/*/read",
        "kots/app/*/licensefields/read",
        "kots/app/*/license/**"
      ],
      "denied": [
        "**/*"
      ]
    }
  }
}
```

## Configure a Custom RBAC Policy

To configure a custom RBAC policy:

1. From the Vendor Portal [Team page](https://vendor.replicated.com/team), select **RBAC** from the left menu.

1. Do _one_ of the following:

    - Click **Create Policy** from the RBAC page to create a new policy.
    - Click **View policy** to edit an existing custom policy in the list.

      <CollabRbacResourcesImportant/>

1. Edit the fields in the policy dialog. In the **Definition** pane, specify the `allow` and `denied` arrays in the resources key to create limits for the role.

   The default policy allows everything and the **Config help** pane displays any errors.

    ![Create RBAC Policy](/images/policy-create.png) 

    - For more information, see [Policy Definition](#policy-definition).
    - For more information about and examples of rule order, see [Rule Order](#rule-order).
    - For a list of resource names, see [RBAC Resource Names](team-management-rbac-resource-names).

1. Click **Create Policy** to create a new policy, or click **Update Policy** to update an existing policy.

   :::note
   Click **Cancel** to exit without saving changes.
   :::

1. To apply RBAC policies to Vendor Portal team members, you can:

    - Assign policies to existing team members
    - Specify a policy when inviting new team members
    - Set a default policy for auto-joining a team

    See [Managing Team Members](team-management).

## Policy Definition

A policy is defined in a single JSON document:

```
{
  "v1": {
    "name": "Read Only",
    "resources": {
      "allowed": [
        "**/read",
        "**/list"
      ],
      "denied": [
        "**/*"
      ]
    }
  }
}
```

The primary content of a policy document is the resources key. The resources key should contain two arrays, identified as `allowed` and `denied`. Resources specified in the allowed list are allowed for users assigned to the policy, and resources specified in the denied list are denied.

Resource names are hierarchical, and support wildcards and globs. For a complete list of resource names that can be defined in a policy document, see [RBAC Resource Names](team-management-rbac-resource-names).

When a policy document has conflicting rules, the behavior is predictable. For more information about conflicting rules, see [Rule Order](#rule-order).

### Example: View Specific Application and Channel

  The following policy definition example limits any user with this role to viewing a specific application and a specific channel for that application:

    ```
    {
      "v1": {
        "name": "Policy Name",
        "resources": {
          "allowed": [
            "kots/app/appID/list",
            "kots/app/appID/read",
            "kots/app/appID/channel/channelID/list",
            "kots/app/appID/channel/channelID/read"
          ],
          "denied": []
        }
      }
    }
    ```
  The example above uses an application ID and a channel ID to scope the permissions of the RBAC policy. To find your application and channel IDs, do the following:

  - To get the application ID, click **Settings > Show Application ID (Advanced)** in the Vendor Portal.

  - To get the channel ID, click **Channels** in the Vendor Portal. Then click the Release History link for the channel that you want to limit access to. The channel ID displays in your browser URL.

## Rule Order

When a resource name is specified in both the `allow` and the `deny` chains of a policy, defined rules determine which rule is applied.

If `denied` is left empty, it is implied as a `**/*` rule, unless `**/*` rule is specified in the `allowed` resources. If a rule exactly conflicts with another rule, the `denied` rule takes precedence.

### Defining Precedence Using Rule Specificity
The most specific rule definition is always applied, when compared with less specific rules. Specificity of a rule is calculated by the number of asterisks (`**` and `*`) in the definition. A `**` in the rule definition is the least specific, followed by rules with `*`, and finally rules with no wildcards as the most specific.

### Example: No Access To Stable Channel

In the following example, a policy grants access to promote releases to any channel except the Stable channel. It uses the rule pattern `kots/app/[:appId]/channel/[:channelId]/promote`. Note that you specify the channel ID, rather than the channel name. To find the channel ID, go to the Vendor Portal **Channels** page and click the **Settings** icon for the target channel.

```json
{
  "v1": {
    "name": "No Access To Stable Channel",
    "resources": {
      "allowed": [
        "**/*"
      ],
      "denied": [
        "kots/app/*/channel/1eg7CyEofYSmVAnK0pEKUlv36Y3/promote"
      ]
    }
  }
}
```

### Example: View Customers Only

In the following example, a policy grants access to viewing all customers, but not to creating releases, promoting releases, or creating new customers.

```json
{
  "v1": {
    "name": "View Customers Only",
    "resources": {
      "allowed": [
        "kots/app/*/license/*/read",
        "kots/app/*/license/*/list",
        "kots/app/*/read",
        "kots/app/*/list"
      ],
      "denied": [
        "**/*"
      ]
    }
  }
}
```


---


import CollabRbacResourcesImportant from "../partials/collab-repo/_collab-rbac-resources-important.mdx"

# RBAC Resource Names

This a list of all available resource names for the Replicated vendor role-based access control (RBAC) policy:

## Integration Catalog

### integration/catalog/list

Grants the holder permission to view the catalog events and triggers available for integrations.

## kots

### kots/app/create

When allowed, the holder will be allowed to create new applications.

### kots/app/[:appId]/read
Grants the holder permission to view the application. If the holder does not have permissions to view an application, it will not appear in lists.

### kots/externalregistry/list
Grants the holder the ability to list external docker registry for application(s).

### kots/externalregistry/create

Grants the holder the ability to link a new external docker registry to application(s).

### kots/externalregistry/[:registryName]/delete

Grants the holder the ability to delete the specified linked external docker registry in application(s).

### kots/app/[:appId]/channel/create

Grants the holder the ability to create a new channel in the specified application(s).

### kots/app/[:appId]/channel/[:channelId]/archive

Grants the holder permission to archive the specified channel(s) of the specified application(s).

### kots/app/[:appId]/channel/[:channelId]/promote

Grants the holder the ability to promote a new release to the specified channel(s) of the specified application(s).

### kots/app/[:appId]/channel/[:channelId]/update

Grants the holder permission to update the specified channel of the specified application(s).

### kots/app/[:appId]/channel/[:channelId]/read

Grants the holder the permission to view information about the specified channel of the specified application(s).

### kots/app/[:appId]/enterprisechannel/[:channelId]/read

Grants the holder the permission to view information about the specified enterprise channel of the specified application(s).

### kots/app/[:appId]/channel/[:channelId]/releases/airgap

Grants the holder permission to trigger airgap builds for the specified channel.

### kots/app/[:appId]/channel/[:channelId]/releases/airgap/download-url

Grants the holder permission to get an airgap bundle download URL for any release on the specified channel.

### kots/app/[:appId]/installer/create

Grants the holder permission to create kURL installers. For more information, see [Creating a kURL installer](packaging-embedded-kubernetes).

### kots/app/[:appId]/installer/update

Grants the holder permission to update kURL installers. For more information, see [Creating a kURL installer](packaging-embedded-kubernetes).

### kots/app/[:appId]/installer/read

Grants the holder permission to view kURL installers. For more information, see [Creating a kURL installer](packaging-embedded-kubernetes).

### kots/app/[:appId]/installer/promote

Grants the holder permission to promote kURL installers to a channel. For more information, see [Creating a kURL installer](packaging-embedded-kubernetes).

:::note
The `kots/app/[:appId]/installer/promote` policy does not grant the holder permission to view and create installers. Users must be assigned both the `kots/app/[:appId]/installers` and `kots/app/[:appId]/installer/promote` policies to have permissions to view, create, and promote installers.
:::  

### kots/app/[:appId]/license/create

Grants the holder permission to create a new license in the specified application(s).

### kots/app/[:appId]/license/[:customerId]/read

Grants the holder permission to view the license specified by ID. If this is denied, the licenses will not show up in search, CSV export or on the Vendor Portal, and the holder will not be able to subscribe to this license's instance notifications.

### kots/app/[:appId]/license/[:customerId]/update

Grants the holder permission to edit the license specified by ID for the specified application(s).

### kots/app/[:appId]/license/[:customerId]/slack-notifications/read

Grants the holder permission to view the team's Slack notification subscriptions for instances associated with the specified license.

### kots/app/[:appId]/license/[:customerId]/slack-notifications/update

Grants the holder permission to edit the team's Slack notification subscriptions for instances associated with the specified license.

### kots/app/[:appId]/builtin-licensefields/update

Grants the holder permission to edit the builtin license field override values for the specified application(s).

### kots/app/[:appId]/builtin-licensefields/delete

Grants the holder permission to delete the builtin license field override values for the specified application(s).

### kots/license/[:customerId]/airgap/password

Grants the holder permission to generate a new download portal password for the license specified (by ID) for the specified application(s).

### kots/license/[:customerId]/archive

Grants the holder permission to archive the specified license (by ID).

### kots/license/[:customerId]/unarchive

Grants the holder permissions to unarchive the specified license (by ID).

### kots/app/[:appId]/licensefields/create

Grants the holder permission to create new license fields in the specified application(s).

### kots/app/[:appId]/licensefields/read

Grants the holder permission to view the license fields in the specified application(s).

### kots/app/[:appId]/licensefields/update

Grants the holder permission to edit the license fields for the specified application(s).

### kots/app/[:appId]/licensefields/delete

Grants the holder permission to delete the license fields for the specified application(s).

### kots/app/[:appId]/release/create

Grants the holder permission to create a new release in the specified application(s).

### kots/app/[:appId]/release/[:sequence]/update

Grants the holder permission to update the files saved in release sequence `[:sequence]` in the specified application(s). Once a release is promoted to a channel, it's not editable by anyone.

### kots/app/[:appId]/release/[:sequence]/read

Grants the holder permission to read the files at release sequence `[:sequence]` in the specified application(s).

### kots/app/[:appId]/customhostname/list

Grants the holder permission to view custom hostnames for the team.

### kots/app/[:appId]/customhostname/create

Grants the holder permission to create custom hostnames for the team.

### kots/app/[:appId]/customhostname/delete

Grants the holder permission to delete custom hostnames for the team.

### kots/app/[:appId]/customhostname/default/set

Grants the holder permission to set default custom hostnames.

### kots/app/[:appId]/customhostname/default/unset

Grants the holder permission to unset the default custom hostnames.

### kots/app/[:appId]/supportbundle/read

Grants the holder permission to view and download support bundles.

## Registry

### registry/namespace/:namespace/pull

Grants the holder permission to pull images from Replicated registry.

### registry/namespace/:namespace/push

Grants the holder permission to push images into Replicated registry.

## Compatibility Matrix

### kots/cluster/create

Grants the holder permission to create new clusters.

### kots/cluster/list

Grants the holder permission to list running and terminated clusters.

### kots/cluster/[:clusterId]

Grants the holder permission to get cluster details.

### kots/cluster/[:clusterId]/upgrade

Grants the holder permission to upgrade a cluster.

### kots/cluster/tag/update

Grants the holder permission to update cluster tags.

### kots/cluster/ttl/update

Grants the holder permission to update cluster ttl.

### kots/cluster/[:clusterId]/nodegroup

Grants the holder permission to update nodegroup details.

### kots/cluster[:clusterId]/kubeconfig

Grants the holder permision to get the kubeconfig for a cluster.

### kots/cluster/[:clusterId]/delete

Grants the holder permission to delete a cluster.

### kots/cluster/[:clusterId]/addon/list

Grants the holder permission to list addons for a cluster.

### kots/cluster/[:clusterId]/addon/[:addonId]/read

Grants the holder permission to read the addon for a cluster.

### kots/cluster/[:clusterId]/addon/[:addonId]/delete

Grants the holder permission to delete the addon for a cluster.

### kots/cluster/[:clusterId]/addon/create/objectStore

Grants the holder permission to create an object store for a cluster.

### kots/cluster/[:clusterId]/port/expose

Grants the holder permission to expose a port for a cluster.

### kots/cluster/[:clusterId]/port/delete

Grants the holder permission to delete a port for a cluster.

### kots/cluster/[:clusterId]/port/list

Grants the holder permission to list exposed ports for a cluster.

### kots/cluster/list-quotas

Grants the holder permission to list the quotas.

### kots/cluster/increase-quota

Grants the holder permission to request an increase in the quota.

### kots/vm/tag/update

Grants the holder permission to update vm tags.

### kots/vm/ttl/update

Grants the holder permission to update vm ttl.

### kots/vm/[:vmId]/port/expose

Grants the holder permission to expose a port for a vm.

### kots/vm/[:vmId]/port/list

Grants the holder permission to list exposed ports for a vm.

### kots/vm/[:vmId]/addon/[:addonId]/delete

Grants the holder permission to delete the addon for a vm.

## Team

### team/auditlog/read

Grants the holder permission to view the audit log for the team.

### team/authentication/update

Grants the holder permission to manage the following team authentication settings: Google authentication, Auto-join, and SAML authentication.

### team/authentication/read

Grants the holder permission to read the following authentication settings: Google authentication, Auto-join, and SAML authentication.

### team/integration/list

Grants the holder permission to view team's integrations.

### team/integration/create

Grants the holder permission to create an integration.

### team/integration/[:integrationId]/delete

Grants the holder permission to delete specified integration(s).

### team/integration/[:integrationId]/update

Grants the holder permission to update specified integration(s).

### team/members/list

Grants the holder permission to list team members and invitations.

### team/member/invite

Grants the holder permission to invite additional people to the team.

### team/members/delete

Grants the holder permission to delete other team members.

### team/notifications/slack-webhook/read

Grants the holder permission to view the team's Slack webhook for instance notifications.

### team/notifications/slack-webhook/update

Grants the holder permission to edit the team's Slack webhook for instance notifications.

### team/policy/read

Grants the holder permission to view RBAC policies for the team.

### team/policy/update

Grants the holder permission to update RBAC policies for the team.

### team/policy/delete

Grants the holder permission to delete RBAC policies for the team.

### team/policy/create

Grants the holder permission to create RBAC policies for the team.

### team/security/update

Grants the holder permission to manage team password requirements including two-factor authentication and password complexity requirements.

### team/serviceaccount/list

Grants the holder permission to list service accounts.

### team/serviceaccount/create

Grants the holder permission to create new service accounts.

### team/serviceaccount/[:name]/delete

Grants the holder permission to delete the service account identified by the name specified.

### team/support-issues/read

Grants the holder Read permissions in the Replicated collab repository in GitHub for the Vendor Portal team. Applies after the user adds their GitHub username to the Vendor Portal [Account Settings](https://vendor.replicated.com/account-settings) page.

To prevent access to the collab repository for an RBAC policy, add `team/support-issues/read` to the `denied:` list in the policy. For example:

```
{
 "v1": {
   "name": "Policy Name",
   "resources": {
     "allowed": [],
     "denied": [
    	"team/support-issues/read"
      ]
    }
  }
}
```

For more information about the Read role in GitHub, see [Permissions for each role](https://docs.github.com/en/organizations/managing-user-access-to-your-organizations-repositories/repository-roles-for-an-organization#permissions-for-each-role) in the GitHub documentation.

<CollabRbacResourcesImportant/>

### team/support-issues/write

Grants the holder Write permissions in the Replicated collab repository in GitHub for the Vendor Portal team. Applies after the user adds their GitHub username to the Vendor Portal [Account Settings](https://vendor.replicated.com/account-settings) page.

For more information about the Write role in GitHub, see [Permissions for each role](https://docs.github.com/en/organizations/managing-user-access-to-your-organizations-repositories/repository-roles-for-an-organization#permissions-for-each-role) in the GitHub documentation.

<CollabRbacResourcesImportant/>

### team/support-issues/triage

Grants the holder Triage permissions in the Replicated collab repository in GitHub for the Vendor Portal team. Applies after the user adds their GitHub username to the Vendor Portal [Account Settings](https://vendor.replicated.com/account-settings) page.

For more information about the Triage role in GitHub, see [Permissions for each role](https://docs.github.com/en/organizations/managing-user-access-to-your-organizations-repositories/repository-roles-for-an-organization#permissions-for-each-role) in the GitHub documentation.

<CollabRbacResourcesImportant/>

### team/support-issues/admin

Grants the holder Admin permissions in the Replicated collab repository in GitHub for the Vendor Portal team. Applies after the user adds their GitHub username to the Vendor Portal [Account Settings](https://vendor.replicated.com/account-settings) page.

For more information about the Admin role in GitHub, see [Permissions for each role](https://docs.github.com/en/organizations/managing-user-access-to-your-organizations-repositories/repository-roles-for-an-organization#permissions-for-each-role) in the GitHub documentation.

<CollabRbacResourcesImportant/>

## User

### user/token/list

Grants the holder permission to list user tokens.

### user/token/create

Grants the holder permission to create new user tokens.

### user/token/delete

Grants the holder permission to delete user tokens.


---


import CollabRepoAbout from "../partials/collab-repo/_collab-repo-about.mdx"
import CollabRbacImportant from "../partials/collab-repo/_collab-rbac-important.mdx"

# Managing Team Members

This topic describes how to manage team members in the Replicated Vendor Portal, such as inviting and removing members, and editing permissions. For information about managing user access to the Replicated collab repository in GitHub, see [Managing Collab Repository Access](team-management-github-username).

## Viewing Team Members
The [Team](https://vendor.replicated.com/team/members) page provides a list of all accounts currently associated with or invited to your team. Each row contains information about the user, including their two-factor authentication (2FA) status and role-based access control (RBAC) role, and lets administrators take additional actions, such as remove, re-invite, and edit permissions.

<img src="/images/teams-view.png" alt="View team members list in the Vendor Portal" width="700"/>

[View a larger image](/images/teams-view.png)

All users, including read-only, can see the name of the RBAC role assigned to each team member. When SAML authentication is enabled, users with the built-in read-only policy cannot see the RBAC role assigned to team members.

## Invite Members
By default, team administrators can invite more team members to collaborate. Invited users receive an email to activate their account. The activation link in the email is unique to the invited user. Following the activation link in the email also ensures that the invited user joins the team from which the invitation originated.

:::note
Teams that have enforced SAML-only authentication do not use the email invitation flow described in this procedure. These teams and their users must log in through their SAML provider.
:::

To invite a new team member:

1. From the [Team Members](https://vendor.replicated.com/team/members) page, click **Invite team member**.

   The Invite team member dialog opens.

   <img src="/images/teams-invite-member.png" alt="Invite team member dialog in the Vendor Portal" width="500"/>

   [Invite team member dialog](/images/teams-invite-member.png)

1. Enter the email address of the member.

1. In the **Permissions** field, assign an RBAC policy from the dropdown list.

     <CollabRbacImportant/>

1. Click **Invite member**.

   People invited to join your team receive an email notification to accept the invitation. They must follow the link in the email to accept the invitation and join the team. If they do not have a Replicated account already, they can create one that complies with your password policies, 2FA, and Google authentication requirements. If an invited user's email address is already associated with a Replicated account, by accepting your invitation, they automatically leave their current team and join the team that you have invited them to.

## Managing Invitations

Invitations expire after 7 days. If a prospective member has not accepted their invitation in this time frame, you can re-invite them without having to reenter their details. You can also remove the prospective member from the list.

You must be an administrator to perform this action.

To re-invite or remove a prospective member, do one of the following on the **Team Members** page:

* Click **Reinvite** from the row with the user's email address, and then click **Reinvite** in the confirmation dialog.

* Click **Remove** from the row with the user's email address, and then click **Delete Invitation** in the confirmation dialog.

## Edit Policy Permissions

You can edit the RBAC policy that is assigned to a member at any time.

<CollabRbacImportant/>

To edit policy permissions for individual team members:

1. From the the Team Members list, click **Edit permissions** next to a members name.

   :::note
   The two-factor authentication (2FA) status displays on the **Team members** page, but it is not configured on this page. For more information about configuring 2FA, see [Managing Two-Factor Authentication](team-management-two-factor-auth).
   :::

1. Select an RBAC policy from the **Permissions** dropdown list, and click **Save**. For information about configuring the RBAC policies that display in this list, see [Configuring RBAC Policies](team-management-rbac-configuring).

   <img src="/images/teams-edit-permissions.png" alt="Edit team member permissions in the Vendor Portal" width="400"/>

## Enable Users to Auto-join Your Team
By default, users must be invited to your team. Team administrators can use the auto-join feature to allow users from the same email domain to join their team automatically. This applies to users registering with an email, or with Google authentication if it is enabled for the team. The auto-join feature does not apply to SAML authentication because SAML users log in using their SAML provider's application portal instead of the Vendor Portal.

To add, edit, or delete custom RBAC policies, see [Configuring RBAC Policies](team-management-rbac-configuring).

To enable users to auto-join your team:

1. From the Team Members page, click **Auto-join** from the left navigation.
1. Enable the **Allow all users from my domain to be added to my team** toggle.

   <img src="/images/teams-auto-join.png" alt="Auto join dialog in the Vendor Portal" width="600"/>

   [View a larger image](/images/teams-auto-join.png)

1. For **Default RBAC policy level for new accounts**, you can use the default Read Only policy or select another policy from the list. This RBAC policy is applied to all users who join the team with the auto-join feature.

   <CollabRbacImportant/>


## Remove Members and End Sessions
As a Vendor Portal team admin, you can remove team members, except for the account you are currently logged in with.

If the team member that you remove added their GitHub username to their Account Settings page in the Vendor Portal to access the Replicated collab repository, then the Vendor Portal also automatically removes their username from the collab repository. For more information, see [Managing Collab Repository Access](team-management-github-username).

SAML-created users must be removed using this method to expire their existing sessions because Replicated does not support System for Cross-domain Identity Management (SCIM).

To remove a member:

1. From the Team Members page, click **Remove** on the right side of a user's row.

1. Click **Remove** in the confirmation dialog.

   The member is removed. All of their current user sessions are deleted and their next attempt at communicating with the server logs them out of their browser's session.

   If the member added their GitHub username to the Vendor Portal to access the collab repository, then the Vendor Portal also removes their GitHub username from the collab repository.

   For Google-authenticated users, if the user's Google account is suspended or deleted, Replicated logs that user out of all Google authenticated Vendor Portal sessions within 10 minutes. The user remains in the team list, but they cannot log into the Vendor Portal unless the username and password are allowed.

## Update Email Addresses

:::important
Changing team member email addresses has security implications. Replicated advises that you avoid changing team member email addresses if possible.
:::

Updating the email address for a team member requires creating a new account with the updated email address, and then deactivating the previous account.

To update the email address for a team member:

1. From the Team Members page, click **Invite team member**.

1. Assign the required RBAC policies to the new user.

1. Deactivate the previous team member account. 

---


import AirGapTelemetry from "../partials/instance-insights/_airgap-telemetry.mdx"

# Collecting Telemetry for Air Gap Instances

This topic describes how to collect telemetry for instances in air gap environments.

## Overview

Air gap instances run in environments without outbound internet access. This limitation prevents these instances from periodically sending telemetry to the Replicated Vendor Portal through the Replicated SDK or Replicated KOTS. For more information about how the Vendor Portal collects telemetry from online (internet-connected) instances, see [About Instance and Event Data](/vendor/instance-insights-event-data#about-reporting).

<AirGapTelemetry/>

The following diagram demonstrates how air gap telemetry is collected and stored by the Replicated SDK in a customer environment, and then shared to the Vendor Portal in a support bundle:

<img alt="Air gap telemetry collected by the SDK in a support bundle" src="/images/airgap-telemetry.png" width="800px"/>

[View a larger version of this image](/images/airgap-telemetry.png)

All support bundles uploaded to the Vendor Portal from air gap customers contributes to a comprehensive dataset, providing parity in the telemetry for air gap and online instances. Replicated recommends that you collect support bundles from air gap customers regularly (monthly or quarterly) to improve the completeness of the dataset. The Vendor Portal handles any overlapping event archives idempotently, ensuring data integrity.

## Requirement

Air gap telemetry has the following requirements:

* To collect telemetry from air gap instances, one of the following must be installed in the cluster where the instance is running:
   
   * The Replicated SDK installed in air gap mode. See [Installing the SDK in Air Gap Environments](/vendor/replicated-sdk-airgap).
   
   * KOTS v1.92.1 or later

   :::note
   When both the Replicated SDK and KOTS v1.92.1 or later are installed in the cluster (such as when a Helm chart that includes the SDK is installed by KOTS), both collect and store instance telemetry in their own dedicated secret, subject to the size limitation noted below. In the case of any overlapping data points, the Vendor Portal will report these data points chronologically based on their timestamp.
   :::

* To collect custom metrics from air gap instances, the Replicated SDK must installed in the cluster in air gap mode. See [Installing the SDK in Air Gap Environments](/vendor/replicated-sdk-airgap).

    For more information about custom metrics, see [Configuring Custom Metrics](https://docs.replicated.com/vendor/custom-metrics).

Replicated strongly recommends that all applications include the Replicated SDK because it enables access to both standard instance telemetry and custom metrics for air gap instances.

## Limitation

Telemetry data is capped at 4,000 events or 1MB per Secret; whichever limit is reached first.

When a limit is reached, the oldest events are purged until the payload is within the limit. For optimal use, consider collecting support bundles regularly (monthly or quarterly) from air gap customers.

## Collect and View Air Gap Telemetry

To collect telemetry from air gap instances:

1. Ask your customer to collect a support bundle. See [Generating Support Bundles](/vendor/support-bundle-generating).

1. After receiving the support bundle from your customer, go to the Vendor Portal **Customers**, **Customer Reporting**, or **Instance Details** page and upload the support bundle:

     ![upload new bundle button on instance details page](/images/airgap-upload-telemetry.png)

     The telemetry collected from the support bundle appears in the instance data shortly. Allow a few minutes for all data to be processed.


---


# Managing Applications

This topic provides information about managing applications, including how to create, delete, and retrieve the slug for applications in the Replicated Vendor Portal and with the Replicated CLI.

For information about creating and managing application with the Vendor API v3, see the [apps](https://replicated-vendor-api.readme.io/reference/createapp) section in the Vendor API v3 documentation.

## Create an Application

Teams can create one or more applications. It is common to create multiple applications for testing purposes.

### Vendor Portal

To create a new application:

1. Log in to the [Vendor Portal](https://vendor.replicated.com/). If you do not have an account, see [Creating a Vendor Account](/vendor/vendor-portal-creating-account).

1. In the top left of the page, open the application drop down and click **Create new app...**.

   <img alt="create new app drop down" src="/images/create-new-app.png" width="300px"/>

   [View a larger version of this image](/images/create-new-app.png)

1. On the **Create application** page, enter a name for the application.

   <img alt="create new app page" src="/images/create-application-page.png" width="500px"/>

   [View a larger version of this image](/images/create-application-page.png)

   :::important
   If you intend to use the application for testing purposes, Replicated recommends that you use a temporary name such as `My Application Demo` or `My Application Test`.

   You are not able to restore or modify previously-used application names or application slugs.
   :::

1. Click **Create application**.

### Replicated CLI

To create an application with the Replicated CLI:

1. Install the Replicated CLI. See [Installing the Replicated CLI](/reference/replicated-cli-installing).

1. Run the following command:

   ```bash
   replicated app create APP-NAME
   ```
   Replace `APP-NAME` with the name that you want to use for the new application.

   **Example**:

   ```bash
   replicated app create cli-app
   ID                             NAME               SLUG               SCHEDULER
   1xy9t8G9CO0PRGzTwSwWFkMUjZO    cli-app            cli-app            kots
   ```  

## Get the Application Slug {#slug}

Each application has a slug, which is used for interacting with the application using the Replicated CLI. The slug is automatically generated based on the application name and cannot be changed.

### Vendor Portal 

To get an application slug in the Vendor Portal:

1. Log in to the [Vendor Portal](https://vendor.replicated.com/) and go to **_Application Name_ > Settings**.

1. Under **Application Slug**, copy the slug.

   <img alt="Application slug" src="/images/application-settings.png" width="600px"/>

   [View a larger version of this image](/images/application-settings.png)

### Replicated CLI

To get an application slug with the Replicated CLI:

1. Install the Replicated CLI. See [Installing the Replicated CLI](/reference/replicated-cli-installing).

1. Run the following command:

   ```bash
   replicated app ls APP-NAME
   ```
   Replace `APP-NAME` with the name of the target application. Or, exclude `APP-NAME` to list all applications in the team.

   **Example:**

   ```bash
   replicated app ls cli-app
   ID                             NAME               SLUG               SCHEDULER
   1xy9t8G9CO0PRGzTwSwWFkMUjZO    cli-app            cli-app            kots
   ```

1. Copy the value in the `SLUG` field.

## Delete an Application

When you delete an application, you also delete all licenses and data associated with the application. You can also optionally delete all images associated with the application from the Replicated registry. Deleting an application cannot be undone.

### Vendor Portal 

To delete an application in the Vendor Portal:

1. Log in to the [Vendor Portal](https://vendor.replicated.com/) and go to **_Application Name_ > Settings**.

1. Under **Danger Zone**, click **Delete App**.

   <img alt="Setting page" src="/images/application-settings.png" width="600px"/>

   [View a larger version of this image](/images/application-settings.png)

1. In the **Are you sure you want to delete this app?** dialog, enter the application name. Optionally, enter your password if you want to delete all images associated with the application from the Replicated registry.

   <img alt="delete app dialog" src="/images/delete-app-dialog.png" width="400px"/>

   [View a larger version of this image](/images/delete-app-dialog.png)

1. Click **Delete app**.

### Replicated CLI

To delete an application with the Replicated CLI:

1. Install the Replicated CLI. See [Installing the Replicated CLI](/reference/replicated-cli-installing).

1. Run the following command:

    ```bash
    replicated app delete APP-NAME
    ```
    Replace `APP-NAME` with the name of the target application.

1. When prompted, type `yes` to confirm that you want to delete the application.

    **Example:**

    ```bash
    replicated app delete deletion-example
      • Fetching App ✓
    ID                NAME                SLUG                 SCHEDULER
    1xyAIzrmbvq...    deletion-example    deletion-example     kots
    Delete the above listed application? There is no undo: yes█
     • Deleting App ✓
    ```

---

