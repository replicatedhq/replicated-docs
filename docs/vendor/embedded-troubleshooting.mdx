import SupportBundleIntro from "../partials/support-bundles/_ec-support-bundle-intro.mdx"
import EmbeddedClusterSupportBundle from "../partials/support-bundles/_generate-bundle-ec.mdx"
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Troubleshooting Embedded Cluster

This topic provides information about troubleshooting Replicated Embedded Cluster.

## Troubleshoot with Support Bundles

This section includes information about how to collect support bundles for Embedded Cluster installations. For more information about support bundles, see [About Preflight Checks and Support Bundles](/vendor/preflight-support-bundle-about).

### About the Default Embedded Cluster Support Bundle

<SupportBundleIntro/>

<EmbeddedClusterSupportBundle/>

## View Logs

You can view logs for both Embedded Cluster and the systemd k0s service running in the cluster to help troubleshoot installation errors.

### View Logs for Embedded Cluster

To view installation logs for Embedded Cluster:

1. SSH onto the installation VM.

1. Navigate to `/var/log/embedded-cluster` and open the `.log` file to view logs. 

### View k0s Logs

You can use the journalctl command line tool to access logs for systemd services, including k0s.

To use journalctl to view k0s logs:

1. SSH onto the installation VM.

1. Use journalctl to view logs for the k0s systemd service that was deployed by Embedded Cluster.

    **Example:**

    ```bash
    journalctl -u k0scontroller
    ```

## Access the Cluster

With Embedded Cluster, end-users are rarely supposed to need to use the CLI. Typical workflows, like updating the application and the cluster, are driven through the Admin Console.

Nonetheless, there are times when vendors or their customers need to use the CLI for development or troubleshooting.

To access the cluster and use other included binaries:

1. SSH onto a controller node.

1. Use the Embedded Cluster shell command to start a shell with access to the cluster:

     ```
     sudo ./APP_SLUG shell
     ```

     The output looks similar to the following:
     ```
        __4___
    _  \ \ \ \   Welcome to APP_SLUG debug shell.
    <'\ /_/_/_/   This terminal is now configured to access your cluster.
    ((____!___/) Type 'exit' (or CTRL+d) to exit.
    \0\0\0\0\/  Happy hacking.
    ~~~~~~~~~~~
    root@alex-ec-2:/home/alex# export KUBECONFIG="/var/lib/embedded-cluster/k0s/pki/admin.conf"
    root@alex-ec-2:/home/alex# export PATH="$PATH:/var/lib/embedded-cluster/bin"
    root@alex-ec-2:/home/alex# source <(kubectl completion bash)
    root@alex-ec-2:/home/alex# source /etc/bash_completion
    ```

     The appropriate kubeconfig is exported, and the location of useful binaries like kubectl and Replicatedâ€™s preflight and support-bundle plugins is added to PATH.

     :::note
     You cannot run the `shell` command on worker nodes.
     :::

1. Use the available binaries as needed.

     **Example**:

     ```bash
     kubectl version
     ```
     ```
     Client Version: v1.29.1
     Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
     Server Version: v1.29.1+k0s
     ```

1. Type `exit` or **Ctrl + D** to exit the shell.

     :::note
     If you encounter a typical workflow where your customers have to use the Embedded Cluster shell, reach out to Alex Parker at alexp@replicated.com. These workflows might be candidates for additional Admin Console functionality.
     :::

## Reset a Node

Resetting a node removes the cluster and your application from that node. This is useful for iteration, development, and when mistakes are made, so you can reset a machine and reuse it instead of having to procure another machine.

If you want to completely remove a cluster, you need to reset each node individually.

When resetting a node, OpenEBS PVCs on the node are deleted. Only PVCs created as part of a StatefulSet will be recreated automatically on another node. To recreate other PVCs, the application will need to be redeployed.

To reset a node:

1. SSH onto the machine. Ensure that the Embedded Cluster binary is still available on that machine.

1. Run the following command to reset the node and automatically reboot the machine to ensure that transient configuration is also reset:

    ```
    sudo ./APP_SLUG reset
    ```
    Where `APP_SLUG` is the unique slug for the application.

    :::note
    Pass the `--no-prompt` flag to disable interactive prompts. Pass the `--force` flag to ignore any errors encountered during the reset.
    :::     

## Troubleshoot Errors

This section provides troubleshooting advice for common errors.

### Installation failure when NVIDIA GPU Operator is included as Helm extension

#### Symptom

A release that includes that includes the NVIDIA GPU Operator as a Helm extensions fails to install.

#### Cause 

If there are any containerd services on the host, the NVIDIA GPU Operator will generate an invalid containerd config, causing the installation to fail.

#### Solution

Remove any existing containerd services that are running on the host (such as those deployed by Docker) before attempting to install the release with Embedded Cluster.

For more information, see [NVIDIA GPU Operator](/vendor/embedded-using#nvidia-gpu-operator) in _Using Embedded Cluster_.

### Calico networking issues 

#### Symptom

Symptoms of Calico networking issues can include:

* The pod is stuck in a CrashLoopBackOff state with failed health checks:

    ```
    Warning Unhealthy 6h51m (x3 over 6h52m) kubelet Liveness probe failed: Get "http://<ip:port>/readyz": dial tcp <ip:port>: connect: no route to host
    Warning Unhealthy 6h51m (x19 over 6h52m) kubelet Readiness probe failed: Get "http://<ip:port>/readyz": dial tcp <ip:port>: connect: no route to host
    ....
    Unhealthy               pod/registry-dc699cbcf-pkkbr     Readiness probe failed: Get "https://<ip:port>/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
    Unhealthy               pod/registry-dc699cbcf-pkkbr     Liveness probe failed: Get "https://<ip:port>/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
    ...
    ```

* The pod log contains an I/O timeout:

    ```
    server APIs: config.k8ssandra.io/v1beta1: Get \"https://***HIDDEN***:443/apis/config.k8ssandra.io/v1beta1\": dial tcp ***HIDDEN***:443: i/o timeout"}
    ```

#### Cause 

Reasons can include:

* podCIDR and serviceCIDR overlapping with the host network CIDR.

* Incorrect kernel parameters values.

* VXLAN traffic getting dropped. By default, Calico uses VXLAN as the overlay networking protocol, with Always mode. This mode encapsulates all pod-to-pod traffic in VXLAN packets. If for some reasons, the VXLAN packets get filtered by the network, the pod will not able to communicate with other pods.

#### Solution

<Tabs>
  <TabItem value="overlap" label="podCIDR and serviceCIDR overlapping with the host network CIDR" default>
  To troubleshoot podCIDR and serviceCIDR overlapping with the host network CIDR:
  1. Run the following command to verify:
        ```
        cat /etc/k0s/k0s.yaml | grep -i cidr
            podCIDR: 10.244.0.0/17
            serviceCIDR: 10.244.128.0/17
        ```
        The default podCIDR is 10.244.0.0/16 and serviceCIDR is 10.96.0.0/12.

    1. View pod network interfaces excluding Calico interfaces, and ensure no overlapping CIDRs.
        ```
        ip route | grep -v cali
        default via 10.152.0.1 dev ens4 proto dhcp src 10.152.0.4 metric 100
        10.152.0.1 dev ens4 proto dhcp scope link src 10.152.0.4 metric 100
        blackhole 10.244.101.192/26 proto 80
        169.254.169.254 via 10.152.0.1 dev ens4 proto dhcp src 10.152.0.4 metric 100
        ```

    1. Reset and reboot the installation. See [Reset a Node](#reset-a-node) above.

    1. Reinstall the application with different CIDRs using the `--cidr` flag:

        ```bash
        sudo ./APP_SLUG install --license license.yaml --cidr 172.16.136.0/16
        ```
        Where `APP_SLUG` is the unique slug for the installed application.

        For more information, see [Embedded Cluster Install Options](/reference/embedded-cluster-install).
  </TabItem>
  <TabItem value="kernel" label="Incorrect kernel parameter values">
  To troubleshoot incorrect kernel parameter values:
  1. Use sysctl to verify that these parameters are set correctly:

        ```bash
        net.ipv4.conf.default.arp_filter = 0
        net.ipv4.conf.default.arp_ignore = 0
        net.ipv4.ip_forward = 1
        ```            
    
    1. If the values are not set correctly, run the following command to set them:

        ```
        sysctl -w net.ipv4.conf.default.arp_filter=0
        sysctl -w net.ipv4.conf.default.arp_ignore=0
        sysctl -w net.ipv4.ip_forward=1


        echo "net.ipv4.conf.default.arp_filter=0" >> /etc/sysctl.conf
        echo "net.ipv4.conf.default.arp_ignore=0" >> /etc/sysctl.conf
        echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf

        sysctl -p
        ```

    1. Reset and reboot the installation. See [Reset a Node](#reset-a-node) above.

    1. Re-run the installation.
  </TabItem>
  <TabItem value="vxlan" label="VXLAN traffic getting dropped">

  As a temporary troubleshooting measure, set the mode to CrossSubnet and see if the issue persists. This mode only encapsulates traffic between pods across different subnets with VXLAN.

        ```bash
        kubectl patch ippool default-ipv4-ippool --type=merge -p '{"spec": {"vxlanMode": "CrossSubnet"}}'
        ```
    
    If this resolves the connectivity issues, there is likely an underlying network configuration problem with VXLAN traffic that should be addressed.
  </TabItem>
</Tabs>