import HaArchitecture from "../partials/embedded-cluster/_multi-node-ha-arch.mdx"

# Manage Multi-Node Clusters with Embedded Cluster

This topic describes managing nodes in clusters created with Replicated Embedded Cluster, including how to add nodes and enable high-availability for multi-node clusters.

## Limitations

Multi-node clusters with Embedded Cluster have the following limitations:

* Support for multi-node clusters with Embedded Cluster is Beta. Only single-node embedded clusters are Generally Available (GA).

* High availability for Embedded Cluster in an Alpha feature. This feature is subject to change, including breaking changes. For more information about this feature, reach out to Alex Parker at [alexp@replicated.com](mailto:alexp@replicated.com).

* The same Embedded Cluster data directory used at installation is used for all nodes joined to the cluster. This is either the default `/var/lib/embedded-cluster` directory or the directory set with the [`--data-dir`](/reference/embedded-cluster-install#flags) flag. You cannot choose a different data directory for Embedded Cluster when joining nodes.

* More than one controller node should not be joined at the same time. When joining a controller node, a warning is printed that explains that the user should not attempt to join another node until the controller node joins successfully.

* Setting node roles with the Embedded Cluster Config [roles](/reference/embedded-config#roles) key is Beta.

* The `get join-command` command always returns the commands for joining a node with the controller role. It does not support printing the join command for any custom node roles defined in the Embedded Cluster Config `roles` key. See [Automate Controller Node Joins](#automate-node-joins) below.

## Add Nodes to a Cluster (Beta) {#add-nodes}

### Online Installations

To add a node to a cluster with Embedded Cluster in online (internet-connected) installations:

1. (Optional) In the Embedded Cluster Config, configure the `roles` key to customize node roles. For more information, see [roles](/reference/embedded-config#roles) in _Embedded Cluster Config_. When you are done, create and promote a new release with the updated Config.

1. Do one of the following:

   1. Follow the steps in [Online Installation with Embedded Cluster](/enterprise/installing-embedded) to install. A **Nodes** screen is displayed as part of the installation flow in the Admin Console.

   1. Otherwise, to add a node to an existing cluster:

       1. Log in to the Admin Console.
      
       1. If you promoted a new release that configures the `roles` key in the Embedded Cluster Config, update the instance to the new version. See [Perform Updates in Embedded Clusters](/enterprise/updating-embedded).
      
       1. Go to **Cluster Management > Add node** at the top of the page.
   
1. If you configured the [roles](/reference/embedded-config#roles) key to customize node roles, select one or more roles for the node.

    The Admin Console page is updated to display the commands for downloading the Embedded Cluster binary, extracting the binary, and joining the node to the cluster based on the roles you selected.

    Note the following:

     * If the [roles](/reference/embedded-config#roles) key is not configured, all new nodes joined to the cluster are assigned the `controller` role by default. The `controller` role designates nodes that run the Kubernetes control plane. Controller nodes can also run other workloads, such as application or Replicated KOTS workloads.

     * The role cannot be changed after a node is added. If you need to change a node’s role, reset the node and add it again with the new role.

1. SSH onto the node that you want to join. Keep the Admin Console page that shows the join commands open for the next steps.

1. Copy the command to download the Embedded Cluster binary and then run it on the node you want to join.

1. Copy and run the command to extract the Embedded Cluster binary.

1. Copy and run the join command to add the node to the cluster.

1. In the Admin Console, either on the installation **Nodes** screen or on the **Cluster Management** page, verify that the node appears. Wait for the node's status to change to Ready.

1. Repeat these steps for each node you want to add.

### Air Gap Installations

To add a node to a cluster with Embedded Cluster in air gap installations:

1. (Optional) In the Embedded Cluster Config, configure the `roles` key to customize node roles. For more information, see [roles](/reference/embedded-config#roles) in _Embedded Cluster Config_. When you are done, create and promote a new release with the updated Config.

1. Do one of the following:

   1. Follow the steps in [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap) to install. A **Nodes** screen is displayed as part of the installation flow in the Admin Console that allows you to choose a node role and copy the relevant join commands.

   1. Otherwise, to add a node to an existing cluster:

       1. Log in to the Admin Console.
      
       1. If you promoted a new release that configures the `roles` key in the Embedded Cluster Config, update the instance to the new version. See [Perform Updates in Embedded Clusters](/enterprise/updating-embedded).
      
       1. Go to **Cluster Management > Add node** at the top of the page.
   
1. If you configured the [roles](/reference/embedded-config#roles) key to customize node roles, select one or more roles for the node.

    The Admin Console page is updated to display the commands for downloading the Embedded Cluster binary, extracting the binary, and joining the node to the cluster based on the roles you selected.

    Note the following:

     * If the [roles](/reference/embedded-config#roles) key is not configured, all new nodes joined to the cluster are assigned the `controller` role by default. The `controller` role designates nodes that run the Kubernetes control plane. Controller nodes can also run other workloads, such as application or Replicated KOTS workloads.

     * The role cannot be changed after a node is added. If you need to change a node’s role, reset the node and add it again with the new role.

1. SSH onto a node that has internet access. Keep the Admin Console page that shows the join commands open for the next steps.

1. On the node with internet access, copy and run the first command to download the Embedded Cluster binary.

1. Move the downloaded assets to the air-gapped node that you want to join.

1. Copy and run the second command from the Admin Console to untar the Embedded Cluster assets.

1. Copy and run the join command.

    **Example:**

    ```bash
    sudo ./APP_SLUG join --airgap-bundle APP_SLUG.airgap 10.128.0.32:30000 TxXboDstBAamXaPdleSK7Lid
    ```

1. In the Admin Console, either on the installation **Nodes** screen or on the **Cluster Management** page, verify that the node appears. Wait for the node's status to change to Ready.

1. Repeat these steps for each node you want to add.

## Automate Controller Node Joins (Beta) {#automate-node-joins}

With Embedded Cluster, you can use the command line to get the commands for joining controller nodes, rather than having to log into the Admin Console UI to get the commands. This is especially useful when testing multi-node Embedded Cluster installations where you need to automate the process of joining controller nodes to a cluster.

To automate controller node joins with Embedded Cluster:

1. On a node that is already joined to the cluster, run the following command:

   ```bash
   sudo ./APP_SLUG get join-command
   ```
   Where `APP_SLUG` is the unique application slug.

   The output lists the commands for downloading the Embedded Cluster binary, extracting the binary, and joining a new controller node to the cluster.

   **Example:**

   ```bash
   sudo ./your-app-slug get join-command
   ```
   ```bash
   curl -k https://172.17.0.2:30000/api/v1/embedded-cluster/binary -o embedded-cluster.tar.gz && \
   tar -xvf embedded-cluster.tar.gz && \
   sudo ./embedded-cluster join 172.17.0.2:30000 1234aBcD
   ```
   **Example with JSON output:**

   ```bash
   sudo ./your-app-slug get join-command -ojson
   ```
   ```bash
   {
   "commands": [
      "curl -k https://172.17.0.2:30000/api/v1/embedded-cluster/binary -o embedded-cluster.tar.gz",
      "tar -xvf embedded-cluster.tar.gz",
      "sudo ./embedded-cluster join 172.17.0.2:30000 1234aBcD"
     ]
   }
   ```

1. On the node that you want to join as a controller, run each of the commands from the `get join-command` output.

## High Availability for Multi-Node Clusters (Alpha) {#ha}

:::important
High availability for Embedded Cluster in an Alpha feature. This feature is subject to change, including breaking changes. For more information about this feature, reach out to Alex Parker at [alexp@replicated.com](mailto:alexp@replicated.com).
:::

Multi-node clusters are not highly available by default. The first node of the cluster holds important data for Kubernetes and KOTS, such that the loss of this node would be catastrophic for the cluster. Enabling high availability requires that at least three controller nodes are present in the cluster.

Users are automatically prompted to enable HA when joining the third controller node to a cluster. Alternatively, users can enable HA with the `enable-ha` command after adding three or more controller nodes.

### HA Architecture

<HaArchitecture/>

For more information about the Embedded Cluster built-in extensions, see [Built-In Extensions](/vendor/embedded-overview#built-in-extensions) in _Embedded Cluster Overview_.

### Requirements

Enabling high availability has the following requirements:

* High availability is supported with Embedded Cluster 1.4.1 and later.

* The [`enable-ha`](#enable-ha-existing) command is available with Embedded Cluster 2.3.0 and later.

* High availability is supported only for clusters where at least three nodes with the `controller` role are present.

### Best Practices for High Availability

Consider the following best practices and recommendations for creating HA clusters:

* At least three _controller_ nodes that run the Kubernetes control plane are required for HA. This is because clusters use a quorum system, in which more than half the nodes must be up and reachable. In clusters with three controller nodes, the Kubernetes control plane can continue to operate if one node fails because a quorum can still be reached by the remaining two nodes. By default, with Embedded Cluster, all new nodes added to a cluster are controller nodes. For information about customizing the `controller` node role, see [roles](/reference/embedded-config#roles) in _Embedded Cluster Config_.

* Always use an odd number of controller nodes in HA clusters. Using an odd number of controller nodes ensures that the cluster can make decisions efficiently with quorum calculations. Clusters with an odd number of controller nodes also avoid split-brain scenarios where the cluster runs as two, independent groups of nodes, resulting in inconsistencies and conflicts.

* You can have any number of _worker_ nodes in HA clusters. Worker nodes do not run the Kubernetes control plane, but can run workloads such as application or Replicated KOTS workloads.

### Create a Multi-Node Cluster with High Availability {#create-ha}

You can enable high availability for a multi-node cluster when joining the third controller node. Alternatively, you can enable HA for an existing cluster with three or more controller nodes. For more information, see [Enable High Availability For an Existing Cluster](#enable-ha-existing) below.

To create a multi-node HA cluster:

1. Set up a cluster with at least two controller nodes. You can do an online (internet-connected) or air gap installation. For more information, see [Online Installation with Embedded Cluster](/enterprise/installing-embedded) or [Air Gap Installation with Embedded Cluster](/enterprise/installing-embedded-air-gap).

1. Get the command for joining the third controller node either in the Admin Console **Cluster Management** tab or by running the following command on an existing node:

   ```bash
   sudo ./APP_SLUG get join-command
   ```
   Where `APP_SLUG` is the unique application slug.

1. SSH onto the node that you want to add as a third controller.

1. On the node, run the join command that you copied.

     **Example:**

     ```bash
     sudo ./APP_SLUG join 10.128.0.80:30000 tI13KUWITdIerfdMcWTA4Hpf
     ```
     Where `APP_SLUG` is the unique slug for the application.

     :::note
     For Embedded Cluster versions earlier than 2.3.0, pass the `--enable-ha` flag with the `join` command.
     :::

1. In response to the prompt asking if you want to enable high availability, type `y` or `yes`.

1. Wait for the migration to HA to complete.

### Enable High Availability For an Existing Cluster {#enable-ha-existing}

To enable high availability for an existing Embedded Cluster installation with three or more controller nodes, run the following command:

```bash
sudo ./APP_SLUG enable-ha
```

Where `APP_SLUG` is the unique slug for the application.